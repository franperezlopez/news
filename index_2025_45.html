<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="initial-scale=1.0">
    <meta name="description" content="" />
    <meta property="og:title" content="" />
    <meta property="og:description" content="" />
    <meta property="og:image" content="" />

    <title>ML NEWS / 2025 / 45th week</title>
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/night.css" />
    <link rel="stylesheet" href="dist/slider.css" />

</head>
<body>
  <script src="dist/reveal.js"></script>
  <script src="dist/scrolling-image.js"></script>

  <div class="reveal">
    <div class="slides">

<section data-background-video="assets/slide_2025_45/interrupt-long-running-queries-add-context-gpt-5-pro.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/OpenAI/status/1986194298971590988" target="_blank">
  <img alt='You can now interrupt ongoing queries and provide new context without needing to restart or lose your progress. This feature is especially helpful for refining complex research or advanced GPT-5 Pro queries, as the model will incorporate your updated requirements into its responses.To use this, simply click "update" in the sidebar and enter any extra details or clarifications you want to add.' src="assets/slide_2025_45/interrupt-long-running-queries-add-context-gpt-5-pro.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_45/semantic-search-accuracy-improvement-code-retrieval.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/cursor_ai/status/1986124270548709620" target="_blank">
  <img alt="Semantic search enhances the accuracy of agents using various advanced models, particularly in large codebases where traditional search methods like grep are insufficient.Further details are available about the results and the process of training an embedding model specifically for code retrieval." src="assets/slide_2025_45/semantic-search-accuracy-improvement-code-retrieval.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_45/codemaps-introduction-swe-1-5-sonnet-4-5-windsurf.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/cognition/status/1985755284527010167" target="_blank">
   <img alt="Codemaps has been introduced in @windsurf, leveraging the capabilities of SWE-1.5 and Sonnet 4.5. This feature aims to enhance developers' understanding and navigation of their codebases.Emphasizing the importance of internalizing code for deeper problem comprehension, Codemaps helps users visualize and better grasp their projects, aligning with the philosophy that true understanding comes from having the code &quot;in your head.&quot;" src="assets/slide_2025_45/codemaps-introduction-swe-1-5-sonnet-4-5-windsurf.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/windsurf/status/1985757575745593459" target="_blank">
   <img alt='Codemaps has launched to address the primary bottleneck in coding productivity: understanding existing code. The tool aims to enhance both manual and AI-assisted coding by improving code comprehension.By increasing understanding, Codemaps helps users avoid mistakes ("slop") that result from moving too quickly without sufficient context. With Codemaps, developers can boost their output and code quality by leveraging AI to scale their understanding as they work.' class="half-size" src="assets/slide_2025_45/codemaps-ai-understanding-code-productivity.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_45/voice-collaboration-microsoft-365-copilot-microsoft.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://techcommunity.microsoft.com/blog/microsoft365copilotblog/introducing-voice-in-microsoft-365-copilot-a-more-productive-way-to-work-on-the-/4466034?ocid=usoc_TWITTER_M365_spl100008929257593" target="_blank">
  <img alt="Microsoft has introduced voice capabilities to Microsoft 365 Copilot, allowing users to interact with Copilot through spoken commands. This feature enables users to talk to Copilot, interrupt responses naturally, and receive real-time spoken replies grounded in their work and web data. Users can adjust Copilot's speaking style, mute or end conversations as needed, and access saved transcripts for future reference. Voice chat is currently available in the Microsoft 365 Copilot mobile app for licensed users, with desktop and web support coming soon.Security and privacy remain a priority, with voice transcripts managed like regular Copilot conversations and no audio being stored. The rollout will expand to users without a Copilot license in the coming months and is subject to service capacity. Microsoft encourages feedback on the evolving voice features to help improve the user experience." src="assets/slide_2025_45/introducing-voice-in-microsoft-365-copilot-microsoft.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://techcommunity.microsoft.com/blog/microsoft365copilotblog/build-with-copilot-pages-a-new-way-to-bring-your-ideas-to-life/4466758" target="_blank">
  <img alt="Microsoft has introduced Copilot Pages, a new feature in Microsoft 365 Copilot that allows users to easily create dynamic, interactive web pages without any coding skills. By leveraging the latest large language models (LLMs), Copilot can write code and render rich experiences directly in a Copilot Page. Users can start by enabling the GPT-5 toggle in Copilot Chat and describing their idea in plain language; Copilot then generates a preview of the interactive page, which can be edited and shared with collaborators.This capability is currently available to Frontier users with M365 Copilot licenses and will be expanded soon. Users can build prototypes, visualizations, reports, and more, accelerating decision-making and comprehension of complex concepts. Additionally, Copilot Pages can be converted into managed apps with governance and storage using App Builder. For further guidance, Microsoft provides support articles and blog resources." src="assets/slide_2025_45/build-with-copilot-pages-microsoft.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/GoogleLabs/status/1986542481002143921" target="_blank">
  <img alt="Opal, the no-code AI app builder, is now available in over 160 countries, enabling more users to create mini-apps using natural language. The platform supports tasks like automating research and generating marketing campaigns from a single idea.Builders are encouraged to showcase their creations, and further details about the expansion can be found on the official blog." class="half-size" src="assets/slide_2025_45/opal-no-code-ai-app-builder-expands-to-160-countries-opal.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/Kimi_Moonshot/status/1986449512538513505" target="_blank">
   <img alt="K2 Thinking is an open-source agentic AI model designed for advanced reasoning, agentic search, and coding tasks. It achieves state-of-the-art results on HLE (44.9%) and BrowseComp (60.2%), and can execute up to 200–300 sequential tool calls autonomously. With a 256K context window, it demonstrates strong capabilities in handling complex, multi-step processes.The model is available for use in chat mode at the provided website, with a full agentic mode coming soon. Users can access K2 Thinking via API, and further details including a tech blog, weights, and code are available through the shared links." class="half-size" src="assets/slide_2025_45/k2-thinking-agent-open-source-model-launch.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://www.interconnects.ai/p/kimi-k2-thinking-what-it-means" target="_blank">
   <img alt="Moonshot AI's Kimi K2 Thinking is a newly released open-source reasoning model from a rapidly advancing Chinese lab, notable for its strong benchmark performance—sometimes surpassing leading closed models—and its preserved writing quality through reinforcement learning. With 1 trillion total parameters (32B active), 256K context length, and advanced agentic tool-use capabilities (supporting up to 200–300 sequential tool calls), Kimi K2 Thinking represents the closest performance open models have achieved to the closed frontier, challenging industry leaders and accelerating the pace of global AI development.The article highlights the rapid iteration and public release pace of Chinese labs, the importance of both benchmark scores and real-world user feedback, and the growing influence of China's AI ecosystem. The rise of models like DeepSeek, Qwen, and Kimi is shifting mindshare internationally and putting pricing and innovation pressure on American closed labs. The post also explains Kimi's technical advances such as INT4 quantization for faster inference and discusses the broader implications for the global AI landscape, emphasizing a move toward more nuanced evaluation beyond benchmarks." src="assets/slide_2025_45/kimi-k2-thinking-moonshot-ai.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/abs/2511.01689" target="_blank">
   <img alt="This paper introduces the first open-source implementation of character training for large language model (LLM) AI assistants, moving beyond traditional &quot;helpfulness, honesty, and harmlessness&quot; paradigms. The authors leverage Constitutional AI and a new pipeline using synthetic introspective data to fine-tune models for specific personas (e.g., humorous, deeply caring, malevolent). Their method allows for more precise and robust shaping of the assistant's character compared to alternatives like system prompts or activation steering, without negatively impacting general capabilities.The study also presents a novel evaluation method that analyzes revealed preferences to measure holistic changes in assistant character, demonstrating greater resistance to adversarial prompting and improved persona coherence. All methods and code are open-sourced, providing a foundation for more nuanced, controlled, and transparent character training in open LLMs." src="assets/slide_2025_45/open-character-training-shaping-the-persona-of-ai-assistants-through-constitutional-ai-university-of.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/_maiush/status/1985710942039261196" target="_blank">
   <img alt='AI assistants can be designed to appear "genuinely good" rather than simply being "forced to be good," and this distinction is important. A new open implementation of character training is being released, which shapes AI personas more robustly than previous methods like prompting or activation steering.' src="assets/slide_2025_45/character-training-ai-genuine-goodness.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/pdf/2511.02824" target="_blank">
   <img alt="Kosmos is an AI scientist designed to automate data-driven scientific discovery by iteratively performing literature searches, hypothesis generation, and data analysis. Unlike previous systems, Kosmos employs a structured world model that allows its data analysis and literature search agents to share information, enabling it to maintain coherent pursuit of open-ended scientific objectives over extended periods (up to 12 hours and 200 cycles). In each run, Kosmos executes thousands of lines of code, analyzes datasets, and reads scientific papers, producing reports in which every claim is cited with code or primary literature for traceability.Evaluation of Kosmos shows that nearly 80% of its report statements are accurate, and its output can match or exceed the research productivity of human scientists over months of work. The number of valuable findings scales linearly with the number of Kosmos cycles, and the system has made both novel and reproducible discoveries in various scientific fields, including metabolomics, materials science, neuroscience, and genetics." src="assets/slide_2025_45/kosmos-an-ai-scientist-for-autonomous-discovery-edison-scientific.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/andrewwhite01/status/1986094948048093389" target="_blank">
   <img alt="A team has developed an AI Scientist capable of running for days and making real scientific discoveries. After two years of development and collaboration with external partners, the AI has produced seven validated findings across various fields.The AI Scientist is now publicly available for anyone to use." class="half-size" src="assets/slide_2025_45/ai-scientist-makes-externally-validated-discoveries.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/rryssf_/status/1986362046469099945" target="_blank">
   <img alt='Microsoft has introduced Kosmos, an AI system designed to function as an autonomous scientist. Unlike traditional AI models that primarily analyze data, Kosmos is capable of generating hypotheses, planning experiments, and refining its understanding based on results, mirroring the scientific process used by human researchers.In initial tests, Kosmos was able to independently rediscover established scientific laws and propose new, previously unknown relationships. This development marks a significant step toward true autonomous discovery in AI, as detailed in the paper "Kosmos: An AI Scientist for Autonomous Discovery" from Microsoft Research (2025).' class="half-size" src="assets/slide_2025_45/kosmos-an-ai-scientist-for-autonomous-discovery-microsoft-research.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/abs/2510.24256" target="_blank">
   <img alt="This preprint investigates how memorization is encoded within transformer models, including both language models (LMs) and vision transformers (ViTs), by analyzing the curvature of the loss landscape. The authors propose a method to decompose model weights based on curvature, enabling the identification and suppression of memorized content without explicit labels. Their weight editing procedure is shown to more effectively reduce recitation of untargeted memorized data compared to existing unlearning methods, while maintaining better model performance.Additionally, the study finds that certain tasks, such as fact retrieval and arithmetic, are negatively impacted by this editing procedure, suggesting these rely on specialized directions in weight space rather than general mechanisms. The work advances understanding of memorization in neural networks and offers practical techniques for removing it, providing evidence that tasks like math and fact retrieval depend on idiosyncratic, narrowly-used model structures." class="half-size" src="assets/slide_2025_45/from-memorization-to-reasoning-in-the-spectrum-of-loss-curvature.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/GoodfireAI/status/1986495330201051246" target="_blank">
   <img alt="LLMs are known to memorize large amounts of their training data, but the mechanisms and extent of this memorization remain unclear. Key questions include where memorized information is stored within the model, how it is represented, and how it affects performance on various tasks.A new paper by Jack Merullo and Srihita Raju explores these issues by analyzing the loss curvature of models. Their research provides insights into the nature of memorization in LLMs and its implications for model behavior and understanding." class="half-size" src="assets/slide_2025_45/where-does-memorization-live-inside-language-models.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/abs/2511.02208" target="_blank">
   <img alt="The paper introduces USERVILLE, an interactive environment with LLM-based user simulators designed for training AI agents to handle diverse and configurable user preferences. The authors propose PPP, a multi-objective reinforcement learning approach that jointly optimizes Productivity (task completion), Proactivity (asking clarifying questions), and Personalization (adapting to user preferences). Their experiments on software engineering and research tasks demonstrate that agents trained with PPP outperform strong baselines like GPT-5, especially in their ability to interact effectively, ask strategic questions, and adapt to unseen user needs.The authors argue that most existing LLM agents focus only on task success and neglect the quality of agent-user interaction, leading to unsatisfactory performance in real-world scenarios. By explicitly optimizing for user-centered interaction, their approach leads to higher user satisfaction and more practical, effective AI agents." class="half-size" src="assets/slide_2025_45/training-proactive-and-personalized-llm-agents-carnegie-mellon-university.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/sunweiwei12/status/1986133618880594403" target="_blank">
   <img alt="The text highlights the limitations of current AI agents in real-world collaboration, emphasizing that productivity alone (like task accuracy) is insufficient. For practical deployment, agents must also be proactive in communication and tailored to individual user preferences.To address this, the authors introduce PPP, an optimization framework that explicitly trains large language model (LLM) agents to be Productive, Proactive, and Personalized. PPP demonstrates significant improvements in complex user scenarios, such as software engineering and deep research, outperforming even GPT-5 when handling initially vague instructions." class="third-size" src="assets/slide_2025_45/ppp-productive-proactive-personalized-llm-agents.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://arxiv.org/abs/2506.08837" target="_blank">
  <img alt="This paper addresses the increasing security challenges posed by prompt injection attacks in AI agents powered by Large Language Models (LLMs). As LLMs become central to software systems and interact with tools or sensitive data, prompt injections—malicious manipulations of natural language inputs—emerge as a critical threat that traditional security frameworks struggle to mitigate.The authors propose a set of principled design patterns for developing AI agents with provable resistance to prompt injection attacks. They systematically analyze these patterns, discuss the trade-offs between utility and security, and demonstrate their applicability through real-world case studies." src="assets/slide_2025_45/design-patterns-for-securing-llm-agents-against-prompt-injections-epfl-ibm-google-microsoft-eth-zuri.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/abs/2511.02886" target="_blank">
   <img alt="This paper discusses advancements in solving the ARC (Abstract Reasoning Challenge) Prize tasks, focusing on the 2025 competition. The leading method, Tiny Recursive Models (TRM), uses a small (7M parameter) recursive neural network trained on augmented ARC tasks. While earlier approaches relied on large, compute-intensive models, TRM achieves competitive results even within strict compute limits by pre-training on public ARC tasks and then efficiently fine-tuning on competition tasks, reaching 6.67% on semi-private evaluation with just 12,500 gradient steps.The paper also reviews the evolution of ARC-solving strategies, noting the shift from brute-force search methods to deep transformers and, more recently, to smaller recursive models. Key innovations include the use of recursive layers and efficient full-model fine-tuning, rather than parameter-efficient methods like LoRA or only tuning task embeddings, enabling better adaptation to new, more difficult ARC AGI II tasks." class="half-size" src="assets/slide_2025_45/test-time-adaptation-of-tiny-recursive-models.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/TrelisResearch/status/1985713050633056681" target="_blank">
   <img alt="A new paper introduces a test-time adaptation approach for Tiny Recursive Models (TRM) as part of the Trelis Submission for the 2025 ARC Prize competition. While the original TRM method doesn't fit the competition's compute constraints, the authors show that pre-trained models can still achieve around 6.67% accuracy on the semi-private test set with full fine-tuning for about 12,500 epochs. Attempts to improve performance by tuning only embeddings or using LoRA plus embeddings were not successful, nor did extended training on larger or harder datasets yield better results.The authors note that the TRM and HRM papers provide the foundational work, with post-training offering only marginal gains. The paper, now open-sourced, details these findings and emphasizes the quality of the original TRM model and hyperparameters. A livestream recap and further discussion are planned, and collaborators and compute sponsors are acknowledged for their contributions." class="half-size" src="assets/slide_2025_45/test-time-adaptation-of-tiny-recursive-models-1.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.anthropic.com/engineering/code-execution-with-mcp" target="_blank">
  <img alt="The article discusses how code execution environments, in conjunction with the Model Context Protocol (MCP), make AI agents more efficient when interacting with external tools and data. Traditionally, loading all tool definitions and piping intermediate results through the model's context window leads to high token usage and increased latency, especially as agents connect to hundreds or thousands of tools. By instead allowing agents to write and execute code to interact with tools—loading only what's needed on demand—developers can dramatically reduce context window usage, lower costs, and minimize latency.Code execution with MCP also enables more advanced workflows: agents can filter, transform, and aggregate data before returning only relevant results, keep sensitive data out of model context for privacy, and persist state or reusable skills for future tasks. While this approach introduces operational complexities such as the need for secure sandboxing, the efficiency and compositional benefits often outweigh these challenges, making it a best practice for scalable and cost-effective agent development." class="half-size" src="assets/slide_2025_45/code-execution-with-mcp-building-more-efficient-agents-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://developers.googleblog.com/en/unlocking-multi-spectral-data-with-gemini/" target="_blank">
  <img alt="Google's Gemini 2.5 model enables developers to analyze multi-spectral satellite imagery—images captured in wavelengths beyond visible RGB, such as near-infrared (NIR) and shortwave infrared (SWIR)—without the need for custom-trained models or complex pipelines. By mapping selected spectral bands into RGB channels and providing clear prompts that explain what each color represents, Gemini can interpret false-color composites and extract valuable insights, such as vegetation health, water detection, burn scars, and material identification.This approach significantly lowers the barrier for remote sensing applications, making it possible to rapidly prototype solutions for environmental monitoring, agriculture, and disaster response with minimal expertise. Developers are encouraged to experiment using public satellite data and the provided Colab notebook, leveraging Gemini’s in-context learning to dynamically adapt to new spectral data and use cases." src="assets/slide_2025_45/unlocking-multi-spectral-data-with-gemini-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://research.google/blog/ds-star-a-state-of-the-art-versatile-data-science-agent/" target="_blank">
  <img alt="DS-STAR is a state-of-the-art autonomous data science agent designed to automate complex tasks such as statistical analysis, visualization, and data wrangling across diverse and heterogeneous data formats. Its core innovations include automatic data file analysis, an LLM-based verification system, and an iterative planning process that refines solutions step-by-step. The DS-STAR framework examines data files, summarizes their contents, and uses multiple agents (Planner, Coder, Verifier, Router) to plan, execute, and verify code solutions, mimicking how an expert analyst would approach data science problems.In extensive benchmarking against existing methods, DS-STAR outperformed alternatives like AutoGen and DA-Agent, notably raising accuracy on the DABStep, KramaBench, and DA-Code benchmarks. Ablation studies confirmed the importance of its individual components and demonstrated its generalizability across different language models. By automating and improving data science workflows, DS-STAR aims to make data science more accessible and drive innovation in various fields." src="assets/slide_2025_45/ds-star-a-state-of-the-art-versatile-data-science-agent-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/" target="_blank">
  <img alt='Google Research has introduced "Nested Learning," a new machine learning paradigm designed to address the issue of "catastrophic forgetting" in continual learning. Unlike traditional approaches that treat model architecture and optimization as separate entities, Nested Learning views models as a set of interconnected, multi-level optimization problems, each with its own update frequency and context flow. This unified perspective enables models to learn and retain knowledge over time, drawing inspiration from the neuroplasticity observed in the human brain.The team demonstrated Nested Learning through a proof-of-concept architecture called "Hope," which features continuum memory systems (CMS) for managing information at multiple timescales and self-modifying capabilities for enhanced in-context learning. Experimental results show that Hope outperforms state-of-the-art models in language modeling, long-context reasoning, and continual learning tasks. The Nested Learning approach offers a promising foundation for building more robust, self-improving AI systems with memory and learning abilities closer to those of humans.' src="assets/slide_2025_45/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning-google-research.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/GoogleAIStudio/status/1986127034800914543" target="_blank">
  <img alt="Based on the provided transcription link, I am unable to access or view the actual content of the image. Please provide the transcribed text content directly, and I will generate a concise summary following your rules." src="assets/slide_2025_45/efficientformervisiontransformersmit.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://blog.google/technology/developers/file-search-gemini-api/" target="_blank">
   <img alt="Google has introduced the File Search Tool in the Gemini API, a fully managed Retrieval-Augmented Generation (RAG) system designed to simplify the process of grounding AI models with custom data. This tool automates file storage, chunking, embedding generation, and dynamic context injection, allowing developers to focus on building applications without managing the retrieval pipeline themselves. File Search supports various formats (like PDF, DOCX, TXT, JSON, and code files), provides built-in citations, and employs vector search using the latest Gemini Embedding model to deliver accurate and relevant responses. Storage and query-time embedding generation are free, with a low fixed cost for initial file indexing.Developers in early access have used File Search for intelligent support bots, internal knowledge assistants, and content discovery platforms. For example, Beam, an AI-driven game platform, significantly accelerated its content search and prototyping using the tool. Getting started involves simple integration with the Gemini API, and comprehensive documentation and demo apps are available for hands-on exploration." src="assets/slide_2025_45/file-search-tool-gemini-api-google.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://www.philschmid.de/gemini-file-search-javascript" target="_blank">
   <img alt="This tutorial explains how to use the Gemini API File Search—a managed Retrieval-Augmented Generation (RAG) feature that lets developers upload, index, and search files efficiently using the Gemini API. The guide covers the full workflow: creating a File Search Store, locating it by display name, uploading multiple files concurrently, customizing chunking and metadata, running RAG queries (including filtered searches), retrieving and managing specific documents, updating (by deletion and re-upload), and cleaning up by deleting stores. Key points include the cost model (free storage/query-time embeddings; $0.15 per million tokens for initial indexing) and the ability to attach custom metadata to documents.Practical code examples in JavaScript/TypeScript illustrate each step using the @google/genai SDK, showing how to automate file management and integrate file-based knowledge into Gemini-powered applications. The File Search API supports advanced features like concurrent uploads, metadata filtering for targeted retrieval, and requires explicit deletion for updates or cleanup." src="assets/slide_2025_45/gemini-api-file-search-philschmid.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://developers.googleblog.com/en/beyond-request-response-architecting-real-time-bidirectional-streaming-multi-agent-system/" target="_blank">
  <img alt="As AI agents grow more sophisticated, the traditional request-response model is proving inadequate for real-time, concurrent, and multimodal interactions. This blog post argues for a shift to a real-time bidirectional streaming architecture for multi-agent systems, which enables true concurrency, natural interruptibility, proactive tool usage, and seamless multimodal processing. Such an architecture lets agents behave more like collaborative partners, able to process and respond to input continuously rather than in rigid turns.Building these systems introduces new engineering challenges, including context and session management without clear interaction boundaries, high-concurrency performance, and developer-friendly abstractions. Google's open-source Agent Development Kit (ADK) addresses these with asynchronous I/O management, stateful transferable sessions, event-driven callbacks, and streaming-native tools. This framework allows for robust, extensible, real-time multi-agent applications and lays the foundation for further research into even more interactive and performant AI agentic systems." src="assets/slide_2025_45/beyond-request-response-architecting-real-time-bidirectional-streaming-multi-agent-system-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://developers.googleblog.com/en/announcing-user-simulation-in-adk-evaluation/" target="_blank">
  <img alt='Google has introduced a new "User Simulation" feature in the Agent Development Kit (ADK) to streamline and strengthen the testing of conversational AI agents. Instead of relying on brittle, turn-by-turn test scripts, developers can now define high-level goals using a JSON-based ConversationScenario. The LLM-powered user simulator dynamically generates user prompts to achieve these goals, making tests more resilient to changes in agent behavior and reducing maintenance overhead. The simulator can be configured with various models, behaviors, and turn budgets, and supports persona customization for diverse user types.This release enables faster test creation, more robust regression suites, and a focus on intent rather than specific conversational flows. Developers can configure and run evaluations locally, receiving detailed breakdowns of interactions and metrics like hallucination rates. The feature is aimed at making AI agent testing more efficient and reliable, with supporting documentation and tutorials available for immediate adoption.' src="assets/slide_2025_45/announcing-user-simulation-in-adk-evaluation-google.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_45/magentic-marketplace-open-source-simulation-agentic-market-design.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/" target="_blank">
  <img alt="Microsoft Research introduces Magentic Marketplace, an open-source simulation environment designed to study agentic markets where autonomous AI agents interact as buyers and sellers. The platform models complex market dynamics by enabling large-scale, reproducible experiments involving hundreds of customer and business agents. Experiments reveal that advanced AI agents can improve consumer welfare by reducing information asymmetries, but their effectiveness depends on robust discovery mechanisms. The research highlights challenges including agent vulnerability to manipulation, the paradox of choice (where more options can reduce welfare), and systemic biases in agent decision-making.Magentic Marketplace supports modular extensions for future research and provides comprehensive documentation and datasets for the community. Key findings show that even state-of-the-art models are susceptible to unfair biases and manipulation, emphasizing the need for oversight and iterative development before wide-scale deployment. The platform is available on GitHub and Azure AI Foundry Labs, with related publications and further resources provided for researchers interested in agentic market dynamics." src="assets/slide_2025_45/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets-microsoft.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://codecut.ai/deep-dive-into-duckdb-data-scientists/" target="_blank">
  <img alt="DuckDB is an embedded, in-process SQL OLAP database designed for fast, memory-efficient analytics, especially useful for data scientists. Unlike traditional databases that require server setup, DuckDB allows direct SQL querying on local files and DataFrames with zero configuration. It integrates seamlessly with pandas and Polars, making it easy to combine familiar data manipulation workflows with high-performance SQL operations. DuckDB manages large datasets efficiently by only loading necessary data into memory and outperforms pandas in complex operations like joins and aggregations due to its vectorized execution engine.The article demonstrates DuckDB’s capabilities, including automatic parsing and flattening of CSV, Parquet, and JSON files, reading from multiple files and sources in a single query, support for parameterized queries to enhance security and reusability, and ACID transaction support for data integrity. Additionally, DuckDB is extensible, offering features like full-text search through extensions. These strengths make DuckDB a powerful tool for scalable, reliable, and efficient data science workflows." src="assets/slide_2025_45/duckdb-for-data-scientists-guide.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/bu7emba/status/1985775324387250649" target="_blank">
  <img alt="Snowflake has announced an integration or usage of DuckDB, a fast, in-process analytical database. This move has surprised some in the data community, given the two technologies' distinct roles and architectures.The integration suggests Snowflake is exploring ways to leverage DuckDB's capabilities, potentially enhancing analytical workflows or performance within its platform. This development could signal broader industry recognition of DuckDB's strengths." src="assets/slide_2025_45/snowflake-uses-duckdb-snowflake.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://cursor.com/blog/semsearch" target="_blank">
  <img alt="Cursor’s coding agent uses semantic search to improve its ability to retrieve relevant code segments based on natural language queries, outperforming traditional keyword-based tools like grep. By training a custom embedding model and building dedicated indexing pipelines, the agent achieves notably higher accuracy, increased code retention, and requires fewer user iterations to produce correct solutions—especially in large codebases.Evaluations, including offline benchmarks and online A/B tests, show that semantic search consistently leads to better outcomes across multiple metrics. The custom embedding model is trained using agent session traces and LLM-generated rankings, allowing the agent to learn from real usage patterns. The combination of semantic and traditional search tools yields the best results, making semantic search essential for optimal agent performance." src="assets/slide_2025_45/improving-agent-with-semantic-search-cursor.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://towardsdatascience.com/the-handbook-of-reinforcement-learning-guide-to-the-foundational-questions/" target="_blank">
  <img alt="This article provides a comprehensive yet accessible introduction to the foundational concepts of Reinforcement Learning (RL). It explains how RL involves an agent interacting with an environment, collecting experiences through exploration, and using these experiences to train a policy—a strategy that maps observations (states) to actions. Key ideas include the distinction between the agent, policy, and environment, the nature of observations and actions, and the crucial balance between exploration and exploitation. The article also clarifies discrete vs. continuous action spaces and highlights common exploration strategies like epsilon-greedy and curiosity-driven methods.The discussion covers major RL algorithm types, contrasting model-based (where the agent builds an internal model of the environment) and model-free methods (where the agent learns directly from experiences). It breaks down value-based approaches (like Q-learning and Deep Q Networks), policy-based methods (like REINFORCE and policy gradients), and hybrid actor-critic algorithms (such as A2C and PPO), explaining their update mechanisms and trade-offs between bias and variance. The piece emphasizes understanding these core choices to navigate RL research and build effective agents." src="assets/slide_2025_45/the-reinforcement-learning-handbook-guide-to-foundational-questions-towardsdatascience.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://magazine.sebastianraschka.com/p/beyond-standard-llms?utm_campaign=post&amp;utm_medium=web" target="_blank">
  <img alt="This article by Sebastian Raschka provides a comprehensive overview of recent alternatives to standard large language models (LLMs) beyond classic autoregressive, decoder-style transformers. It discusses the rise of linear attention hybrids (such as Qwen3-Next and Kimi Linear) that combine efficient, recurrent-inspired mechanisms with traditional attention to reduce memory and computation costs for long contexts, as well as text diffusion models that generate text through parallel, denoising steps rather than sequential token prediction. The article also covers world models—particularly code world models—which enhance LLMs by teaching them to simulate program execution, and explores small, recursive transformer architectures like the Hierarchical Reasoning Model (HRM) and Tiny Recursive Model (TRM) that use iterative self-refinement to achieve strong reasoning on specialized tasks with minimal parameters.In summary, while standard transformer-based LLMs remain the mainstream choice for general-purpose applications due to their maturity and performance, there is a vibrant ecosystem of alternative architectures aiming to improve efficiency, reasoning, or domain specialization. Linear attention hybrids promise significant speed and memory gains, diffusion models offer fresh approaches to text generation, code world models push forward code understanding, and tiny recursive transformers excel at task-specific reasoning. The article encourages ongoing exploration of these novel directions, suggesting they may complement or even surpass current LLM paradigms in certain contexts." src="assets/slide_2025_45/beyond-standard-llms-ahead-of-ai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://booking.ai/llm-evaluation-practical-tips-at-booking-com-1b038a0d6662" target="_blank">
  <img alt='This article from Booking.com Data Science outlines practical methods for evaluating Large Language Model (LLM) applications, focusing on the "LLM-as-a-judge" framework. It discusses the unique challenges of assessing generative AI outputs, such as hallucinations, instruction-following failures, and the absence of clear ground truth. The core solution involves creating a high-quality, representative "golden dataset" with rigorously annotated labels, which enables a powerful LLM (the judge) to reliably automate evaluation of target LLM outputs. The article provides detailed protocols for dataset creation (basic and advanced), guidance on prompt engineering for judge-LLMs, and describes monitoring systems for production environments.Future directions include exploring comparative (pairwise) judges for stronger ranking signals, automating prompt engineering and synthetic data generation to streamline annotation, and extending evaluation methodologies for complex LLM-based agents. The key takeaway is that robust, scalable LLM evaluation requires both high-quality data and careful protocol design, with automation and advanced judging techniques offering promising avenues for further improvement.' src="assets/slide_2025_45/llm-evaluation-practical-tips-at-booking-com-booking.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://code.visualstudio.com/blogs/2025/11/03/unified-agent-experience" target="_blank">
  <img alt='The article announces a unified agent experience in Visual Studio Code (VS Code), enabling developers to manage and orchestrate multiple coding agents—including GitHub Copilot, Copilot coding agent (cloud), Copilot CLI, and OpenAI Codex—within a single interface called Agent Sessions. This new sidebar view allows users to see, manage, and delegate tasks across all agents, whether local or remote, streamlining workflow and providing a "mission control" for coding assistance. The integration of OpenAI Codex into Copilot Pro+ subscriptions further expands agent choice without the need for additional accounts.VS Code also introduces new features for customizing agent behavior. "Planning Agent" (formerly chat modes) helps break down vague prompts into actionable plans, asks clarifying questions, and supports handoff to implementation. The platform supports creating custom agents and introduces "subagents," which run in isolated contexts for specific tasks, helping manage prompt context and complexity. These advancements aim to reduce fragmentation in the agent ecosystem and empower developers with flexible, multi-agent workflows.' src="assets/slide_2025_45/a-unified-experience-for-all-coding-agents-visual-studio-code.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://pair.withgoogle.com/explorables/sae/" target="_blank">
  <img alt="Sparse Autoencoders (SAEs) are a promising technique to enhance the interpretability of large language models (LLMs) by disentangling complex, polysemantic activations into sparse, monosemantic features. By training an autoencoder with a sparsity penalty, researchers can create larger latent spaces that ideally represent individual concepts or features, making it easier to understand and label what each neuron in the model is doing. This process enables the extraction of meaningful feature labels via autolabelers, which analyze the inputs that most activate each latent neuron and use LLMs to suggest descriptive labels.With these labeled features, SAEs allow for the steering of model behavior by adjusting the activation of specific concepts before reinserting them into the original model, thus directly influencing its responses. Additionally, mapping these features provides a global view of the model’s internal representations, revealing how features cluster and what domains the model is most specialized in. SAEs, therefore, offer valuable tools for both interpreting and controlling LLMs, advancing our understanding of their reasoning processes and enabling more targeted interventions." src="assets/slide_2025_45/mapping-llms-with-sparse-autoencoders-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.amazon.science/blog/demystifying-agents?utm_campaign=demystifying-agents&amp;utm_medium=organic-asw&amp;utm_source=twitter&amp;utm_content=2025-11-05-demystifying-agents&amp;utm_term=2025-november" target="_blank">
  <img alt="Amazon Science's article &quot;Demystifying AI agents&quot; by Marc Brooker explains the fundamentals of agentic AI systems, focusing on how autonomous agents use large language models (LLMs) and tools to achieve complex goals on behalf of users. Brooker introduces AWS's AgentCore framework, detailing its key components: agent development, AI model hosting, agentic code execution, tool call management, short-term and long-term memory, and system observability. He highlights best practices such as chain-of-thought reasoning and the ReAct (reasoning + action) loop, and discusses infrastructure innovations like Firecracker microVMs for secure and efficient runtime isolation.The article emphasizes AgentCore’s flexibility, supporting various development frameworks and protocols, and its ability to manage agent interactions, memory, and tracing for performance evaluation. Brooker also notes the rapid evolution of agentic AI, the importance of combining LLM orchestration with imperative code, and provides guidance for developers interested in building their own AI agents using AWS’s tools and services." src="assets/slide_2025_45/demystifying-ai-agents-amazon.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.camel-ai.org/blogs/brainwash-your-agent-how-we-keep-the-memory-clean" target="_blank">
  <img alt="This blog post from the CAMEL team explores three key context engineering techniques for improving the memory management of AI agents: Context Summarization, Workflow Memory, and Tool Output Caching. Context Summarization condenses conversations to their most critical components, reducing token bloat and keeping the agent focused on its main tasks. Workflow Memory captures generalized strategies and lessons learned from previous tasks, enabling agents to perform better on similar future tasks without redundant trial and error. Tool Output Caching aims to store large, often unnecessary tool outputs outside of the agent's main context, referencing them only when needed to save tokens, though this approach requires careful handling to avoid information loss and increased agent complexity.The post emphasizes that optimizing an agent's context is crucial for both performance and cost efficiency, often yielding significant improvements without modifying the underlying LLM. The CAMEL team shares practical implementations, highlights ongoing research, and invites contributions to further enhance context management, underscoring the creative potential in this evolving field." class="half-size" src="assets/slide_2025_45/brainwash-your-agent-how-we-keep-the-memory-clean.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/MiniMax__AI/status/1985375617622454566" target="_blank">
  <img alt="Sure! However, I am unable to access the content of the provided link directly. If you could provide the text content or a transcription from the image, I can generate a concise summary following your rules. Please paste the relevant text here, and I'll be happy to help!" class="half-size" src="assets/slide_2025_45/a-generative-model-for-protein-design-with-explicit-function-control-mit.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/jeremyphoward/status/1987394259130241437" target="_blank">
   <img alt="Blocking Chinese firms from accessing top NVIDIA GPUs has led to model developers optimizing AI models for older and more affordable hardware. The latest state-of-the-art model from @Kimi_Moonshot is designed to run efficiently on GPUs using standard BF16 operations after dequantizing from INT4, removing the need for costly FP4 support.This development enables broader accessibility and cost-effectiveness for advanced AI models, as high performance can now be achieved without relying on the most expensive GPU technologies." class="half-size" src="assets/slide_2025_45/kimi-moonshot-model-optimized-for-older-nvidia-gpus.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/ZhihuFrontier/status/1987125624599970218" target="_blank">
   <img alt="Quantization, specifically native INT4 quantization, is presented as a new paradigm for large language model (LLM) training and inference, rather than a simple compromise between speed and precision. The Kimi-K2 model leverages weight-only QAT (Quantization-Aware Training) over PTQ (Post-Training Quantization) to achieve high throughput and low latency without sacrificing quality, especially for long-context reasoning and memory-bound MoE architectures. This approach allows for near-lossless results, more stable long-context performance, and significant speedups in both inference and RL (Reinforcement Learning) training stages.Kimi chose INT4 over other formats like MXFP4/NVFP4 due to better hardware adaptability and strong kernel support, making it suitable for a wider range of GPUs. The quantization strategy is set to evolve further with even lower-bit formats as hardware advances, positioning quantization as a core enabler for rapid LLM development and deployment." class="half-size" src="assets/slide_2025_45/int4-quantization-next-paradigm-llms-kimi.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/rohit4verse/status/1986106051893367195" target="_blank">
  <img alt="TOON is a token-optimized alternative to JSON, designed specifically for transmitting flat, tabular data to Large Language Models (LLMs). Unlike JSON, which repeats keys and uses verbose syntax, TOON declares field keys once in a header and streams rows in a compact, indentation-based format, reducing token usage by up to 60% for uniform arrays. However, TOON loses efficiency and defaults to a YAML-like structure for nested or non-uniform data, making JSON better suited for complex or deeply nested objects.The main advantages of TOON are lower LLM API costs, faster prompt processing, and the ability to include more tabular data within the LLM context window. TOON is best used for LLM prompts containing large, simple tables or uniform arrays of objects, with nested data ideally flattened before conversion. Despite TOON's benefits for specific cases, JSON remains essential for handling complex data and is not obsolete for LLM calls." class="half-size" src="assets/slide_2025_45/json-vs-toon-efficient-llm-tabular-data-format.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/kwindla/status/1986952416848454012" target="_blank">
  <img alt="The meetup will focus on emerging patterns for building advanced voice agents, moving beyond simple LLM prompt loops. Key techniques include state machines, multi-agent systems, combining fast and thoughtful models, guardrails, and memory sub-systems. One highlighted pattern involves initiating long-running tasks via tool calls, returning immediate success/failure, and injecting events into the agent’s context while the task runs. This allows for interleaving user speech with event metadata, formatted (often in XML) for better LLM comprehension, though it increases the risk of hallucinations.Best practices include prompting LLMs to understand event structures, providing few-shot examples, and having context managers decide when to trigger inference. Mixing structured data with user input in conversations can make hallucinations more common, especially compared to tool response messages, but tool messages aren't suitable for ongoing tasks. The discussion encourages participation from those working on realtime, multimodal AI systems." src="assets/slide_2025_45/voice-agent-patterns-long-running-tasks-event-injection.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://kalomaze.bearblog.dev/rl-lora-ddd/" target="_blank">
  <img alt="This blog post by kalomaze explores the practical use of LoRA (Low-Rank Adaptation) for reinforcement learning finetuning in the prime-rl library, comparing its effectiveness to full finetuning (FFT) across several custom RL environments. The author details how LoRA can be flexibly applied to different model components, explains the impact of LoRA rank and alpha scaling (including rsLoRA normalization), and presents experimental results on tasks ranging from simple sorting to complex reasoning. Findings show that LoRA provides significant memory savings and competitive performance, especially for tasks with less information-rich signals, but higher ranks may be needed for more semantically demanding tasks.The experiments demonstrate that LoRA’s efficiency and adaptability make it especially suitable for RL with verifiable rewards (RLVR), while the relationship between rank, alpha, and learning performance varies across environments. The post concludes with prime-rl’s ongoing commitment to advancing LoRA support, inviting contributions from the research and developer community." src="assets/slide_2025_45/rl-learning-with-lora-a-diverse-deep-dive-primeintellect.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/ben_burtenshaw/status/1986862009304904132" target="_blank">
  <img alt="Agentic RL environments from OpenEnv are being explored, with various example implementations provided for users. These examples utilize the OpenAI inference client, enabling compatibility with any preferred model, including open models.The author has submitted pull requests containing inference examples, making it easier for others to try out these environments. A step-by-step walkthrough of the examples will be provided in the discussion thread." src="assets/slide_2025_45/agentic-rl-environment-examples-with-openenv-and-openai-inference.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/boringmarketer/status/1986167860356379063" target="_blank">
  <img alt="This guide shares 15 actionable habits to dramatically reduce costs and increase efficiency when using Claude for coding tasks. Key strategies include relying on the haiku model for most tasks due to its lower cost, using targeted file searches and reading only necessary file segments, and handling tasks in parallel. Other tips emphasize leveraging built-in agents for code exploration, planning major changes before execution, and automating budget alerts and path shortcuts.Additional recommendations focus on being specific in requests, limiting search results, creating task checklists, and efficiently using session memory. The underlying theme is to automate and optimize your workflow—minimizing unnecessary interactions, reducing redundant operations, and making smart use of Claude’s features to maximize both speed and savings." src="assets/slide_2025_45/claude-code-habits-to-save-money-and-boost-efficiency.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/ttorres/status/1986134931781599603" target="_blank">
  <img alt="Claude Code can be enhanced with a memory system that retains essential business details, preferences, and workflows across conversations, eliminating the need to repeat information. The article outlines a three-layer memory system: global preferences that load in every session, project-specific instructions for different tasks, and reference context files that can be customized as needed.Step-by-step guidance is provided for creating and managing these context files, allowing users to organize and update business information efficiently. This transforms Claude from a simple assistant into a reliable advisor tailored to individual business needs." class="half-size" src="assets/slide_2025_45/how-to-give-claude-memory-three-layer-ai-context-system.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/alxfazio/status/1986814927072305503" target="_blank">
  <img alt="Anthropic recommends a new approach to Model-Callable Programs (MCPs) that avoids loading all tool definitions into the model prompt. Instead, MCP servers should be treated like standard code libraries, allowing agents to dynamically discover and import only the necessary tools and functions as needed.This method enables local data processing, with only essential results sent back to the model, leading to significant token savings. Anthropic demonstrates that this approach can reduce token usage from around 150,000 to just 2,000 in typical workflows, a savings of nearly 99%." class="half-size" src="assets/slide_2025_45/efficient-mcp-architecture-token-savings-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/simonw/status/1987329464695726197" target="_blank">
  <img alt="MCP (Matrix Communication Protocol) utilizes the Dynamic Client Registration feature in OAuth, enabling clients to register with servers dynamically. This marks the first instance of this OAuth feature being adopted by widely used software.The referenced post highlights the significance of this implementation in enhancing flexibility and interoperability for client-server communication within secure authentication frameworks." src="assets/slide_2025_45/dynamic-client-registration-oauth-mcp.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/dani_avila7/status/1986181263708070313" target="_blank">
   <img alt="The post discusses the differences between CLI tools and MCP (presumably a dynamic tool and resource discovery platform) in the context of AI agents. CLI tools are favored by agents due to their inclusion in pre-training, but can become outdated as tools evolve, requiring frequent use of --help to stay current.MCP, on the other hand, offers dynamic discovery of available tools and resources, ensuring up-to-date capabilities. The author suggests using CLI for standard Unix commands and MCP for more dynamic or external needs, and asks the community for their experiences or if any important considerations are missing." class="half-size" src="assets/slide_2025_45/cli-vs-mcp-differences-use-cases-agent-tool-discovery.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/dani_avila7/status/1986065188387266987" target="_blank">
   <img alt="The user compares using CLI (Command Line Interface) and MCP (Managed Control Panel) within Claude Code, finding that CLI is superior for their workflow. They maintain a list of installed tools in a markdown file and leverage Claude to interact with these tools for tasks like checking logs and adjusting configurations.According to the user, the CLI offers simpler setup, native authentication, and better observability compared to MCP. Despite ongoing debate, the user strongly prefers the CLI for its efficiency and transparency." class="half-size" src="assets/slide_2025_45/cli-vs-mcp-claude-workflow-comparison.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://mariozechner.at/posts/2025-11-02-what-if-you-dont-need-mcp/?t=0" target="_blank">
   <img alt="The article argues that for many agentic coding and browser automation tasks, you don't need heavy, generic MCP (Machine Control Protocol) servers like Playwright MCP or Chrome DevTools MCP, which consume significant context and are hard to extend or compose. Instead, it demonstrates how you can use a minimal set of custom Bash/Node.js scripts (leveraging Puppeteer Core) to start a browser, navigate, execute JavaScript, take screenshots, and add specialized tools like DOM pickers or cookie extractors. These scripts are easy for coding agents to understand and use, require far fewer tokens, and are highly composable and customizable.By organizing these lightweight CLI tools in a structured directory, referencing their concise README, and making them globally available in agent sessions, users can efficiently extend and reuse their automation capabilities across different agents. This approach yields greater flexibility, efficiency, and maintainability compared to using monolithic MCP servers, especially when agents are already skilled at using Bash and writing code." src="assets/slide_2025_45/one-chonky-mcp-server.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/_philschmid/status/1985363147071386048" target="_blank">
  <img alt="The Gemini Docs MCP Server is a local STDIO server that enables efficient searching and retrieval of Google Gemini API documentation. It runs directly via uvx without installation, uses a local SQLite database with FTS5 for fast full-text search, and supports integration with various tools such as @code, @cursor_ai, and the Gemini CLI.The server offers three main tools—search_documentation, get_capability_page, and get_current_model—and has been tested successfully with the latest Python and Typescript SDKs and models. It aims to streamline development by providing quick access to up-to-date documentation locally." class="half-size" src="assets/slide_2025_45/gemini-docs-mcp-server-google.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_45/agile-and-cooperative-aerial-manipulation-of-a-cable-suspended-load.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/kimmonismus/status/1985312873636340151" target="_blank">
  <img alt="Agile and cooperative aerial manipulation refers to the advanced coordination of multiple aerial robots (such as drones) to control and maneuver a load suspended by a cable. This technology enables precise and flexible handling of objects in mid-air, allowing for complex transportation and placement tasks.The linked content likely showcases a demonstration or research achievement highlighting the effectiveness of this approach. Such innovations have significant potential for applications in logistics, construction, and disaster response, where remote or difficult-to-reach objects need to be transported safely and efficiently." src="assets/slide_2025_45/agile-and-cooperative-aerial-manipulation-of-a-cable-suspended-load.webp"/>
 </a>
</section>


      <section>
        <div class="fragment fade-out" data-fragment-index="-1"
          style="align-items: center; justify-content: center; min-height: 70vh; margin: 0; font-size: 50%; margin: auto; display: flex; flex-direction: column;">
          <div style="margin-top: 10px; text-align:center;">
            <img src="assets/qr.png" height="100px" /> <br/>
            📚 <a href="index_all.html"><b>view all previous issues</b></a><br/>
            ✨ see you next week!
          </div>
        </div>
      </section>

    </div>
  </div>

  <script>
      let deck = new Reveal();
      deck.initialize({controls: true, progress: false, hash: true, plugins: [RevealScrollingImage]});
  </script>
</body>
</html>