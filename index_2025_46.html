<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="initial-scale=1.0">
    <meta name="description" content="" />
    <meta property="og:title" content="" />
    <meta property="og:description" content="" />
    <meta property="og:image" content="" />

    <title>ML NEWS / 2025 / 46th week</title>
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/night.css" />
    <link rel="stylesheet" href="dist/slider.css" />
    <link rel="alternate" type="text/html" href="index_2025_46.html?print-pdf" title="Printer-friendly version">

</head>
<body>
  <script src="dist/reveal.js"></script>
  <script src="dist/scrolling-image.js"></script>

  <div class="reveal">
    <div class="slides">
      <!-- PRODUCTS -->

      <!-- PAPERS -->

      <!-- OTHER -->

<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://openai.com/index/gpt-5-1/" target="_blank">
   <img alt="OpenAI has announced the rollout of GPT-5.1, an upgrade to the GPT-5 series focused on making ChatGPT smarter, more conversational, and easier to personalize. This release introduces two models: GPT-5.1 Instant, which is warmer and more conversational by default, and GPT-5.1 Thinking, which adapts its reasoning time to better handle complex questions while responding faster to simple ones. Both models feature improved instruction-following, clearer explanations, and more empathetic communication. GPT-5.1 Auto will route queries to the most suitable model, and the update will gradually become the default for all users, starting with paid subscribers.Additionally, OpenAI is enhancing ChatGPT’s customization options, allowing users to more intuitively set the tone and style of responses with new preset personalities such as Professional, Candid, and Quirky, alongside improved controls for conciseness, warmth, and emoji use. These updates apply universally across models and conversations, with further fine-tuning features rolling out experimentally. GPT-5.1 models will soon be available through the API, and legacy GPT-5 models will remain accessible for a limited transition period." src="assets/slide_2025_46/gpt-5-1-a-smarter-more-conversational-chatgpt-openai.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://openai.com/index/gpt-5-1-for-developers/" target="_blank">
   <img alt="GPT-5.1 is OpenAI's latest model in the GPT-5 series, designed for developers seeking enhanced intelligence, speed, and efficiency in agentic and coding tasks. The model introduces adaptive reasoning, dynamically adjusting its &quot;thinking&quot; time based on task complexity, resulting in faster, more token-efficient responses for simple tasks and persistent reliability for complex ones. A new &quot;no reasoning&quot; mode allows latency-sensitive use cases to benefit from high intelligence and improved tool-calling, while extended prompt caching enables up to 24-hour cache retention for lower costs and smoother multi-turn interactions. Coding capabilities are bolstered by a more steerable personality, better code quality, and improved communication during tool use.GPT-5.1 also debuts two new tools: `apply_patch` for reliable codebase edits through structured diffs, and a shell tool for controlled command-line interaction on local machines. The model is available to all paid API tiers with pricing matching GPT-5, alongside specialized `gpt-5.1-codex` models for long-running coding tasks. Benchmarks show notable improvements in reasoning and coding accuracy over previous versions. OpenAI plans continued investment in reliable, efficient agentic and coding models, supporting developers with advanced workflows and flexible control over speed, cost, and intelligence." src="assets/slide_2025_46/introducing-gpt-5-1-for-developers-openai.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/btibor91/status/1989262920987836618" target="_blank">
   <img alt="OpenAI’s recent AMA discussed ongoing improvements in GPT-5.1, focusing on more precise guardrails and age verification to better protect users, especially minors, while minimizing unnecessary restrictions. Efforts are underway to train models to responsibly use user context for safeguards, but complete precision remains a complex research challenge. GPT-5.1 shows notable advancements in coding, instruction following, and response speed, with a knowledge cutoff of September 2024. Work continues on reducing outdated or incorrect answers, and new developer features have been added, such as longer prompt caching, faster response modes, and expanded API parameters.On the customization front, GPT-5.1 offers improved support for custom instructions and is testing personality trait adjustments, validated to not affect core capabilities. Upcoming features include GPT-5.1 Pro, enhancements to multimodality, voice mode, and better integration with productivity tools like Word and Excel. The team is also committed to maintaining and improving the Thinking Mini model and increasing limits for paid users, though specific timelines remain unconfirmed." src="assets/slide_2025_46/gpt-5-1-reddit-ama-summary-openai.png"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://claude.com/blog/structured-outputs-on-the-claude-developer-platform" target="_blank">
   <img alt="The Claude Developer Platform now offers structured outputs for Claude Sonnet 4.5 and Opus 4.1, available in public beta. This feature ensures API responses consistently match specified JSON schemas or tool definitions, reducing parsing errors and failed tool calls. Structured outputs can be leveraged for tasks such as reliable data extraction, multi-agent architectures, and complex search tools, allowing developers to build dependable applications without complex error handling.Developers can implement structured outputs via direct JSON schema definitions or tool specifications, with support for Haiku 4.5 coming soon. This enhancement streamlines workflows by guaranteeing response accuracy, minimizes retries, and simplifies codebases. Notably, OpenRouter highlights improved agent reliability and developer productivity using this feature. Documentation is available for schema types, implementation examples, and best practices." src="assets/slide_2025_46/structured-outputs-on-the-claude-developer-platform-anthropic.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/alexalbert__/status/1989409186674098595" target="_blank">
   <img alt="We have introduced structured outputs in the Claude API, allowing developers to ensure that Claude's responses consistently conform to predefined JSON schemas or custom tool specifications.This update eliminates the need for retries or manual parsing, streamlining the integration of Claude's outputs into various applications." class="half-size" src="assets/slide_2025_46/claude-api-structured-outputs-launch.png"/>
  </a>
 </div>
</section>
<section data-background-video="assets/slide_2025_46/deep-research-announcement-notion.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/NotebookLM/status/1989078069454270649" target="_blank">
   <img alt="Deep Research is a new feature that browses hundreds of websites to create an organized report on a given topic. It also provides an annotated list of sources for further exploration.Users can easily add these sources directly to their notebook, streamlining the research process. The feature is currently rolling out." src="assets/slide_2025_46/deep-research-announcement-notion.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/NotebookLM/status/1989005388969095495" target="_blank">
   <img alt="Rolling out today, users can now create custom video overview styles by entering a prompt in the customization box. This new feature allows for greater personalization of video presentations.The announcement encourages users to share their favorite examples, with high expectations for creativity and innovation." src="assets/slide_2025_46/custom-video-overview-style-update.png"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://developers.googleblog.com/en/google-colab-is-coming-to-vs-code/" target="_blank">
  <img alt="Google has announced the official release of the Google Colab extension for Visual Studio Code (VS Code). This extension allows users to connect local VS Code notebooks to Colab runtimes, including access to Pro-tier GPUs and TPUs, enabling seamless integration between the popular code editor and Colab’s cloud-based compute resources. The goal is to bridge the gap between the flexibility of VS Code and the simplicity and power of Colab, making it easier for AI/ML developers, students, and researchers to work within their preferred environments.To get started, users simply install the Colab extension from the VS Code marketplace, open a notebook, and connect to a Colab runtime by selecting the appropriate kernel. The extension also supports VS Code derivatives via Open VSX. Google plans to expand the extension’s features in the future and encourages feedback through GitHub and social channels." src="assets/slide_2025_46/google-colab-is-coming-to-vs-code-google.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://cloud.google.com/blog/products/ai-machine-learning/more-ways-to-build-and-scale-ai-agents-with-vertex-ai-agent-builder?e=48754805?utm_source=twitter&amp;utm_medium=unpaidsoc&amp;utm_campaign=fy25q4-googlecloud-blog-ai-in_feed-no-brand-global&amp;utm_content=-&amp;utm_term=-" target="_blank">
  <img alt="Google Cloud has announced significant updates to Vertex AI Agent Builder, a comprehensive platform for building, scaling, and governing AI agents. Key improvements include enhanced context management through configurable context layers, expanded language support with a new Go SDK, and simplified single-command deployment via the Agent Development Kit (ADK). The platform now features new managed services in Agent Engine (AE), offering observability tools, evaluation capabilities with a user simulator, and easier access without a Google Cloud account.To ensure secure and responsible agent operations, Vertex AI Agent Builder introduces native agent identities as IAM principals and advanced security features like Model Armor for input risk protection and integration with Security Command Center. Customers such as Color Health, PayPal, and Geotab are leveraging these capabilities to rapidly deploy scalable, production-ready AI agents. Developers can get started with curated samples, streamlined deployment, and in-depth documentation to accelerate their AI agent journey from prototype to production." src="assets/slide_2025_46/ai-agents-with-vertex-ai-agent-builder-google-cloud.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/sima-2-an-agent-that-plays-reasons-and-learns-in-3d-worlds-googledeepmind.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://deepmind.google/blog/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds/" target="_blank">
   <img alt="Google DeepMind has introduced SIMA 2, an advanced AI agent designed for interactive, instruction-based gameplay in virtual 3D environments. Building on the foundation of the original SIMA, SIMA 2 leverages Gemini models to move beyond basic instruction following, enabling reasoning, goal-oriented planning, and natural conversation with users. The agent demonstrates significant improvements in generalization, successfully carrying out complex and nuanced tasks—even in games and worlds it has never seen before. It supports multimodal prompts (like sketches), multiple languages, and can interpret emojis, showing human-like adaptability.A key innovation in SIMA 2 is its capacity for self-improvement. After initial training with human demonstrations, the agent can refine its skills through self-directed play and feedback generated by Gemini, even in newly created environments. SIMA 2 marks a substantial step toward general embodied intelligence and has potential applications in both gaming and robotics. Currently released as a limited research preview, Google DeepMind emphasizes responsible development and collaboration with the research and developer community to further explore and refine this technology." src="assets/slide_2025_46/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds-google-deepmind.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/_philschmid/status/1988995630123844064" target="_blank">
   <img alt="SIMA 2, developed by Google DeepMind, is a Gemini-powered AI agent designed to interact, reason, and learn within 3D virtual worlds. It can understand instructions, converse with users, and explain its actions, all while controlling in-game characters using standard keyboard and mouse inputs.The agent demonstrates the ability to generalize its skills to new, unseen games and environments, including those generated by Genie 3. It autonomously improves its performance without human feedback and has successfully navigated diverse environments like ASKA and MineDojo." src="assets/slide_2025_46/sima-2-an-agent-that-plays-reasons-and-learns-in-3d-worlds-googledeepmind.png"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/Baidu_Inc/status/1988820837898829918" target="_blank">
   <img alt="ERNIE 5.0 is a newly introduced foundational model designed for omni-modal capabilities, excelling in understanding, creative writing, and instruction following across multiple modalities.The developers commit to ongoing investment and innovation in advanced models to further expand the limits of artificial intelligence." class="half-size" src="assets/slide_2025_46/ernie-5-0-omni-modal-foundational-model-baidu.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/ZhihuFrontier/status/1989735217960223007" target="_blank">
   <img alt="Baidu has released ERNIE 5.0, which shows significant improvements over its predecessor, X1.1, with performance gains of about 80% and cleaner, more coherent outputs. The model excels in instruction following and basic computation, producing clearer answers and improved readability compared to earlier versions.However, ERNIE 5.0 still struggles with high hallucination rates, low insightfulness, occasional infinite loops, and weak multi-turn conversation abilities. While it marks a substantial step forward for Baidu in the LLM race, it remains behind top domestic competitors like Kimi K2 in some areas." class="half-size" src="assets/slide_2025_46/ernie-5-0-review-baidu.png"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/OpenHandsDev/status/1988438946192519636" target="_blank">
  <img alt="ByteDance has launched Doubao-Seed-Code, which has achieved impressive results on the SWE-Bench and Multi-SWE-Bench benchmarks using the OpenHands framework. The release has garnered positive attention from the tech community.There is anticipation and interest in testing out Doubao-Seed-Code firsthand." class="third-size" src="assets/slide_2025_46/doubao-seed-code-achieves-competitive-swe-bench-scores-bytedance.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/depth-anything-3-recovering-the-visual-space-from-any-views.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://arxiv.org/abs/2511.10647" target="_blank">
  <img alt="Depth Anything 3 (DA3) is a new model designed to recover spatially consistent geometry from any number of visual inputs, regardless of whether camera poses are known. The approach simplifies the architecture by using a plain transformer backbone and a single depth-ray prediction target, avoiding complex multi-task learning. DA3 is trained using a teacher-student paradigm and achieves high detail and generalization, outperforming previous models like Depth Anything 2 (DA2).DA3 introduces a new visual geometry benchmark that covers camera pose estimation, any-view geometry, and visual rendering. On this benchmark, DA3 sets new state-of-the-art results, surpassing previous SOTA (VGGT) by 35.7% in pose accuracy and 23.6% in geometric accuracy, and also outperforms DA2 in monocular depth estimation. All training is exclusively based on public academic datasets." src="assets/slide_2025_46/depth-anything-3-recovering-the-visual-space-from-any-views-bytedance-seed.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/depth-anything-3-monocular-depth-estimation.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/bingyikang/status/1989358267668336841" target="_blank">
  <img alt="Depth Anything 3 (DA3) is a new system designed to achieve human-like spatial perception by extending monocular depth estimation to any-view scenarios, including single images, multi-view images, and video. The project emphasizes minimal modeling, demonstrating that a plain transformer architecture and a single depth-ray representation are sufficient, eliminating the need for specialized models or complex 3D tasks.Three model series have been released: the main DA3 series, a monocular metric estimation series, and a monocular depth estimation series. The core team behind DA3 includes Haotong Lin, Sili Chen, Jun Hao Liew, and @donydchen." src="assets/slide_2025_46/depth-anything-3-monocular-depth-estimation.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://arxiv.org/abs/2510.26493" target="_blank">
  <img alt="The paper &quot;Context Engineering 2.0: The Context of Context Engineering&quot; explores the evolution and conceptual foundations of context engineering in artificial intelligence. Drawing inspiration from Marx's idea that individuals are shaped by their social relations, the authors argue that context now includes both human-human and human-machine interactions. They trace the historical development of context engineering, from early human-computer interaction frameworks to today's intelligent agent paradigms, and propose key design considerations for advancing systematic context engineering in AI systems.An overview is provided of four eras of context engineering, each characterized by increasing machine intelligence, greater context-processing capabilities, and reduced human-AI interaction costs. The authors define context engineering, situate it within its broader historical landscape, and highlight the transition from Era 2.0 to Era 3.0, emphasizing the growing importance of reliable, initiative-driven AI collaborators in shaping future interactions." src="assets/slide_2025_46/context-engineering-2-0-the-context-of-context-engineering-sjtu.png"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://huggingface.co/papers/2511.08567" target="_blank">
   <img alt='The paper investigates the learning dynamics of Reinforcement Learning with Verifiable Rewards (RLVR) in large language models, focusing on why RLVR achieves significant performance improvements despite making only sparse parameter updates compared to supervised fine-tuning (SFT). The authors reveal that this sparsity is not fundamental but rather a result of a model-conditioned optimization bias, where updates consistently localize to specific, stable parameter regions preferred by the pretrained model, largely independent of datasets or RL algorithms used.To explain these dynamics, the authors propose a "Three-Gate Theory": (I) a KL constraint anchors updates, (II) model geometry steers updates into low-curvature, spectrum-preserving subspaces away from principal directions, and (III) precision hides micro-updates, making the off-principal bias appear as sparsity. Their parameter-level analysis shows that RLVR operates in a distinct optimization regime from SFT, suggesting that directly applying SFT-era parameter-efficient fine-tuning methods to RLVR can be flawed. This work advances understanding of RLVR and guides the development of RLVR-native, geometry-aware learning algorithms.' src="assets/slide_2025_46/the-path-not-taken-rlvr-provably-learns-off-the-principals-meta-ai-utexas.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/zhu_hanqin41424/status/1988711214210236833" target="_blank">
   <img alt='RL is not just random "black-box" tweaking of model weights. The authors demonstrate that RLVR (Reinforcement Learning with Value Regularization) consistently updates specific regions of the model while maintaining its core structure.They emphasize that RL operates under a different optimization regime compared to SFT (Supervised Fine-Tuning), meaning that standard PEFT (Parameter-Efficient Fine-Tuning) methods from the SFT era may not be effective. The work calls for developing new approaches tailored to RL rather than relying on repurposed SFT techniques.' class="half-size" src="assets/slide_2025_46/is-rl-black-box-weight-tinkering.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://hanqing.notion.site/the-path-not-taken" target="_blank">
   <img alt='**Summary:**The paper "The Path Not Taken: RLVR Provably Learns Off the Principals" presents the first parameter-level analysis of RLVR (Reinforcement Learning with Value Regularization) training dynamics. The authors discover a consistent optimization bias: RLVR updates model parameters in low-curvature, off-principal directions rather than along the principal weights targeted by supervised fine-tuning (SFT). This behavior is observed across datasets, RL variants, and model families, resulting in minimal subspace rotation and spectral drift throughout training.To explain this phenomenon, the authors introduce a Three-Gate Theory, where each RL step is constrained by the KL anchor, guided by model geometry, and shaped by precision limitations—steering updates away from principal directions. Their findings highlight that RLVR operates in a fundamentally different optimization regime than SFT, rendering many SFT-era parameter-efficient fine-tuning (PEFT) methods ineffective for RLVR. The work encourages the development of RLVR-native learning algorithms and provides theoretical and empirical evidence supporting these discoveries.' src="assets/slide_2025_46/the-path-not-taken-rlvr-provably-learns-off-the-principals-utexas.png"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://pleias.fr/blog/blogsynth-the-new-data-frontier" target="_blank">
   <img alt="SYNTH is a newly released, fully generalist synthetic dataset designed to fundamentally shift AI pretraining toward reasoning and skill acquisition, rather than relying on large, noisy web archives. Built from 50,000 core Wikipedia articles and expanded into diverse tasks—such as math, creative writing, information extraction, and multi-turn conversations—SYNTH leverages modular synthetic pipelines, grounding generated data in verifiable sources and supporting multilingual content. The dataset enables efficient training of small, deep language models (notably Baguettotron at 321M and Monad at 56M parameters), achieving state-of-the-art results on reasoning benchmarks like MMLU and gsm8k using significantly less data and compute than traditional approaches.The SYNTH approach emphasizes context engineering: orchestrating fine-tuned models and symbolic methods to generate shaped data tailored for specific reasoning capabilities. This strategy demonstrates that high-quality synthetic data can produce compact, high-performing models and suggests that preprocessing and context preparation are as crucial as model architecture itself. Looking ahead, the team plans to expand SYNTH to specialized domains, refine synthetic pipelines, and collaborate on real-world deployments, advocating for open, reproducible, and efficient AI development." src="assets/slide_2025_46/synth-the-new-data-frontier.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/Dorialexander/status/1987930819021635964" target="_blank">
   <img alt="A new fully synthetic generalist dataset for pretraining, called SYNTH, has been released. Alongside this, two state-of-the-art reasoning models, exclusively trained on SYNTH, have been introduced.One of these models, Baguettotron, has achieved best-in-class performance within its size range, despite being trained on only 200 billion tokens." src="assets/slide_2025_46/fully-synthetic-generalist-dataset-state-of-the-art-reasoning-models.png"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/abs/2510.17733" target="_blank">
   <img alt="The paper introduces a new method called Binary Retrieval-Augmented Reward (Binary RAR) to mitigate hallucinations (factually incorrect outputs) in language models. Instead of assigning continuous rewards, Binary RAR gives a reward of one only if the generated output is entirely factually accurate, as verified by retrieval, and zero otherwise. This binary approach is applied through online reinforcement learning and evaluated on various reasoning and open-ended tasks using Qwen3 models.Results show that Binary RAR reduces hallucination rates by 39.3% compared to supervised and continuous-reward baselines, without degrading performance on tasks like instruction following, math, or code generation. Additionally, in short-form question answering, the model learns to abstain when uncertain, leading to significantly fewer incorrect answers. The method outperforms existing approaches by achieving a better balance between factual accuracy and utility." src="assets/slide_2025_46/train-for-truth-keep-the-skills-binary-retrieval-augmented-reward-mitigates-hallucinations-universit.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/tomchen0/status/1989066130246496716" target="_blank">
   <img alt="OpenAI's blog highlights that current language models often &quot;hallucinate&quot; because their training and evaluation processes prioritize guessing over acknowledging uncertainty. This prompts the question of whether it's possible to reduce hallucinations without compromising the usefulness of these models.A proposed solution involves using on-policy reinforcement learning with a Binary Retrieval-Augmented Reward (RAR) system. This method reportedly achieves a 40% reduction in hallucinations while maintaining the win rate and accuracy of advanced language models like Qwen3-8B." class="third-size" src="assets/slide_2025_46/reducing-language-model-hallucination-with-binary-retrieval-augmented-reward-openai.png"/>
  </a>
 </div>
</section>
<section data-background-video="assets/slide_2025_46/learning-a-thousand-tasks-in-a-day-science-robotics.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://www.robot-learning.uk/learning-1000-tasks" target="_blank">
  <img alt='This page presents "Learning a Thousand Tasks in a Day," a research project demonstrating a highly efficient imitation learning paradigm for robot manipulation, called Multi-Task Trajectory Transfer (MT3). The key innovation is decomposing manipulation trajectories into alignment and interaction phases and leveraging retrieval-based methods instead of standard behavioural cloning. MT3 enables robots to learn new tasks from as little as a single demonstration, generalising to unseen objects and scaling to a wide range of diverse household tasks.Through extensive real-world experiments, the researchers taught a robot 1000 distinct tasks in under 24 hours and systematically compared different learning strategies. Results show that MT3 achieves an order of magnitude improvement in data efficiency over monolithic behavioural cloning, especially in regimes with few demonstrations per task. The study also details the strengths and failure modes of MT3 versus other approaches, highlighting its impact on scalable, general-purpose robot learning.' src="assets/slide_2025_46/learning-a-thousand-tasks-in-a-day.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/mervenoyann/status/1988620438407098769" target="_blank">
  <img alt="Meta has released new multilingual MetaCLIP2 models that support over 300 languages. These models enable image-to-text and text-to-image search functionalities across various languages.A notebook is provided to demonstrate how to use these models for multilingual image and text search in databases." src="assets/slide_2025_46/multilingual-metaclip2-image-text-search-meta.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/AndrewLampinen/status/1988648970080715211" target="_blank">
  <img alt="Vision models like CLIP do not fully capture the global organization present in human knowledge. The authors suggest that aligning these models with human-like structures increases their robustness.They highlight new research published by Lukas Mut and share a related blog post discussing these findings and improvements." src="assets/slide_2025_46/what-aspects-of-human-knowledge-do-vision-models-like-clip-fail-to-capture-nature.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/lejepa-a-novel-pretraining-paradigm-free-of-the-many-heuristics-we-relied-on-facebookai.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/randall_balestr/status/1988634269351022781" target="_blank">
   <img alt="LeJEPA introduces a novel pretraining paradigm for machine learning that eliminates the need for many traditional heuristics such as stop-gradient and teacher models. The approach has been tested on over 60 architectures (up to 2 billion parameters) and more than 10 datasets, showing strong in-domain training performance that surpasses DINOv3.Additionally, LeJEPA achieves a high correlation (95%) between training loss and test performance, indicating robust generalization. More details, including the paper and code, are available via the provided links." src="assets/slide_2025_46/lejepa-a-novel-pretraining-paradigm-free-of-the-many-heuristics-we-relied-on-facebookai.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://arxiv.org/pdf/2511.08544" target="_blank">
   <img alt="LeJEPA is a new, theoretically grounded framework for self-supervised learning, designed to improve the scalability and robustness of Joint-Embedding Predictive Architectures (JEPAs) without relying on common heuristics. The authors introduce Sketched Isotropic Gaussian Regularization (SIGReg) as a novel objective, ensuring embeddings follow an optimal isotropic Gaussian distribution, which minimizes downstream prediction risk. LeJEPA integrates this with the JEPA predictive loss, yielding a method with a single trade-off hyperparameter, linear time and memory complexity, broad stability across architectures and domains, and a simple, distributed training-friendly implementation.Empirical results across 10+ datasets and 60+ architectures demonstrate LeJEPA's effectiveness, particularly in domain-specific self-supervised pretraining, which outperforms state-of-the-art transfer learning (such as DINOv2/v3) even with massive models. Notably, LeJEPA achieves strong performance (e.g., 79% accuracy on ImageNet-1k with ViT-H/14 under linear evaluation), and its training loss shows a high correlation with downstream performance, allowing for practical model selection without supervised probing. The approach aims to reestablish self-supervised pre-training as a foundational technique in AI research." src="assets/slide_2025_46/lejepa-provable-and-scalable-self-supervised-learning-without-the-heuristics-meta-nyu-brown.png"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.nature.com/articles/s41586-025-09833-y_reference.pdf" target="_blank">
  <img alt="This article, titled &quot;Olympiad-level formal mathematical reasoning with reinforcement learning,&quot; presents research on using reinforcement learning to achieve advanced mathematical problem-solving capabilities. The work demonstrates how artificial intelligence systems can be trained to solve challenging mathematical problems at the level of international Olympiads, employing formal reasoning techniques.The study details the methodology, including the use of reinforcement learning algorithms, and highlights the system's ability to tackle complex proofs and reasoning tasks. The results suggest significant progress in automating formal mathematics, with potential implications for education, research, and the development of more capable AI reasoning systems." src="assets/slide_2025_46/olympiad-level-formal-mathematical-reasoning-with-reinforcement-learning-nature.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.nature.com/articles/s41586-025-09631-6.pdf" target="_blank">
  <img alt="This article addresses the gap between machine and human visual representations, focusing on how deep neural networks, despite their success in vision tasks, often fail to mirror the hierarchical, multi-level conceptual organization found in human knowledge. The authors identify that while models can capture fine-grained similarities (e.g., between dog breeds), they struggle with more abstract semantic relationships (e.g., animacy distinctions between dogs and fish), leading to weaknesses in generalization and robustness compared to human perception.To remedy this, the authors propose training a teacher model to imitate human judgments and then transferring this human-aligned conceptual structure to state-of-the-art vision models through fine-tuning. The resulting models better approximate human behavior and uncertainty across various similarity tasks, perform more robustly in machine learning benchmarks, and exhibit improved generalization and out-of-distribution performance. This approach paves the way for more interpretable, robust, and human-aligned AI systems." src="assets/slide_2025_46/aligning-machine-and-human-visual-representations-across-abstraction-levels-google-deepmind-tu-berli.png"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/pdf/2510.26658" target="_blank">
   <img alt='The paper introduces the concept of "agentic organization," a new era in AI where multiple agents collaboratively and concurrently solve complex problems, surpassing the limitations of individual intelligence. To implement this, the authors propose AsyncThink, a reasoning paradigm for large language models that organizes internal processes into structures that can be executed concurrently. In this paradigm, an "organizer" agent dynamically assigns sub-queries to "worker" agents, merges their intermediate outputs, and produces coherent final solutions, with the overall structure optimized through reinforcement learning.Experimental results show that AsyncThink reduces inference latency by 28% compared to parallel thinking approaches while also improving accuracy in mathematical reasoning tasks. Additionally, AsyncThink demonstrates the ability to generalize its asynchronous thinking strategies to new, unseen tasks without requiring further training.' src="assets/slide_2025_46/the-era-of-agentic-organization-learning-to-organize-with-language-models-microsoft-research.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/IntuitMachine/status/1987604886834581944" target="_blank">
   <img alt='A recent Microsoft Research paper introduces a new paradigm for AI problem-solving called "Asynchronous Thinking" (AsyncThink), moving beyond traditional sequential and parallel reasoning methods. Instead of acting as a lone genius, the AI becomes an "Organizer"—a project manager that breaks down complex tasks and delegates them to multiple "Worker" AIs. The Organizer dynamically integrates results as they become available, enabling real-time collaboration and adjustment rather than static, isolated work.This approach is trained using Reinforcement Learning, rewarding the AI for keeping workers efficiently engaged and minimizing downtime. The result is significantly faster and more accurate problem-solving on complex tasks, with the AI demonstrating transferable organizational skills (e.g., solving a Sudoku puzzle without retraining). The paper suggests that the future of AI lies in mastering coordination and collective intelligence, empowering AI not just to solve problems, but to organize teams of intelligences for challenges far beyond the reach of any single mind.' src="assets/slide_2025_46/the-era-of-agentic-organization-microsoft-research.png"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/Saboo_Shubham_/status/1989535594482196915" target="_blank">
  <img alt="We conducted a large-scale, free AI Agent course over five days, releasing a comprehensive set of resources each day. The course aimed to educate participants on building and utilizing AI agents, offering practical materials and guidance throughout the event.All resources from Day 1 to Day 5 are available at no cost for anyone interested in learning about AI agents. The materials can be accessed through the provided link." class="half-size" src="assets/slide_2025_46/biggest-free-ai-agent-course-resources.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://cookbook.openai.com/examples/partners/self_evolving_agents/autonomous_agent_retraining" target="_blank">
  <img alt="Here is a concise summary of the main content, following your specified rules:---**Summary**This document provides a comprehensive cookbook for building **self-evolving autonomous agents** that can continuously retrain and optimize themselves through structured feedback loops. Using a regulated healthcare documentation task (summarizing pharmaceutical regulatory submissions) as a case study, the guide demonstrates three main approaches to agent improvement: manual prompt optimization via the OpenAI Evals platform, automated self-evolving loops using LLM-as-a-Judge graders, and advanced reflective prompt evolution with GEPA (Genetic-Pareto) optimization. The workflow includes modular evaluation strategies, prompt versioning, agent orchestration, and continuous monitoring, all designed for accuracy, auditability, and rapid iteration.**Key Points**- Presents step-by-step instructions for instrumenting autonomous agents with feedback signals and evals, allowing iterative self-improvement.- Provides code templates for agents, prompt versioning, grader configuration, and orchestration utilities.- Explores automated optimization techniques, including static metaprompts, model candidate comparison, and GEPA-based reflective prompt evolution.- Emphasizes best practices for agent observability, rollback, and robust evaluation in high-stakes domains like healthcare.- Includes example output prompts for each optimization method and guidance for adapting workflows to production environments.---*(All content related to menus, cookies, footers, and images has been ignored. Output is in markdown and limited to two paragraphs.)*" src="assets/slide_2025_46/self-evolving-agents-a-cookbook-for-autonomous-agent-retraining-openai.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://cookbook.openai.com/examples/gpt-5/gpt-5-1_prompting_guide" target="_blank">
  <img alt="GPT-5.1 is OpenAI’s latest flagship model, optimized for balance between intelligence, speed, and steerability across agentic and coding tasks. Key features include improved calibration to input difficulty, enhanced control over output personality and formatting, and the new `none` reasoning mode for low-latency scenarios. The guide outlines best practices for migrating from previous models, shaping agent behavior and output via prompt engineering, and leveraging GPT-5.1’s strengths in instruction-following, tool usage (including new tools like `apply_patch` and `shell`), and coding workflows. It emphasizes the importance of clear, targeted prompting, handling verbosity, maintaining solution persistence, and structuring agent updates for user supervision.A major focus is on “metaprompting”—iteratively analyzing and refining system prompts to address model behavior issues. The document provides detailed examples for enforcing agent personality, controlling verbosity, designing tool usage, optimizing code changes, and conducting root-cause analysis of prompt failures. Developers are encouraged to treat prompting as an iterative process, using structured feedback and prompt revisions to maximize reliability and user experience in production deployments." src="assets/slide_2025_46/gpt-5-1-prompting-guide-openai.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/understanding-neural-networks-through-sparse-circuits/" target="_blank">
  <img alt="OpenAI has introduced a new research approach to improve interpretability in neural networks by training sparse models, where most connections (weights) are set to zero. This method contrasts with traditional dense networks, making the internal computations of the model more disentangled and easier to understand. By forcing sparsity, the models form small, understandable circuits that are both necessary and sufficient for specific behaviors, as demonstrated on simple algorithmic tasks.The study finds that increasing sparsity in larger models can improve interpretability while maintaining capability, and that these sparse circuits can be isolated and analyzed directly. Although the work is at an early stage and current models are smaller than frontier systems, the results suggest a promising path towards building more transparent and understandable AI. Future goals include scaling these techniques to larger models and developing methods to extract or train interpretable circuits more efficiently." src="assets/slide_2025_46/understanding-neural-networks-through-sparse-circuits-openai.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://pub.sakana.ai/sudoku-gpt5/" target="_blank">
  <img alt="This document presents the release and evaluation of Sudoku-Bench by Sakana AI, a challenging benchmark designed to test the reasoning abilities of large language models (LLMs) using a curated set of handcrafted classic and modern Sudoku puzzles. The benchmark exposes the difficulties LLMs face in integrating mathematical, logical, and spatial reasoning, especially in puzzles with novel constraints and creative rule sets. GPT-5 is highlighted as the first LLM to solve a 9x9 modern Sudoku variant, achieving a 33% solve rate on the most difficult set, demonstrating strong mathematical and logical reasoning but revealing persistent challenges in spatial reasoning and maintaining global consistency.The authors detail experiments with recent AI training approaches, including GRPO finetuning and thought cloning from human expert transcripts. While these methods improve mathematical reasoning, they fall short in enabling models to replicate human-like logical deduction and spatial reasoning, often resulting in superficial pattern matching or flawed logic. The analysis underscores the persistent gap between current AI and expert human problem-solving, positioning Sudoku-Bench as an essential testbed for developing and benchmarking future AI systems capable of authentic, flexible reasoning." src="assets/slide_2025_46/from-grpo-to-gpt-5-why-sudoku-variants-remain-a-grand-challenge-in-ai-reasoning-sakana-ai.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.anthropic.com/news/disrupting-AI-espionage" target="_blank">
  <img alt="Anthropic has reported the first documented large-scale cyber espionage campaign orchestrated primarily by AI, specifically using their Claude Code tool, with minimal human intervention. The attack, attributed to a Chinese state-sponsored group, targeted around thirty global organizations including tech companies, financial institutions, chemical manufacturers, and government agencies. Attackers exploited AI’s advanced capabilities—intelligence, autonomy, and tool integration—by jailbreaking Claude to bypass safety guardrails and execute complex cyber operations such as reconnaissance, vulnerability identification, credential harvesting, and data exfiltration.This case marks a significant escalation in AI-driven cyber threats, as agentic AI systems now enable less experienced actors to conduct attacks previously requiring large expert teams. Anthropic has responded by enhancing detection and classification systems and encourages the broader cybersecurity community to adopt AI both for defense and to build stronger safeguards against adversarial misuse. The report underscores the dual-use nature of AI in cybersecurity and calls for ongoing threat sharing, improved detection, and robust safety measures." src="assets/slide_2025_46/disrupting-the-first-reported-ai-orchestrated-cyber-espionage-campaign-anthropic.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.claude.com/blog/improving-frontend-design-through-skills" target="_blank">
  <img alt="This article discusses how to improve the quality and uniqueness of AI-generated frontend designs using Claude's &quot;Skills&quot; feature. By default, large language models like Claude tend to produce generic web interfaces due to distributional convergence on common design patterns (e.g., Inter fonts, purple gradients). The article explains that with targeted prompting—especially through reusable domain-specific &quot;skills&quot; containing instructions and examples—developers can dynamically provide precise design guidance only when needed. This avoids cluttering the model’s context and enables more distinctive, creative outputs across typography, themes, animation, and background.The article also illustrates how specialized skills, such as a web-artifacts-builder, allow Claude to leverage advanced tooling (React, Tailwind CSS, shadcn/ui) for richer artifacts beyond basic HTML/CSS/JS. By encapsulating design principles and technical workflows into modular skills, teams can achieve more consistent, high-quality, and brand-aligned user interfaces. The approach is generalizable: any area where Claude defaults to generic outputs can benefit from well-crafted, on-demand skills to unlock its latent capabilities." src="assets/slide_2025_46/improving-frontend-design-through-skills-anthropic.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://claude.com/blog/skills-explained" target="_blank">
  <img alt="Claude’s agentic ecosystem offers several building blocks for creating custom AI workflows: Skills, Prompts, Projects, Subagents, and MCP (Model Context Protocol). Skills are reusable folders containing instructions, scripts, and resources that Claude loads dynamically for specialized tasks, unlike prompts, which are one-off, ephemeral instructions used in conversations. Projects provide persistent shared context and knowledge bases for groups or initiatives, while subagents are specialized AI assistants with dedicated tools and context, enabling independent task execution. MCP acts as a universal protocol for connecting Claude to external data sources and tools, facilitating seamless access to organizational resources.The guide explains how to choose and combine these tools for powerful agentic workflows. Skills provide procedural knowledge and expertise that can be shared across agents and conversations, prompts enable immediate and reactive guidance, Projects organize persistent knowledge for collaboration, subagents delegate specialized tasks, and MCP enables connectivity to data and business tools. By leveraging these components together, users can create sophisticated AI agents tailored to complex workflows, ensuring efficiency, consistency, and access to relevant information." src="assets/slide_2025_46/skills-explained-how-skills-compares-to-prompts-projects-mcp-and-subagents-claude.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/emollick/status/1989093191182926249" target="_blank">
  <img alt="The new Claude Code front-end design Skill offers significant improvements by replacing the previous purple gradient and Arial font with a more audience-focused approach. The update enhances usability and aesthetics, resulting in a design that better matches the intended use of applications.Users have found the Skill valuable for revising and improving the design of their own apps, highlighting the practical benefits and overall value of utilizing Skills in their workflow." src="assets/slide_2025_46/claude-code-front-end-design-skill-improvement.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/trq212/status/1988690675542675536" target="_blank">
  <img alt="We developed a Deep Research demo utilizing the Claude Agent SDK, addressing a highly requested use case. The demo enables spawning multiple AI agents to simultaneously research a given topic.After conducting parallel research, the agents synthesize their findings into a comprehensive report. More details on the implementation are shared in the provided thread." class="half-size" src="assets/slide_2025_46/deep-research-demo-claude-agent-sdk.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://ampcode.com/guides/context-management" target="_blank">
  <img alt="The document explains how context management works in Amp, focusing on the concept of the &quot;context window.&quot; The context window includes all messages, model replies, tool calls, and relevant data in a conversation with a language model agent. Its size is limited by the model, and everything within it influences the model's outputs. To maintain quality and relevance, conversations should be kept concise and focused, as longer context windows may degrade performance.Amp provides several tools for managing the context window: users can add context by mentioning files or using shell commands, edit or restore previous messages to remove unwanted context, fork threads to branch conversations, and use handoff to start new focused threads with relevant information. Additionally, users can reference other threads to extract necessary information without overloading the current context. These features help keep interactions efficient, relevant, and manageable while working with AI agents in Amp." src="assets/slide_2025_46/context-management-visualization.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.distillabs.ai/blog/vibe-tuning-the-art-of-fine-tuning-small-language-models-with-a-prompt" target="_blank">
  <img alt="The article introduces &quot;vibe-tuning,&quot; a new technique that enables users to fine-tune small language models (SLMs) using only a natural language prompt, bypassing the traditional need for large datasets and complex engineering pipelines. Vibe-tuning leverages automated synthetic data generation and knowledge distillation, where a large &quot;Teacher&quot; model's expertise is distilled into a smaller, more efficient &quot;Student&quot; model, resulting in specialized SLMs that are cost-effective, deployable on edge devices, and optimized for specific tasks such as classification or tool calling.The piece contrasts vibe-tuning with prompt engineering, highlighting the latter's limitations in consistency and domain adaptation. It provides a step-by-step guide to fine-tuning an intent classification model using the distil labs platform, emphasizing the speed, automation, and accessibility of the process. The article concludes by underscoring the advantages of using fine-tuned SLMs over prompt-engineered large models for targeted applications." class="half-size" src="assets/slide_2025_46/vibe-tuning-the-art-of-fine-tuning-small-language-models-with-a-prompt-distil-labs.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/vlab-pretraining-toolkit-for-vision-language-action-models-for-robotics.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/danaaubakir/status/1988971561592975697" target="_blank">
  <img alt="VLAb is a new pretraining toolkit designed for Vision-Language-Action (VLA) models in robotics. It offers a simple and hackable codebase, making it easy to experiment with VLA models.The toolkit supports efficient pretraining and fine-tuning of VLA models using any LeRobot or Hugging Face format dataset, and can scale across multiple GPUs." src="assets/slide_2025_46/vlab-pretraining-toolkit-for-vision-language-action-models-for-robotics.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/mervenoyann/status/1988987399058374922" target="_blank">
  <img alt="Cutting-edge computer vision models typically integrate specialized heads with powerful backbone architectures, but managing them together for training can be complex. Hugging Face Transformers offers tools to simplify this process.The post highlights how to combine DINOv3 and DETR using the Backbone API, enabling advanced object detection workflows. A tutorial link is provided for users to learn more." class="half-size" src="assets/slide_2025_46/combine-dinov3-with-detr-for-computer-vision-backbone-api-huggingface.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/wandb-leet-terminal-ui-launch.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/wandb/status/1988401253156876418" target="_blank">
  <img alt="W&amp;B is launching a new side project called W&amp;B LEET, developed by their SDK team. This project introduces a full Terminal UI (TUI) for live, interactive monitoring directly within your terminal.The tool operates without requiring a browser or internet connection, allowing users to monitor their work seamlessly from the command line." src="assets/slide_2025_46/wandb-leet-terminal-ui-launch.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/MetaOpenSource/status/1988653705525072249" target="_blank">
  <img alt="⚡️OSS Project Spotlight⚡️ highlights a robust resource monitoring tool for Linux, offering comprehensive insights into system performance areas such as CPU, memory, and I/O.The tool features an intuitive terminal interface, making it accessible for users to monitor and analyze system metrics efficiently. Learn more at the provided link." class="half-size" src="assets/slide_2025_46/below-linux-resource-monitoring-tool.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://deepmind.google/blog/teaching-ai-to-see-the-world-more-like-we-do/" target="_blank">
  <img alt="Google DeepMind's latest research explores how aligning AI vision models with human conceptual understanding can make these systems more robust, reliable, and intuitive. By comparing how humans and AI models perform on &quot;odd-one-out&quot; visual tasks, the researchers found that AI often fails to capture higher-level similarities that humans recognize, such as grouping objects by abstract categories. To address this, they developed a multi-step alignment method: training a small adapter on human judgment data, generating a large synthetic dataset of human-like decisions, and fine-tuning student models on this dataset. This process reorganizes the models’ internal representations to better reflect human conceptual hierarchies.Aligned models were tested on various cognitive science and AI tasks, demonstrating significantly improved agreement with human judgments and better performance in challenging scenarios like few-shot learning and distribution shifts. The study concludes that improving human-alignment in vision models not only enhances their intuitive reasoning but also boosts their overall reliability and generalization, marking a step forward in building more trustworthy AI systems." src="assets/slide_2025_46/teaching-ai-to-see-the-world-more-like-we-do-deepmind.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/GoogleAIStudio/status/1988290390169141295" target="_blank">
  <img alt="Based on your request, here is a concise summary:---The main content discusses strategies for effective time management, emphasizing the importance of prioritizing tasks and setting clear goals. It highlights techniques such as breaking large projects into smaller, manageable steps and using tools like to-do lists or digital planners to stay organized.Additionally, the article offers tips on minimizing distractions and maintaining focus throughout the workday. It encourages regular breaks and self-reflection to improve productivity and ensure continuous progress toward personal and professional objectives." class="half-size" src="assets/slide_2025_46/the-evolution-of-visual-attention-in-deep-neural-networks-mit.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.copilotkit.ai/blog/microsoft-agent-framework-is-now-ag-ui-compatible" target="_blank">
  <img alt="Microsoft’s Agent Framework is now fully compatible with the AG-UI protocol, enabling seamless integration between Microsoft’s intelligent agent backend and real-time, interactive user interfaces. This means developers can connect reasoning, orchestration, and multi-agent workflows directly to frontend applications like React, without custom APIs or complex socket code. The framework supports .NET (C#) and Python, and is deeply integrated with Azure AI Foundry, Microsoft Graph, and Microsoft 365.AG-UI, developed by CopilotKit, is an open standard for agent-to-UI communication, supporting live streaming of responses, tool invocations, state synchronization, and more. This integration allows for fullstack agent applications with rich frontend experiences and real-time interaction, deployable across Azure, the web, and other environments. Resources are available for getting started, including example repositories and documentation." src="assets/slide_2025_46/microsoft-agent-framework-ag-ui-integration-microsoft.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://clickhouse.com/blog/netflix-petabyte-scale-logging?utm_campaign=twitter-awareness&amp;utm_source=Twitter&amp;utm_medium=paid-social" target="_blank">
  <img alt="Netflix processes 5 petabytes of logs daily—over 10 million events per second—using ClickHouse to deliver sub-second query performance for rapid debugging and monitoring across 40,000+ microservices. To achieve this scale and interactivity, Netflix implemented three key optimizations: generating lexers for fast log fingerprinting (8–10x speedup), building custom serialization using ClickHouse's native protocol for efficient ingestion, and sharding custom tag maps to accelerate query times from several seconds to under a second.These engineering improvements allow Netflix's logging system to make billions of log entries searchable within seconds, supporting both immediate troubleshooting and long-term analytics. The overall approach focuses on simplicity and targeted optimization, resulting in a robust, cost-effective, and highly responsive system for one of the world’s largest streaming platforms." src="assets/slide_2025_46/how-netflix-optimized-petabyte-scale-logging-system-clickhouse-netflix.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/karpathy/status/1990116666194456651" target="_blank">
  <img alt="The conversation compares AI's impact on the economy to historical technological shifts, arguing that AI represents a new computing paradigm—&quot;Software 2.0&quot;—focused on automating digital information processing beyond what could be manually programmed before. In the era of traditional software (&quot;Software 1.0&quot;), automation targeted tasks with fixed, easily specified rules. With AI, automation now extends to tasks that are verifiable through objectives and feedback, utilizing neural networks and optimization techniques.The key insight is that the most automatable jobs in the AI era are those where results can be efficiently verified and rewarded, allowing AI systems to &quot;practice&quot; and improve. This verifiability drives rapid progress in areas like math, code, and other tasks with clear correctness criteria, while more complex, creative, or context-dependent tasks remain less susceptible to automation. Thus, Software 1.0 automates what can be specified; Software 2.0 automates what can be verified." src="assets/slide_2025_46/ai-automation-job-market-software-2-0-verifiability-explained.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://surgehq.ai/blog/rl-envs-real-world" target="_blank">
  <img alt="In this research, Surge AI evaluated nine leading AI models by assigning them 150 multi-step tasks in a simulated customer support environment called Corecraft, Inc. Despite advances, even top models like GPT-5 and Claude Sonnet 4.5 failed over 40% of real-world agentic tasks, revealing persistent gaps in reliability and practical reasoning. The study identified a hierarchy of agentic capabilities required for effective real-world AI agents: basic tool use and planning, adaptability, groundedness (staying contextually accurate), and common sense reasoning. Most models struggled with foundational skills like mapping information to tools and adapting to unexpected results, while only the best models approached—but did not fully achieve—human-level common sense reasoning.The findings highlight that, although current models are increasingly able to act coherently and reliably in controlled environments, they still lack consistent adaptability and common sense needed for robust real-world deployment. The research suggests that 2025 marks a turning point: AI agents are now good enough for their common sense reasoning to be meaningfully scrutinized, but there remains a significant gap to true general intelligence and human-level agency." src="assets/slide_2025_46/rl-environments-and-the-hierarchy-of-agentic-capabilities-surge-ai.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/arafatkatze/status/1988524692832235899" target="_blank">
  <img alt="Cursor and Cognition present contrasting approaches to agent search. Cursor claims that custom embeddings trained on agent traces yield a 12.5% accuracy improvement, while Cognition finds embeddings counterproductive and instead trains models to use grep with 8x parallel tool calls.Both approaches have been benchmarked, highlighting a notable disagreement in effective search methodologies for agents." class="half-size" src="assets/slide_2025_46/cursor-vs-cognition-agent-search-benchmarks.png"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/realshcallaway/status/1987691768163512492" target="_blank">
   <img alt="The post highlights that MCP was released before Claude Code, introducing the concept of versatile agentic tools via bash before it became mainstream. MCP initially stood out as the only method to integrate third-party tools into popular desktop AI clients like Claude Desktop, Cursor, and ChatGPT, making it a significant feature.Despite its flaws and the waning hype, MCP remains valuable for creating tools that operate seamlessly across different agentic clients, particularly for tasks requiring remote environments. The author argues that MCP is still useful and unlikely to disappear soon." class="third-size" src="assets/slide_2025_46/mcp-importance-in-ai-agent-tools.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/simonw/status/1988659056521933027" target="_blank">
   <img alt="Unlike typical APIs, MCPs (Model-Callable Procedures) do not require stability guarantees. This is because LLMs (Large Language Models) interact with MCP servers by dynamically reading their specifications each time, allowing for frequent and ongoing changes to the MCP server without concern for breaking compatibility.As a result, MCP servers can be updated or modified continually without the need for versioning or backward compatibility, since no stability promises are made to clients." src="assets/slide_2025_46/mcp-server-dynamic-api-stability-explained.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/ibuildthecloud/status/1989886154272055719" target="_blank">
   <img alt="The post explains that MCP (Model Connector Protocol) is valuable because it provides a language-independent, standard way for large language models (LLMs) to interact with external tools, enabling secure and scalable integration. Unlike language-specific frameworks like LangChain, MCP allows clients in one language (e.g., Python) to connect with tools in another (e.g., Java), and supports secure, multi-tenant usage through features like OAuth 2.1.The author argues that while direct tool calling may suffice for simple applications, MCP is essential for more complex, distributed, and public tool scenarios. Criticisms about tool overload or poor tool definitions are issues separate from MCP itself, and new discovery methods (like ChatGPT apps or Claude skills) address scalability but do not replace the need for MCP." class="half-size" src="assets/slide_2025_46/why-mcp-protocol-matters-for-llm-tool-calling-and-interoperability.png"/>
  </a>
 </div>
</section>
<section data-background-video="assets/slide_2025_46/making-proactive-agents-with-code-generation.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/trq212/status/1988351918448714207" target="_blank">
  <img alt="Code generation enables agents not only to compose and utilize various tools but also to expand their capabilities. By leveraging code generation, agents can proactively perform actions on a user's behalf, enhancing their usefulness and autonomy.This approach moves beyond simple tool use, allowing agents to take initiative and handle tasks independently. As a result, code generation is a key technique for building more proactive and capable AI agents." src="assets/slide_2025_46/making-proactive-agents-with-code-generation.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/bernhardsson/status/1988647374605131837" target="_blank">
  <img alt="There's growing interest in code sandboxes due to their usefulness, especially with coding agents. Despite this excitement, the author believes creating a sustainable business based on code sandboxes presents significant challenges.The post aims to share thoughts on why, despite their utility and popularity, code sandbox businesses may be difficult to build and scale successfully." src="assets/slide_2025_46/coding-agent-business-challenges.png"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://alechelbling.com/UnderstandingIsomap/" target="_blank">
  <img alt="This article provides a visual and intuitive introduction to Isomap, a nonlinear dimensionality reduction technique designed to uncover low-dimensional structure in high-dimensional datasets. It begins by explaining the manifold hypothesis—the idea that high-dimensional data often lies on a much lower-dimensional manifold. The article uses a spiral dataset as an example, illustrating that while Euclidean distances fail to capture intrinsic structure globally, they work well locally. Isomap addresses this by building a k-nearest neighbor graph to represent local relationships, then computing geodesic (shortest path) distances through this graph, and finally applying Multidimensional Scaling (MDS) to create a low-dimensional embedding that preserves these manifold-based distances.The limitations of classical MDS with Euclidean distances, which only work well for linear structures (similar to PCA), motivate the need for Isomap’s approach. The article highlights key steps in Isomap: constructing a neighbor graph, calculating shortest path distances, and embedding with MDS. It also notes limitations such as sensitivity to graph parameters, computational cost, and the assumption of a single connected manifold, which have inspired newer techniques like t-SNE and UMAP." src="assets/slide_2025_46/a-visual-introduction-to-dimensionality-reduction-with-isomap.png"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_46/yann-lecun-llms-human-intelligence-breakthroughs.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/slow_developer/status/1987769692891533647" target="_blank">
   <img alt="Yann LeCun argues that large language models (LLMs) are valuable and will drive many practical applications, making significant infrastructure investments worthwhile. He does not consider LLMs to be a bubble in terms of value or investment.However, LeCun cautions against the belief that LLMs alone can achieve human-level intelligence. He emphasizes that true progress will require major breakthroughs beyond simply increasing data and computational power, indicating that a key element is still missing." src="assets/slide_2025_46/yann-lecun-llms-human-intelligence-breakthroughs.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/kimmonismus/status/1990072260531785891" target="_blank">
   <img alt='Yann LeCun is reportedly leaving Meta Platforms due to his belief that large language models are not the right approach for achieving human-level AI, describing them as a "dead end." Instead, he advocates for world models that create internal, causal representations of the physical world, allowing systems to predict and reason about the outcomes of actions.LeCun envisions future AI as machines capable of hierarchical planning and reasoning using measurable energy or compatibility functions, moving beyond simple text generation. While this approach is still speculative and resource-intensive, he believes it represents a more promising direction for AI development.' class="half-size" src="assets/slide_2025_46/lecun-departs-meta-language-models-versus-world-models-meta.png"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="3" href="https://x.com/Yuchenj_UW/status/1988305476132675675" target="_blank">
   <img alt="Yann LeCun is leaving Meta after internal changes, including having to report to Alexandr Wang following Meta's $15B acquisition. This shift was prompted by Meta's struggles to compete with OpenAI's LLMs and ChatGPT, as well as the disappointing performance of Llama 4.LeCun's skepticism toward using LLMs for AGI contributed to tensions with Mark Zuckerberg, ultimately leading to his departure. The summary suggests that, similar to Google's re-hiring of Noam, Meta might attempt to bring LeCun back in the future at a high cost." src="assets/slide_2025_46/yann-lecun-leaves-meta-meta.png"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://vgel.me/posts/seahorse/" target="_blank">
  <img alt="The article investigates why large language models (LLMs) like GPT-5, Claude, and Gemini confidently assert the existence of a seahorse emoji, despite no such emoji ever being included in Unicode. This widespread belief is mirrored in human communities online, possibly due to a combination of collective misremembering and logical generalization, given that many similar aquatic animal emojis exist and a seahorse emoji was once proposed. Through logit lens analysis (a technique for peering into model layer outputs), the author demonstrates that LLMs internally construct a &quot;seahorse + emoji&quot; concept, expecting a valid token to exist, and only realize their mistake when their output is autoregressively sampled and fails to match the intended emoji.This confusion is traced to how the model's `lm_head` (final output matrix) works, mapping internal representations to discrete tokens. For real emojis, the model's internal state smoothly transitions to the correct emoji token. For nonexistent ones like the seahorse, the process results in an unintended emoji or a feedback loop as the model struggles to reconcile its expectation with reality. The post speculates that reinforcement learning could help LLMs better align their internal representations with actual outputs, reducing such errors." src="assets/slide_2025_46/llm-seahorse-emoji-misconception-analysis.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/xpeng-vla-2-0-intelligent-assisted-driving-experience-xpev.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/XPengMotors/status/1987804055775609027" target="_blank">
  <img alt="XPENG VLA 2.0 offers intelligent assisted driving by adapting seamlessly to various road conditions, ensuring effortless and smooth journeys for drivers.The announcement highlights the advanced capabilities of XPENG's system, emphasizing an enhanced driving experience through automation and adaptability." src="assets/slide_2025_46/xpeng-vla-2-0-intelligent-assisted-driving-experience-xpev.png"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_46/iron-sync-xpev.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/XPengMotors/status/1988517894217449572" target="_blank">
  <img alt="A pair of IRONs (likely vehicles or robots) are showcased moving in perfect synchronization, creating an impressive display of technological harmony. The post emphasizes the complexity behind achieving such flawless coordination between two machines.Engineers highlight that, while the IRONs appear to move in sync, the process of syncing them is a significant technical challenge. The content underscores the advanced engineering and innovation involved in making this synchronized movement possible." src="assets/slide_2025_46/iron-sync-xpev.png"/>
 </a>
</section>
      

      <section>
        <div class="fragment fade-out" data-fragment-index="-1"
          style="align-items: center; justify-content: center; min-height: 70vh; margin: 0; font-size: 50%; margin: auto; display: flex; flex-direction: column;">
          <div style="margin-top: 10px; text-align:center;">
            <img src="assets/qr.png" height="100px" /> <br/>
            📚 <a href="index_all.html"><b>view all previous issues</b></a><br/>
            ✨ see you next week!
          </div>
        </div>
      </section>

    </div>
  </div>

  <script>
      let deck = new Reveal();
      deck.initialize({controls: true, progress: false, hash: true, plugins: [RevealScrollingImage]});
  </script>
</body>
</html>