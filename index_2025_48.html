<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="initial-scale=1.0">
    <meta name="description" content="" />
    <meta property="og:title" content="" />
    <meta property="og:description" content="" />
    <meta property="og:image" content="" />

    <title>ML NEWS / 2025 / 48th week</title>
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/night.css" />
    <link rel="stylesheet" href="dist/slider.css" />
    <link rel="alternate" type="text/html" href="index_2025_48.html?print-pdf" title="Printer-friendly version">

</head>
<body>
  <script src="dist/reveal.js"></script>
  <script src="dist/scrolling-image.js"></script>

  <div class="reveal">
    <div class="slides">
      <!-- PRODUCTS -->

      <!-- PAPERS -->

      <!-- OTHER -->

<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://www.anthropic.com/news/claude-opus-4-5" target="_blank">
   <img alt="Anthropic has announced the release of Claude Opus 4.5, their latest advanced AI model, which sets new benchmarks in coding, agentic workflows, and general computer use. Compared to previous models and competitors, Opus 4.5 demonstrates superior performance in software engineering tasks, reasoning, mathematics, and vision, while using fewer tokens—delivering both improved efficiency and cost-effectiveness. It features innovations like an adjustable effort parameter, robust context management, and advanced tool use, enabling better multi-agent coordination and longer autonomous sessions. Early testers and enterprise customers report significant gains in code quality, planning, error reduction, and complex workflow automation.Opus 4.5 also represents a step forward in safety, with enhanced robustness against prompt injection attacks and lower rates of concerning behavior. Anthropic emphasizes the model’s alignment and reliability for critical business tasks and notes that Opus 4.5 often finds creative solutions to real-world problems. The model is available via API, Anthropic’s consumer apps, and major cloud platforms, with updated pricing to make frontier capabilities more broadly accessible. Updates to the Claude Developer Platform and associated products such as Claude Code, Claude for Excel, and Claude for Chrome further showcase Opus 4.5’s strengths in handling long conversations, spreadsheets, and multi-agent workflows." src="assets/slide_2025_48/claude-opus-4-5-launch-anthropic.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/claudeai/status/1993030546243699119" target="_blank">
   <img alt="Claude Opus 4.5 is announced as the leading AI model for coding, agents, and computer use, representing a significant advancement in AI capabilities. The release is positioned as a glimpse into transformative changes in future work processes.For more information, a link is provided to learn about the model and its potential impact." class="half-size" src="assets/slide_2025_48/claude-opus-4-5-announcement-anthropic.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/claude-code-on-desktop-app-anthropic.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/amorriscode/status/1993383422728061335" target="_blank">
  <img alt="Claude Code is now available in the Desktop app, allowing users to run multiple sessions either locally or in the cloud. Users can easily switch between different Claude Code sessions for enhanced flexibility.Learn more at the provided link." src="assets/slide_2025_48/claude-code-on-desktop-app-anthropic.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/chatgpt-voice-feature-launch-openai.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/SlamAltman/status/1993397864425570547" target="_blank">
  <img alt="A recent update has removed the need for users to manually switch to a dedicated voice mode, streamlining interactions and reducing average friction by 0.7 to 1.1 seconds. Early data indicates this improvement could boost daily voice usage by up to 3%.The announcement highlights how even small reductions in latency can have notable impacts, especially when fewer major updates are planned. Additional new features are expected to roll out in the coming days." src="assets/slide_2025_48/voice-interaction-latency-reduction-announcement.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/chatgpt-shopping-research-experience.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://openai.com/index/chatgpt-shopping-research/" target="_blank">
  <img alt="OpenAI has introduced a new &quot;shopping research&quot; experience in ChatGPT, designed to help users discover, compare, and select products tailored to their needs. Instead of manually browsing multiple websites, users can describe what they're looking for, and ChatGPT will ask clarifying questions, research across high-quality sources, and provide a personalized buyer’s guide. This feature supports in-depth product comparisons, finding lookalikes, gift suggestions, and deal hunting, and is available to all logged-in ChatGPT users on Free, Go, Plus, and Pro plans during the holidays.The shopping research feature is powered by a specialized version of GPT-5 mini, trained specifically for product discovery and evaluation. It maintains user privacy, only references public retail sites, and cites its sources. While it improves on accuracy and user personalization, OpenAI notes there may still be occasional errors in details like pricing or availability, and encourages users to verify information before purchasing. The company aims to continually enhance this tool, expanding categories and refining the experience over time." src="assets/slide_2025_48/introducing-shopping-research-in-chatgpt-openai.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/Ali_TongyiLab/status/1993931277695631557" target="_blank">
   <img alt="Z-Image is a newly introduced 6-billion-parameter foundation model designed for efficient image generation. Through targeted optimizations, it demonstrates that exceptional photorealistic results and bilingual text rendering are possible without extremely large model sizes.The model’s performance is on par with leading commercial solutions, proving that high-quality image generation does not require massive computational resources." class="half-size" src="assets/slide_2025_48/z-image-efficient-6b-parameter-foundation-model.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/Ali_TongyiLab/status/1994234717302898726" target="_blank">
   <img alt="Z-Image Turbo has achieved top rankings on both the HF Models and Spaces trending lists. The creators express gratitude to the community for their support and enthusiasm.Their message highlights emotional appreciation for users who contributed to the project's success." class="half-size" src="assets/slide_2025_48/z-image-turbo-tops-hf-models-and-spaces-trending-lists.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/Xianbao_QIAN/status/1993825588935864782" target="_blank">
   <img alt="Z-Image is a newly released image generation and editing model from Alibaba, notable for its ultra-fast generation speed (sub-second on H800 hardware) and ability to run on consumer devices with 16GB RAM. The model produces rich high-frequency details and is available in a base (non-distilled) version.It features a 6B parameter size, is released under the Apache 2 license, and a demo is accessible via Hugging Face." src="assets/slide_2025_48/z-image-ultra-fast-image-generation-ai-model-alibaba.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/AdinaYakup/status/1993981325150163022" target="_blank">
   <img alt="Z-Image is a new open-source image generation model from Ali Tongyi Lab, available under Apache 2.0. It features a 6-billion parameter architecture and delivers photorealistic results, supporting both English and Chinese prompts.Key highlights include sub-second, 8-step image generation on H800 hardware (and compatibility with 16GB GPUs), as well as Turbo, Base, and Edit model variants. Demos showcase impressive outputs, such as film poster-quality images." class="half-size" src="assets/slide_2025_48/z-image-new-ai-image-generation-model-tongyi.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/Dorialexander/status/1994065600226775106" target="_blank">
   <img alt='DeepSeek-Math-V2 focuses on advancing mathematical proof verification in large language models through improved data quality and synthetic pipelines, emphasizing expert human annotation and automation. The approach highlights the limitations of earlier self-verification methods, favoring scalable techniques where both humans and guided models identify flawed reasoning without reference to final answers. A key innovation is the use of "meta-verifiers"—models that evaluate the evaluation process itself—to address reward hacking and ensure more robust verification.The training process establishes a synergy between proof generators and verifiers, using recursive RL-based improvement cycles that produce increasingly challenging proofs and stronger evaluators. This method demonstrates that integrating expert feedback and advanced synthetic data processes is crucial for pushing the boundaries of LLM reasoning capabilities, particularly in complex domains like mathematics.' class="half-size" src="assets/slide_2025_48/deepseek-math-v2.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://simonwillison.net/2025/Nov/27/deepseek-math-v2/" target="_blank">
   <img alt="DeepSeek-AI has released DeepSeek-Math-V2, a large mathematical reasoning language model (LLM) now available on Hugging Face with open weights under the Apache 2 license. This 685B parameter, 689GB model matches the gold-medal performance of proprietary models from OpenAI and Google DeepMind on the International Mathematical Olympiad (IMO) 2025, as well as other major math competitions such as the CMO 2024 and the Putnam 2024.The release marks a significant step for open-source AI in specialist domains, providing a powerful, freely available alternative for advanced mathematics tasks. DeepSeekMath-V2 demonstrates strong competition-level results, leveraging scaled test-time compute to reach near-perfect scores on undergraduate and high-school mathematics challenges." src="assets/slide_2025_48/deepseekmathv2-achieves-gold-medal-imo-putnam-deepseek.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2" target="_blank">
   <img alt="DeepSeekMath-V2 is a large language model developed by deepseek-ai, designed to advance self-verifiable mathematical reasoning. Unlike previous models that primarily optimize for final answer accuracy, DeepSeekMath-V2 prioritizes rigorous, step-by-step proofs by integrating an LLM-based verifier into its training process. The proof generator is rewarded for identifying and resolving issues within its own reasoning, and verification compute is scaled to continually improve the verifier with challenging new proofs. This approach facilitates more reliable theorem proving and self-verification, particularly for open mathematical problems.In recent evaluations, DeepSeekMath-V2 has demonstrated strong performance, earning gold-level scores on competitive mathematics benchmarks such as IMO 2025, CMO 2024, and nearly perfect results on Putnam 2024. The model is built on DeepSeek-V3.2-Exp-Base and is available under the Apache 2.0 license. For further details, users can refer to the provided GitHub repository or contact the developers directly." src="assets/slide_2025_48/deepseekmath-v2-towards-self-verifiable-mathematical-reasoning.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/flux2-image-generation-editing-model.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://bfl.ai/blog/flux-2" target="_blank">
   <img alt="FLUX.2 by Black Forest Labs is a next-generation, open-core visual intelligence model designed for real-world creative workflows and production use. It delivers high-quality image generation and editing with consistent character and style, reliable text rendering, and strong adherence to structured prompts. The model supports multi-reference input (up to 10 images), advanced photorealism, complex infographics, and resolutions up to 4MP, making it suitable for diverse use cases from product visualization to UI mockups.The FLUX.2 family includes several variants: FLUX.2 [pro] for state-of-the-art quality and speed, FLUX.2 [flex] for developer control over generation parameters, FLUX.2 [dev] as the leading open-weight image generation and editing model, and FLUX.2 [klein] (coming soon) as a size-distilled, Apache 2.0 licensed model. Powered by a latent flow architecture combining a 24B parameter vision-language model and a rectified flow transformer, FLUX.2 sets a new standard for open, inspectable, and composable visual AI, supporting sustainable open innovation and responsible development." src="assets/slide_2025_48/flux-2-frontier-visual-intelligence-black-forest-labs.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://huggingface.co/blog/flux-2" target="_blank">
   <img alt="FLUX.2 is the latest open-source image generation model from Black Forest Labs, representing a major architectural overhaul compared to its predecessor, Flux.1. Key improvements include a new single Mistral Small 3.1 text encoder, more efficient multimodal diffusion transformer (DiT) blocks with parallel processing, and architectural changes such as the removal of bias parameters and fused attention/feedforward projections. FLUX.2 enables both text-guided and image-guided generation, supports multi-image reference input, advanced prompting (including structured JSON and hex color control), and achieves high fidelity with improved flexibility.The blog details practical usage through Hugging Face Diffusers, offering memory-saving techniques like quantization, CPU offloading, and remote text encoder deployment for users with limited hardware. It also covers LoRA fine-tuning strategies, including quantized training with FP8 or QLoRA, leveraging remote modules, and other optimization methods to make training and inference accessible. Extensive code examples, links to documentation, and community Q&amp;A are provided for deeper exploration." src="assets/slide_2025_48/welcome-flux2-bfl-image-generation-model-huggingface.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="3" href="https://x.com/fal/status/1993669462550323652" target="_blank">
   <img alt="The announcement highlights the open sourcing of the 'Tiny Autoencoder' for FLUX.2 [dev], a tool that enables streaming of intermediary outputs during image generation. This improvement eliminates the need for traditional progress bars, offering a more interactive user experience.The post includes links to further information and resources related to the project." src="assets/slide_2025_48/tiny-autoencoder-open-source-flux2.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://www.microsoft.com/en-us/research/blog/fara-7b-an-efficient-agentic-model-for-computer-use/" target="_blank">
   <img alt="Microsoft has announced Fara-7B, an open-weight, 7-billion-parameter small language model (SLM) optimized for agentic computer use, such as automating web tasks by interacting with interfaces like a mouse and keyboard. Unlike traditional chat models, Fara-7B can visually perceive web pages and perform real-world tasks—like filling forms, searching, or booking—by predicting actions directly on device, reducing latency and improving privacy. The model was trained using a novel synthetic data pipeline that generated diverse, multi-step web interaction tasks, and demonstrates state-of-the-art performance in its class across several benchmarks, outperforming even larger models on some tasks.Fara-7B is available on Microsoft Foundry and Hugging Face under an MIT license, and is integrated with Magentic-UI for experimentation. Safety and responsible use are emphasized: users are advised to run it in sandboxed environments and oversight features are built in. The model logs all actions and pauses for user consent at critical points. Evaluation shows Fara-7B achieves strong accuracy and efficiency, making it a promising step toward practical, privacy-preserving computer use agents." src="assets/slide_2025_48/fara-7b-an-efficient-agentic-model-for-computer-use-microsoft.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/Ali_TongyiLab/status/1993216249523044830" target="_blank">
   <img alt="Qwen2.5-VL is being recognized for its role in powering projects such as Microsoft's Fara-7B, noted for its accurate output and quick processing. The message expresses appreciation to collaborators and excitement for future developments using the model.This announcement highlights the innovative applications and growing community support surrounding Qwen2.5-VL." class="half-size" src="assets/slide_2025_48/qwen25-vl-powers-fara-7b-microsoft.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/hunyuan-3d-engine-global-launch-tencent.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/TencentHunyuan/status/1993501416036745377" target="_blank">
  <img alt="Tencent has launched the Hunyuan 3D Engine globally, offering advanced AI-powered 3D asset generation for creators and enterprises. The platform supports multimodal input (Text-to-3D, Image-to-3D, Sketch-to-3D), leveraging a new hierarchical carving model for high-precision, ultra-HD results, and seamless integration with popular 3D tools like Unreal, Unity, and Blender.Hunyuan 3D is available via Tencent Cloud International, enabling commercial users to integrate 3D generation into workflows for gaming, e-commerce, advertising, and more. New users receive daily free generations, while enterprise API registrants get 200 free credits." src="assets/slide_2025_48/hunyuan-3d-engine-global-launch-tencent.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/pdf/2511.08892" target="_blank">
   <img alt="Lumine is an open-source framework for developing generalist AI agents capable of performing complex, hours-long missions in real time within challenging 3D open-world environments. It utilizes a unified vision-language model that processes raw visual inputs and generates real-time keyboard and mouse actions, adaptively invoking reasoning as needed. Trained on the game Genshin Impact, Lumine can efficiently complete the full Mondstadt storyline, follow natural language instructions, and handle a diverse range of tasks including exploration, combat, puzzle-solving, and GUI manipulation.Remarkably, Lumine demonstrates strong zero-shot generalization to new games, achieving successful mission completions in both Wuthering Waves and Honkai: Star Rail without additional training. This highlights its effectiveness across different game dynamics and environments, marking significant progress toward the development of versatile, human-like agents for open-ended 3D worlds." src="assets/slide_2025_48/lumine-an-open-recipe-for-building-generalist-agents-in-3d-open-worlds-bytedance.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://www.lumine-ai.org/" target="_blank">
   <img alt="Lumine is a generalist AI agent trained in the complex 3D open-world environment of Genshin Impact, capable of real-time perception, reasoning, and action to autonomously complete hours-long missions. It demonstrates strong generalization by successfully playing through entire storylines in both Mondstadt and Liyue regions, despite limited training, and further showcases cross-game adaptability by completing chapters in Honkai: Star Rail and Wuthering Waves without additional fine-tuning. Lumine’s training includes large-scale human gameplay data, language-grounded instruction, and reasoning tasks, enabling unified perception, reasoning, and action with both 3D exploration and 2D GUI manipulation.The agent excels in diverse gameplay activities such as combat, boss fights, puzzle-solving, NPC interaction, and GUI operations, mastering complex mechanics and following detailed instructions. Lumine’s curriculum-driven approach results in robust skill acquisition and in-context learning, allowing it to efficiently handle both routine and novel tasks across multiple gaming environments." src="assets/slide_2025_48/lumine-building-generalist-agents-in-3d-open-worlds.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/pdf/2509.01092" target="_blank">
   <img alt="REFRAG is a novel decoding framework designed to optimize retrieval-augmented generation (RAG) in large language models (LLMs). The key insight is that much of the retrieved context used in RAG tasks is only marginally relevant to the actual query, resulting in sparse, block-diagonal attention patterns. REFRAG leverages this sparsity by compressing, sensing, and selectively expanding the context during decoding, significantly reducing unnecessary computations without sacrificing performance.Empirical results demonstrate that REFRAG achieves up to 30.85× acceleration in time-to-first-token (a 3.75× improvement over prior work) and extends LLM context windows by 16×, all without loss in accuracy or perplexity. The framework is validated across various tasks—such as RAG, multi-turn conversations, and long document summarization—outperforming LLaMA and other state-of-the-art models in both speed and scalability." src="assets/slide_2025_48/refrag-rethinking-rag-based-decoding-meta-national-university-of-singapore-rice-university.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/techNmak/status/1993626118679892415" target="_blank">
   <img alt="Meta introduced REFRAG, a new technique that dramatically speeds up Retrieval-Augmented Generation (RAG) by compressing retrieved context passages into precomputable embeddings. This approach addresses the inefficiency of processing many irrelevant passages by allowing the model to handle more context with significantly reduced memory and computation requirements, achieving up to 30× faster decoding and supporting much longer context windows without accuracy loss.REFRAG leverages RL-based compression and precomputed embeddings, works flexibly at any context position, and requires no changes to model architecture. The result is faster response times, better performance even with weak retrievers, and the ability to handle longer conversation histories—ultimately making large-context RAG more practical and cost-effective." src="assets/slide_2025_48/refragefficientcontextcompressionmeta.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://arxiv.org/pdf/2511.21631" target="_blank">
  <img alt="Qwen3-VL is the latest and most advanced vision–language model in the Qwen series, designed to excel across a wide range of multimodal tasks. It natively supports interleaved text, image, and video contexts up to 256K tokens, and is available in both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to balance quality and latency needs. The model demonstrates superior performance in pure-text understanding, long-context comprehension, and advanced multimodal reasoning, outperforming comparable models in benchmarks such as MMMU, MathVista, and MathVision.Architectural enhancements include an improved interleaved-MRoPE for spatial–temporal modeling, DeepStack integration for tighter vision–language alignment, and text-based time alignment for more precise video understanding. Training strategies feature square-root reweighting to balance text and multimodal objectives, extended pretraining context lengths, and specialized post-training variants. Qwen3-VL is positioned as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world applications." class="half-size" src="assets/slide_2025_48/qwen3-vl-technical-report.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/intellect-3-scaling-rl-to-100b-moe-model.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/PrimeIntellect/status/1993895068290388134" target="_blank">
   <img alt="INTELLECT-3 is a new reinforcement learning (RL) model with over 100 billion parameters, utilizing a Mixture of Experts (MoE) architecture. It demonstrates state-of-the-art performance for its size in math, code, and reasoning tasks.The model was developed using the same end-to-end stack and tools available to users, including environments, evaluation frameworks, RL tools, and sandboxes." src="assets/slide_2025_48/intellect-3-scaling-rl-to-100b-moe-model.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf" target="_blank">
   <img alt='INTELLECT-3 is a 106-billion-parameter Mixture-of-Experts language model (with 12 billion active parameters per inference) developed by the Prime Intellect team. Trained using large-scale reinforcement learning (RL) on a custom infrastructure stack, INTELLECT-3 achieves state-of-the-art performance for its size across math, code, science, and reasoning benchmarks, outperforming several larger frontier models. The model and its full RL infrastructure—including frameworks, training recipe, and a diverse set of environments—are released as open source.A key innovation is "prime-rl," an open RL framework that supports large-scale asynchronous training, multi-turn interactions, and tool use, scaling from a single node to thousands of GPUs. INTELLECT-3 was trained with both supervised fine-tuning (SFT) and RL atop the GLM-4.5-Air-Base model, utilizing up to 512 H200 GPUs. The technical report details evaluation results and highlights the open-source release of both the model and the stack used for its creation.' src="assets/slide_2025_48/intellect-3-technical-report.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.microsoft.com/en-us/research/blog/reducing-privacy-leaks-in-ai-two-approaches-to-contextual-integrity/" target="_blank">
  <img alt="Microsoft Research published a blog post on reducing privacy leaks in AI, focusing on ensuring that AI agents adhere to contextual integrity—the principle that information sharing should fit the specific social context. The post highlights that large language models (LLMs) often lack this contextual awareness, posing privacy risks by potentially disclosing sensitive data inappropriately. Two research efforts are presented: PrivacyChecker, a model-agnostic, inference-time module that extracts and evaluates information flows in agent actions, reducing leakage rates across various benchmarks and real-world scenarios; and a reasoning-based approach where contextual integrity is built directly into the model via chain-of-thought prompting and reinforcement learning, helping models evaluate whether sharing information is appropriate before responding.Together, these approaches demonstrate practical methods for embedding privacy-preserving norms in AI systems. PrivacyChecker provides an external check that can be integrated flexibly into agent workflows, while the reasoning and reinforcement learning strategies train models to balance task completion with privacy constraints. Evaluation results show substantial reductions in information leakage with minimal impact on task performance, marking progress toward AI agents that respect user privacy in real-world, multi-tool environments." src="assets/slide_2025_48/reducing-privacy-leaks-in-ai-two-approaches-to-contextual-integrity-microsoft.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://arxiv.org/pdf/2511.08394" target="_blank">
  <img alt="This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel approach for aligning Large Language Models (LLMs) in multi-turn conversations by using reward signals derived from the geometric properties of dialogue trajectories in semantic embedding space, termed “conversational geometry.” Unlike traditional methods that rely solely on analyzing textual content, TRACE focuses on the dynamics—how an interaction unfolds—capturing implicit cues like responsiveness and conversational flow, which are often more indicative of user satisfaction.The authors demonstrate that a reward model trained only on these interaction dynamics achieves accuracy comparable to strong text-based baselines and that combining both approaches yields superior results. This suggests that the structural patterns of conversation are as important as content for evaluating collaborative success. Furthermore, the TRACE framework is scalable, computationally efficient, privacy-preserving, and offers new insights for both aligning AI agents and diagnosing what makes interactions successful." src="assets/slide_2025_48/interaction-dynamics-as-a-reward-signal-for-llms-google-deepmind.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://morphllm.com/blog/claude-code-mcp-servers" target="_blank">
  <img alt="The article addresses two major productivity bottlenecks in Claude Code: slow file rewrites for simple edits and sluggish, serial code search that pollutes context and breaks developer flow. To solve these, Morph introduces an MCP server with two sub-agents: FastApply, which merges code edits in under a second, and Warp-Grep, which enables parallel code search across multiple files and patterns simultaneously. This results in significantly faster feedback loops, cleaner context for the AI model, and a substantial reduction in context-switching and lost developer focus.Real-world benchmarks show dramatic improvements in editing and searching speed, allowing developers to stay in flow and ship features up to 2-3x faster. The installation is straightforward, requiring a simple config update and API key. Morph's approach leverages parallelism and targeted context management, ensuring high accuracy and fewer distractions during coding tasks. The tools are available on a usage-based pricing model with a free tier for trial." class="half-size" src="assets/slide_2025_48/claude-code-mcp-fix-the-2-things-that-kill-your-flow.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.anthropic.com/research/prompt-injection-defenses" target="_blank">
  <img alt="Claude Opus 4.5 introduces enhanced robustness against prompt injection attacks—malicious instructions hidden in web content targeting AI browser agents. As AI agents increasingly perform real-world tasks like browsing and email management, the risk of prompt injections grows, especially due to the vast attack surface of the web and the agents’ ability to take diverse actions. Claude’s latest browser extension, now available in beta for Max plan users, demonstrates significant improvements over previous versions, reducing attack success rates but acknowledging that risk remains.Key advances include reinforcement learning to train Claude to resist prompt injections, improved classifiers to detect adversarial content, and ongoing human red teaming to uncover vulnerabilities. Despite these safeguards, prompt injection remains an open challenge, and Anthropic commits to continued research, transparency, and industry collaboration to further strengthen AI agent security in adversarial environments." src="assets/slide_2025_48/mitigating-the-risk-of-prompt-injections-in-browser-use-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2" target="_blank">
  <img alt="Spotify's engineering team shares their journey in developing an effective background coding agent to automate code migrations across thousands of repositories. Early experiments with open-source agents like Goose and Aider revealed limitations in scaling and reliability, prompting Spotify to build an in-house &quot;agentic loop&quot; using LLM APIs. This loop improved results for simple code changes but struggled with complex, multi-file edits and context management. Transitioning to Anthropic's Claude Code enabled more natural, outcome-oriented prompts and dynamic task management, significantly enhancing the agent's capabilities for large-scale migrations.Key lessons include the importance of prompt engineering—tailoring instructions to the agent, clearly stating preconditions, providing examples, defining verifiable outcomes, and focusing on one change at a time. Spotify limits tool access for their agent to maintain predictability and reliability, relying on static, version-controlled prompts and minimal dynamic context fetching. The team continues to iterate, recognizing that prompt writing remains an intuitive, trial-and-error process, and plans to further refine feedback loops and evaluation methods for even more consistent results." src="assets/slide_2025_48/context-engineering-for-background-coding-agents-spotify.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://dlouapre-eiffel-tower-llama.hf.space/the-eiffel-tower-llama.pdf" target="_blank">
   <img alt="David Louapre from Hugging Face discusses the reproduction of the Golden Gate Claude experiment using open-source models. The original experiment by Anthropic in May 2024 involved steering the behavior of Claude Sonnet to consistently reference the Golden Gate, not through prompt engineering but by modifying model activations at inference time with sparse autoencoders.Despite its popularity, the Golden Gate Claude demo was only available for 24 hours and has not been publicly replicated. The article aims to establish a methodology for recreating this behavioral steering technique with open-source language models." src="assets/slide_2025_48/the-eiffel-tower-llama.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/Thom_Wolf/status/1993777786402820394" target="_blank">
   <img alt="A new open-science post details the team's first work on mechanistic interpretability, focusing on reproducing the &quot;Golden Gate Claude&quot; experiments. Key findings include the discovery that the effective range for activation steering is narrow, with optimal steering strength being about half the typical activation magnitude. Clamping activations, rather than simply adding to them, proved more effective for incorporating concepts without reducing fluency. Interestingly, steering multiple features did not significantly outperform steering a single well-chosen feature.The study also found that while advanced methods like SAE steering show some promise, they still do not match the effectiveness of straightforward prompt-based instructions. The results highlight both the challenges and progress in practical mechanistic interpretability, with a full interactive blog post and demo available for further exploration." src="assets/slide_2025_48/mechanistic-interpretability-golden-gate-claude-open-science-experiment.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the" target="_blank">
  <img alt="Google's TPUv7 (Ironwood) marks a major strategic shift as Google aggressively commercializes its AI accelerators, selling both cloud access and physical systems to external customers like Anthropic, Meta, SSI, xAI, and potentially OpenAI. This move threatens Nvidia's dominance by offering comparable or better Total Cost of Ownership (TCO) and performance, especially for large-scale AI workloads. Anthropic's landmark deal includes purchasing over 1 million TPUs (400k bought directly, 600k rented via GCP), enabled by Google’s advanced system and network architecture, notably its scalable, low-cost, reconfigurable ICI 3D torus interconnect supporting up to 9,216 TPUs per cluster.Google is also overhauling its software strategy to improve external usability, with significant investments in native PyTorch support and integration with open inference frameworks (vLLM, SGLang), though gaps remain due to the lack of open-sourcing for key TPU compilation and runtime components. The report argues that Google's system-level engineering, competitive pricing, and growing software ecosystem now offer a credible alternative to Nvidia, with major implications for cloud AI economics, industry hardware strategies, and the future of large-scale AI infrastructure." src="assets/slide_2025_48/tpuv7-google-takes-a-swing-at-the-king-semianalysis-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/GoogleAIStudio/status/1994480371061469306" target="_blank">
  <img class="half-size" src="assets/slide_2025_48/on9ji5c1xh.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/lianapatel_/status/1869454886351389001" target="_blank">
   <img alt="LOTUS is an open-source query engine developed at Stanford and Berkeley designed to simplify and accelerate LLM-powered data processing. It enables users to write data programs as easily as using Pandas, while optimizing execution for significant speed improvements—up to 400x faster.The release of LOTUS 1.0.0 introduces new features that enhance the speed, ease, and effectiveness of data reasoning. The project is publicly available, and users can access the code to leverage its powerful capabilities for their own data workflows." class="half-size" src="assets/slide_2025_48/lotus-llm-powered-query-engine-stanford-berkeley.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://lotus-data.github.io/" target="_blank">
   <img alt="LOTUS is an LLM-powered query engine designed to simplify and accelerate data and document processing using AI. It offers an intuitive Python package with a Pandas-like API, enabling users to apply high-level semantic operators for advanced text, document, and data analytics. LOTUS seamlessly integrates with databases and document stores, extending the relational model to handle both structured and unstructured data. Its semantic operator model and optimized execution can achieve significant speedups—up to 400x—over standard LLM-based workflows.Typical use cases for LOTUS include document fact-checking, ETL and classification tasks, document search and ranking, and research insights extraction. The system achieves state-of-the-art accuracy and efficiency, outperforming existing methods in both accuracy and speed. LOTUS is developed by researchers from Stanford and UC Berkeley and is available with comprehensive documentation, open-source code, and community support." src="assets/slide_2025_48/lotus-llm-powered-document-and-data-processing-engine-stanford-berkeley.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://huggingface.co/blog/continuous_batching" target="_blank">
  <img alt="The article explains &quot;continuous batching,&quot; an advanced technique for efficiently serving large language models (LLMs) to many users simultaneously. It builds up from core concepts like the attention mechanism, where each token's relationship to others is governed by the attention mask, and introduces KV caching, which saves computation by storing key and value projections for already-processed tokens. Chunked prefill enables long prompts to be processed in memory-constrained environments by splitting them into manageable chunks, leveraging the KV cache for efficiency.Traditional batched inference with fixed-size batches leads to significant inefficiencies due to padding, especially when serving variable-length or asynchronous user requests. Continuous batching solves this by combining &quot;ragged batching&quot; (concatenating sequences with custom attention masks to avoid inter-sequence interaction and padding waste) and dynamic scheduling (immediately filling finished slots with new incoming requests). This approach maximizes GPU usage and throughput by mixing prefill and decode phases within a single batch, enabling modern LLM services to efficiently handle thousands of concurrent requests." src="assets/slide_2025_48/continuous-batching-huggingface.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/mixpanel-incident/" target="_blank">
  <img alt="OpenAI announced a recent security incident involving Mixpanel, a third-party analytics provider previously used for web analytics on OpenAI’s API platform. On November 9, 2025, Mixpanel detected unauthorized access to part of their systems, resulting in an export of limited customer identifiable information and analytics data. The affected data included API account names, email addresses, coarse location info, browser and operating system details, referring websites, and organization/user IDs. No OpenAI systems were breached, and sensitive information such as chat data, API keys, passwords, payment details, or government IDs was not exposed.In response, OpenAI removed Mixpanel from production services, reviewed affected datasets, and began notifying impacted users and organizations. The company emphasized that the compromised information could potentially be used for phishing or social engineering attacks and advised users to remain vigilant. OpenAI is conducting further security reviews of its vendor ecosystem and has elevated security requirements for partners. Impacted users will be updated as new information becomes available, and questions can be directed to mixpanelincident@openai.com." src="assets/slide_2025_48/mixpanel-security-incident-openai.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://www.anthropic.com/engineering/advanced-tool-use" target="_blank">
   <img alt="Anthropic has introduced three new beta features for advanced tool use on the Claude Developer Platform: the Tool Search Tool, Programmatic Tool Calling, and Tool Use Examples. The Tool Search Tool enables Claude to discover and load tools on-demand, dramatically reducing context window usage and improving tool selection accuracy for large libraries. Programmatic Tool Calling allows Claude to orchestrate workflows in code, keeping intermediate results out of context and enabling efficient, reliable execution of multi-step tasks. Tool Use Examples let developers provide sample tool calls, teaching Claude correct usage patterns that go beyond schema definitions.These features address major challenges in building scalable AI agents, such as context bloat, inefficient data processing, and parameter errors. Internal testing shows significant improvements in token savings, latency, and accuracy across complex workflows. Anthropic recommends adopting these features strategically according to the bottlenecks in your system, with API documentation and SDK examples available for integration." class="half-size" src="assets/slide_2025_48/introducing-advanced-tool-use-on-the-claude-developer-platform-anthropic.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents" target="_blank">
   <img alt="Anthropic's research addresses the challenge of enabling long-running AI agents, such as those powered by the Claude Agent SDK, to make consistent progress on complex tasks that span multiple context windows. Traditionally, agents struggle due to session-based memory loss, leading to unfinished work and inefficiencies. To solve this, Anthropic implemented a two-part agent harness: an initializer agent sets up the environment and project scaffolding (including scripts, progress logs, and a feature list), while a coding agent iteratively implements features, documents progress, and maintains a clean codebase using tools like git and structured JSON files.Key practices include incremental feature development, rigorous environment and progress management, and explicit end-to-end testing with browser automation. These strategies ensure each coding session starts with full context and leaves the project in a maintainable state. While effective for web app development, the approach may benefit from specialized agents for tasks like testing or QA, and future work will explore generalizing these methods to other domains." class="half-size" src="assets/slide_2025_48/effective-harnesses-for-long-running-agents-anthropic.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf" target="_blank">
   <img alt="Claude Opus 4.5 is a system card released by Anthropic in November 2025. The card provides information about the capabilities and features of the Claude Opus 4.5 model.For further details, users are directed to visit anthropic.com, which likely contains comprehensive documentation and resources related to the system." src="assets/slide_2025_48/claude-opus-4-5-system-card-anthropic.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/scaling01/status/1993031017582784514" target="_blank">
   <img alt='Claude models are criticized for their ineffective reasoning abilities, as suggested by the statement that "thinking is basically useless" for them. The post implies that these models may struggle with tasks that require advanced thought or complex problem-solving.The content likely raises concerns about the reliability of Claude models in handling tasks that depend on strong reasoning skills, suggesting limitations in their current capabilities compared to expectations or other AI models.' src="assets/slide_2025_48/thinking-is-basically-useless-for-claude-models.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/scaling01/status/1993027559194497134" target="_blank">
   <img alt="Claude 4.5 Opus pricing has been announced, with options at $5 and $25. This update signifies a new development in the product's availability.The announcement is accompanied by a celebratory message and a link for further details." src="assets/slide_2025_48/claude-4-5-opus-pricing-anthropic.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/alexalbert__/status/1993038631901315549" target="_blank">
   <img alt="OpenAI is introducing three new API features designed to help developers build agents that efficiently handle hundreds of tools without excessive context: Tool Search Tool, Programmatic Tool Calling, and Tool Use Examples. These features streamline tool discovery, automate tool invocation, and provide practical usage examples.Together, these updates aim to improve scalability and usability for agent-based applications, enabling smoother integration of diverse tools while minimizing context overload." src="assets/slide_2025_48/openai-agent-api-features-scalable-tool-integration.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="3" href="https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices" target="_blank">
   <img alt="This guide details prompt engineering best practices for Claude 4.x models, especially Sonnet 4.5, Haiku 4.5, and Opus 4.5. It emphasizes the importance of explicit, context-rich instructions and the use of structured examples to leverage the models' improved instruction following and state tracking. Key strategies include managing long-horizon reasoning across multiple context windows, providing clear formatting preferences, and guiding tool use and agent behaviors through precise prompts. It also covers how to encourage or limit parallel tool execution, minimize unnecessary file creation, avoid excessive focus on test-passing or hard-coding, and steer the model towards creative, non-generic outputs in tasks like frontend design.Additional guidance addresses research workflows, subagent orchestration, leveraging extended thinking capabilities, and optimizing document creation and vision tasks. The guide recommends being specific about desired behaviors, using modifiers to enhance output quality, and explicitly requesting features. Throughout, sample prompts are provided to help users shape Claude’s actions, response format, and communication style, ensuring robust, efficient, and user-aligned outcomes." src="assets/slide_2025_48/claude-4-5-prompting-best-practices-anthropic.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="4" href="https://ampcode.com/news/opus-4.5" target="_blank">
   <img alt="Claude Opus 4.5 is now the main model in Amp's `smart` mode, replacing Gemini 3 just a week after its historic rollout. While Gemini 3 impressed with its capabilities, it also revealed frustrating issues for users, leading to increased costs and undesirable behaviors. Opus 4.5 offers similar strengths to Gemini 3 but with greater polish, faster speeds, and significantly improved cost-efficiency, especially for longer threads, due to fewer mistakes and less wasted computation.Internal evaluations show Opus 4.5 outperforms both Sonnet 4.5 and Gemini 3 Pro in accuracy and reliability while maintaining competitive costs. Though Opus tokens are more expensive, overall operational costs are reduced thanks to its efficiency. Users are encouraged to transition to Opus 4.5 for better performance, especially if they encountered limitations with previous models." src="assets/slide_2025_48/opus-4-5-better-faster-often-cheaper-amp.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/warpgrep-fast-coding-agent-context-retrieval.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/morphllm/status/1994484969050444103" target="_blank">
  <img alt="WarpGrep is a fast context subagent designed to enhance coding agent performance by accelerating coding tasks by 40% and reducing context degradation by 70% during extended tasks. It achieves these improvements by treating context retrieval as a separate, reinforcement learning-trained system.Inspired by Cognition’s SWE-Grep, WarpGrep is now available for integration with Claude Code, Codex, OpenCode, or any coding agent through MCP or its SDK." src="assets/slide_2025_48/warpgrep-fast-coding-agent-context-retrieval.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/teortaxesTex/status/1994207371565125685" target="_blank">
  <img alt="The post defends Prime Intellect (PI) against criticism, arguing that advancing the post-training of Chinese base models to frontier quality is currently more important than pretraining new Western models. The author asserts that while Western labs have adequate compute and expertise, the real challenge is utilizing the growing number of available high-quality base models—particularly from China—rather than building yet another base model from scratch. They highlight the lack of meaningful open-source progress using these models, criticizing the community for failing to create impactful derivatives or innovations, and lamenting that available resources often go unused.The author praises Prime Intellect for focusing on post-training, signal design, and environment generation, which unlocks the latent value in existing models and serves the broader goal of democratizing AI. They argue this work is essential to counter centralization of AI power and is more geopolitically significant than the US-China competition; instead, the true race is between centralized entities and open, human-driven efforts. The author urges the community to recognize and support such contributions rather than dismiss them." src="assets/slide_2025_48/prime-intellect-open-source-models-geopolitical-significance.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/SebastienBubeck/status/1994087135281102903" target="_blank">
  <img alt="The author discusses the diminishing usefulness of average benchmark scores for evaluating language models (LLMs), suggesting that future assessments should focus on the &quot;argmax&quot;—the single best output a model can produce. They argue that, in some cases, achieving an exceptional result even once can be more important than consistent performance.With this perspective, the author introduces an account of the most impressive LLM output they've ever witnessed, implying a shift in how we value and measure model capabilities." src="assets/slide_2025_48/most-impressive-llm-output.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/ilyasut/status/1994424504370581726" target="_blank">
   <img alt="Scaling the existing approach will consistently yield further improvements and progress, rather than reaching a plateau. However, despite these advancements, a crucial element will still be absent from the current trajectory.This suggests that while continued scaling is beneficial, it alone may not address fundamental limitations or gaps in the system. Identifying and addressing what is missing remains essential for truly transformative change." src="assets/slide_2025_48/scaling-ai-limitations.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/polynoamial/status/1994439121243169176" target="_blank">
   <img alt="The mainstream social media debate on AI often presents two extreme views: skeptics who dismiss AI progress and fanatics who believe superintelligent AI is imminent. However, leading AI researchers largely agree that current AI technologies are already capable of significant societal and economic transformation, even without further breakthroughs.Most experts believe that additional advancements—such as continual learning and improved sample efficiency—are needed to achieve AGI or ASI, but anticipate these could be realized within the next 5 to 20 years. The main differences among researchers are about which breakthroughs are necessary and the timeline for their arrival, with general consensus that ASI is neither a fantasy nor centuries away." class="half-size" src="assets/slide_2025_48/ai-researcher-consensus-on-agi-timeline.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/daniel_mac8/status/1993879903662489775" target="_blank">
   <img alt="OpenAI's strategy for model development is evolving due to economic and computational constraints. While scaling up pretraining continues to improve models, progress is slowing as they approach the top of the &quot;S-curve.&quot; This has led OpenAI to prioritize more efficient models that maintain performance without requiring excessive resources.Around the launch of GPT-4, all available compute was used for training, but the success of ChatGPT shifted GPU resources toward inference. OpenAI expects a significant increase in GPU availability by 2026, and plans to revisit large model training using distillation techniques in the coming year. Despite challenges, there are no insurmountable barriers to future progress." class="half-size" src="assets/slide_2025_48/openai-pretraining-update-openai.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://aiengineering.academy/AIBreakDown/TRM/" target="_blank">
  <img alt="Tiny Recursive Models (TRM) are a novel AI architecture designed for structured reasoning tasks (like Sudoku, mazes, and ARC-AGI puzzles) where traditional large language models (LLMs) fail. Rather than relying on billions of parameters, TRM uses a compact 7M parameter network and achieves superior accuracy by recursively processing inputs 21 times with the same weights, emulating human-like iterative problem solving. The model maintains two distinct latent states: one for the current solution (z_H) and another for reasoning workspace (z_L), updating each at different rates to separate hypothesis refinement from exploratory logic. This weight reuse enforces generalization and prevents overfitting, making TRM highly efficient when training data is limited.TRM introduces memory-efficient training via deep supervision, only backpropagating gradients through the final recursive cycle, and adapts computation effort based on problem difficulty. Benchmarks show TRM dramatically outperforms much larger LLMs—achieving 87.4% accuracy on extreme Sudoku (vs. 0% for GPT-4 and DeepSeek R1) and strong results on abstract reasoning and maze-solving tasks. While not a general-purpose model, TRM demonstrates that tailored architectures and recursive reasoning can surpass brute-force scaling, especially in domains with limited, structured data." src="assets/slide_2025_48/less-is-more-recursive-reasoning-with-tiny-networks.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/from-software-engineer-to-ai-environment-architect.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/BeidiChen/status/1993751082057535629" target="_blank">
   <img alt='A new blog post introduces "Vortex," a framework that demonstrates how the future of engineering is shifting from pure coding to designing AI environments where agents can think, build, and evolve. By architecting the right environment for LLM serving systems, AI agents are now able to generate and implement new Sparse Attention algorithms autonomously, achieving up to 4× performance improvements—work that would typically take weeks for human engineers.The authors argue that such environments enable AI agents to make meaningful engineering contributions today and will serve as learning playgrounds for even more advanced agents in the future. The blog post shares insights from leading AI thinkers and includes a demo, code, and documentation for Vortex.' class="half-size" src="assets/slide_2025_48/from-software-engineer-to-ai-environment-architect.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://infini-ai-lab.github.io/ai-environment-architect/" target="_blank">
   <img alt='The article discusses a paradigm shift in software engineering prompted by advances in AI, particularly in the era of increasingly capable large language models. As AI agents become proficient at implementing well-scoped tasks like writing kernels or functions, the traditional role of software engineers is evolving from "implementers" to "environment architects." Human engineers are now tasked with designing the abstractions, boundaries, and interfaces that allow AI agents to safely and efficiently operate within complex systems, thereby enabling both current productivity and future learning for autonomous agents.A case study of the Vortex system illustrates this transformation: by providing robust abstraction layers and operator interfaces, engineers empower AI agents to implement and optimize advanced algorithms (such as Sparse Attention for LLM serving) without requiring deep system-wide understanding. This approach not only boosts throughput and productivity today but also sets the stage for future AI agents to learn and reason in more complex, real-world software environments. The article concludes that the main question for engineers has shifted from "how do I implement this feature?" to "how do I design a codebase so an AI agent can implement, test, and iterate on this feature for me?"' src="assets/slide_2025_48/from-software-engineer-to-ai-environment-architect-cmu.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="3" href="https://www.philschmid.de/why-engineers-struggle-building-agents" target="_blank">
   <img alt="The article discusses why senior engineers often struggle to build effective AI agents, highlighting the clash between traditional deterministic software engineering and the probabilistic, flexible nature of agent-based systems. Key challenges include letting go of strict data structures in favor of representing state and preferences as text, handing over control flow to agents rather than hard-coding logic, treating errors as recoverable inputs instead of fatal exceptions, replacing rigid unit tests with evaluation-based approaches, and designing explicit, semantically clear APIs for agents rather than relying on implicit context.The author argues that building resilient agentic systems requires embracing ambiguity and probabilistic behavior, using evaluations and self-correction rather than attempting to eliminate all variance. Trusting agents to handle complexity is essential, but it should be balanced with rigorous evaluation and fallback workflows. Ultimately, the transition demands a mindset shift: senior engineers must adapt their habits to engineer for robustness in the face of uncertainty, rather than deterministic certainty." src="assets/slide_2025_48/why-senior-engineers-struggle-to-build-ai-agents-philschmid.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="4" href="https://x.com/cfregly/status/1993521789910368649" target="_blank">
   <img alt="A recent talk by @vibhuuuus explored the differences between Hyper Engineering and Normie Engineering. The main idea is that Hyper Engineers organize their workflow around advanced async code generation tools like @cursor_ai, @OpenAI, and @claudeai.The speaker highlighted how these tools are central to the productivity and planning of Hyper Engineers, distinguishing them from more traditional engineering approaches. The audience responded positively to this perspective." class="half-size" src="assets/slide_2025_48/hyper-engineering-vs-normie-engineering-talk.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="5" href="https://x.com/ryolu_/status/1993408555022758357" target="_blank">
   <img alt="The traditional approach of scaling teams by hiring specialists in isolated roles is being replaced by a new model, driven by AI tools like Cursor that eliminate execution bottlenecks. Now, success hinges on individuals with both broad domain knowledge and deep expertise in areas AI can’t replicate, emphasizing qualities like judgment, taste, systems thinking, and learning agility.As AI enables seamless collaboration and reduces the divide between disciplines, small teams with versatile members who can navigate design, code, and product work outperform large groups of specialists. The most critical skills are now human—vision, judgment, and context—while tools handle execution, empowering teams to focus on creativity and exploration." src="assets/slide_2025_48/ai-driven-team-scaling-cursor.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/petergostev/status/1993697283448197583" target="_blank">
  <img alt='The assessment suggests that OpenAI currently leads in "thinking" (reasoning) models but lags behind competitors like Google and Anthropic in "non-thinking" (base) models, mainly due to a strategic pivot from large-scale pre-training to reinforcement learning (RL) and reasoning-focused development. While OpenAI’s recent models (GPT-4o, GPT-5, etc.) have not meaningfully scaled pre-training over GPT-4, competitors have continued advancing their base models, making their non-reasoning outputs superior in speed and general utility. This difference is leading to practical issues for OpenAI, as their models are slower for day-to-day tasks, potentially losing market share despite better final answers.Looking ahead, the key question is whether OpenAI has a new, more powerful pre-trained model ready to deploy. If not, it could take them over six months to catch up in base model performance. The assessment highlights an opportunity for OpenAI to leverage their strong reasoning models for high-quality synthetic data generation, which could make future base models highly competitive if integrated effectively. Combining the strengths of advanced base models from competitors with OpenAI’s superior reasoning could set a new benchmark in AI capabilities.' class="half-size" src="assets/slide_2025_48/openai-gpt5-vs-anthropic-google-future-ai-competition.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/eli_lifland/status/1993050674729488527" target="_blank">
   <img alt="Opus 4.5 is reported as a significant advancement in AI research and development, particularly in coding capabilities. Employees claim it provides a twofold productivity boost and outperforms humans on tasks lasting 4-8 hours.Further independent evaluations and real-world performance reports are anticipated to validate these claims." class="half-size" src="assets/slide_2025_48/opus-4-5-ai-rnd-coding-capabilities-boost.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://simonwillison.net/2025/Nov/24/claude-opus/" target="_blank">
   <img alt="Simon Willison reviews the release of Claude Opus 4.5, Anthropic’s latest large language model, which boasts improvements in coding, agent capabilities, pricing, and safety features over previous versions. Opus 4.5 offers a 200,000 token context window, a 64,000 token output limit, and a lower price point, positioning it competitively against recent releases from OpenAI and Google. Notable enhancements include a tunable effort parameter for response speed, improved computer-use tools, and better context preservation.Despite its advancements, Willison finds it increasingly difficult to meaningfully evaluate new LLMs, as real-world differences between top models have become less obvious. He highlights that benchmarks now show only marginal gains, making it challenging to identify concrete, practical improvements. Willison suggests that both users and AI labs should maintain collections of tasks that frontier models struggle with, and calls for more illustrative examples in model announcements to better showcase genuine progress. He also notes that, while Opus 4.5 is less vulnerable to prompt injection attacks than competitors, application design should still assume that such vulnerabilities can be exploited." src="assets/slide_2025_48/claude-opus-4-5-prompt-injection-anthropic.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/DimitrisPapail/status/1994381944272908366" target="_blank">
   <img alt="The text compares different approaches to reasoning, suggesting that code operates more like a reasoning chain-of-thought (CoT) than a mathematical proof. It also implies that simply relying on an AI model like Claude for thinking is not equivalent to genuine reasoning.Overall, the emphasis is on the nature of code as a reasoning process, distinguishing it from traditional math proofs and cautioning against overestimating AI's capacity for deep thought." class="half-size" src="assets/slide_2025_48/reasoning-in-coding-vs-math-proof.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/alexalbert__/status/1993365963706913257" target="_blank">
   <img alt="Based on extensive internal testing by research and applied AI teams, a prompting guide for Claude Opus 4.5 has been developed. This guide shares practical insights and recommendations for crafting effective prompts to achieve optimal results with the model.The key findings focus on strategies and best practices learned through hands-on evaluation, aiming to help users maximize the performance and accuracy of Claude Opus 4.5 in various applications." src="assets/slide_2025_48/claude-opus-4-5-prompting-guide.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://leoniemonigatti.com/blog/claude-memory-tool.html" target="_blank">
  <img alt="This blog post demonstrates how to add persistent memory to AI agents using Anthropic’s new memory tool (beta) in the Claude Developer Platform and Python SDK. The author walks through building a barista agent that can remember customers and their usual orders across sessions, highlighting the limitations of stateless agents and the improved user experience with memory. The memory tool operates as a local file directory, with agents able to create, edit, and recall information via tool calls managed by client-side handlers.Step-by-step code examples show the process of setting up the Anthropic SDK, implementing the memory tool’s command handlers, and integrating memory operations into the agent’s workflow using the tool_runner API. The post contrasts agent behavior with and without memory, illustrating how the agent persistently recalls customer info across conversations and updates it as needed. Anthropic’s memory tool stands out for its file-based, developer-controlled approach, which supports custom memory management and long-running workflows." src="assets/slide_2025_48/exploring-anthropic-memory-tool-leonie-monigatti.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/abh1a0/status/1993033150323392720" target="_blank">
  <img alt="Based on the provided transcription, the main content centers around a specific topic or event discussed at the link. It highlights key details, participants, or outcomes relevant to the subject matter, offering context or background information to inform the reader.Additional information provides insights or commentary that help clarify the significance of the topic. The summary avoids extraneous details such as web menus, cookie notices, or footer content, focusing solely on the essential points presented." src="assets/slide_2025_48/reinforcementlearninghumanfeedbackopenai.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_48/the-thinking-game-alphafold-anniversary-deepmind.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/GoogleDeepMind/status/1993714943116386619" target="_blank">
   <img alt="AlphaFold is celebrating its five-year anniversary by releasing &quot;The Thinking Game&quot; documentary on YouTube. The film offers an inside look at the journey, challenges, and key breakthroughs that contributed to solving a major, decades-old problem in biology.The documentary is available to stream for free, providing viewers with insight into the scientific and personal stories behind AlphaFold's success." src="assets/slide_2025_48/the-thinking-game-alphafold-anniversary-deepmind.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://www.youtube.com/watch?v=d95J8yzvjbQ" target="_blank">
   <img alt="The Thinking Game takes you on a journey into the heart of DeepMind, capturing a team striving to unravel the mysteries of intelligence and life itself.Filme..." src="assets/slide_2025_48/slide-2025-48-025.jpg"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/LucSGeorges/status/1994419420924391456" target="_blank">
  <img alt="When asked how Hugging Face (HF) generates revenue, the content suggests a humorous or evasive response, likely referencing the common curiosity around the company's business model. The link indicates that the explanation may be conveyed through a meme or comedic format.Overall, the main focus is the recurring question about HF's monetization strategy and the often lighthearted or ambiguous way it is addressed." src="assets/slide_2025_48/how-hugging-face-makes-money.webp"/>
 </a>
</section>


      <section>
        <div class="fragment fade-out" data-fragment-index="-1"
          style="align-items: center; justify-content: center; min-height: 70vh; margin: 0; font-size: 50%; margin: auto; display: flex; flex-direction: column;">
          <div style="margin-top: 10px; text-align:center;">
            <img src="assets/qr.png" height="100px" /> <br/>
            📚 <a href="index_all.html"><b>view all previous issues</b></a><br/>
            ✨ see you next week!
          </div>
        </div>
      </section>

    </div>
  </div>

  <script>
      let deck = new Reveal();
      deck.initialize({controls: true, progress: false, hash: true, plugins: [RevealScrollingImage]});
  </script>
</body>
</html>