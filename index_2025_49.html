<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="initial-scale=1.0">
    <meta name="description" content="" />
    <meta property="og:title" content="" />
    <meta property="og:description" content="" />
    <meta property="og:image" content="" />

    <title>ML NEWS / 2025 / 49th week</title>
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/night.css" />
    <link rel="stylesheet" href="dist/slider.css" />
    <link rel="alternate" type="text/html" href="index_2025_49.html?print-pdf" title="Printer-friendly version">

</head>
<body>
  <script src="dist/reveal.js"></script>
  <script src="dist/scrolling-image.js"></script>

  <div class="reveal">
    <div class="slides">
      <!-- PRODUCTS -->

      <!-- PAPERS -->

      <!-- OTHER -->


<section data-background-size="contain" data-background-video="assets/slide_2025_49/gemini-3-deep-think-mode-google.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/Google/status/1996658356686074130" target="_blank">
  <img alt="Google is updating its Deep Think mode for Gemini 3, making it available to Google AI Ultra subscribers in the Gemini app. This enhanced mode leverages the advanced capabilities of Gemini 3, Google's most intelligent AI model.With this update, users can tackle more complex challenges using the improved thinking power provided by Gemini 3." src="assets/slide_2025_49/gemini-3-deep-think-mode-google.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/gemini-3-deep-think-mode-update-google.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/GoogleAI/status/1996657213390155927" target="_blank">
   <img alt="Gemini has introduced an updated Deep Think mode in the Gemini app for Google AI Ultra subscribers. This new mode, powered by Gemini 3, enhances reasoning capabilities by using advanced parallel thinking to explore multiple hypotheses at once.Building on the achievements of Gemini 2.5, which excelled in global math and programming competitions, the new Deep Think mode offers subscribers improved intelligence and problem-solving. Ultra subscribers can access it by selecting “Deep Think” in the prompt bar and choosing Gemini 3 Pro in the model selector." class="half-size" src="assets/slide_2025_49/gemini-3-deep-think-mode-update-google.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://blog.google/products/gemini/gemini-3-deep-think/?utm_source=linkedin&amp;utm_medium=social&amp;utm_campaign=og&amp;utm_content=&amp;utm_term=" target="_blank">
   <img alt="Google has launched Gemini 3 Deep Think mode for Google AI Ultra subscribers within the Gemini app. This new mode significantly advances the model's reasoning abilities, enabling it to better solve complex math, science, and logic problems. Gemini 3 Deep Think achieves industry-leading results on benchmarks like Humanity’s Last Exam and ARC-AGI-2 by using advanced parallel reasoning to explore multiple hypotheses at once, building on previous successes at major international competitions.Ultra subscribers can access this feature by selecting &quot;Deep Think&quot; in the prompt bar and choosing Gemini 3 Pro in the model dropdown. The update is part of Google’s ongoing efforts to enhance AI capabilities and provide users with more powerful problem-solving tools." src="assets/slide_2025_49/gemini-3-deep-think-text-gemini-logo-google.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/gpt-5-1-codex-max/" target="_blank">
  <img alt="OpenAI has introduced GPT‑5.1-Codex-Max, a new advanced agentic coding model designed for long-running, complex software engineering tasks. Built on an updated foundational reasoning model, GPT‑5.1-Codex-Max offers greater speed, intelligence, token efficiency, and the ability to operate across multiple context windows through a process called compaction. This enables it to handle project-scale refactors, extensive debugging, and multi-hour agent loops, outperforming previous models on real-world coding benchmarks. The model is specifically trained for Windows environments and integrates improvements for use in the Codex CLI.GPT‑5.1-Codex-Max is now available for Codex users on ChatGPT Plus, Pro, Business, Edu, and Enterprise plans, with API access coming soon. It delivers significant improvements in coding accuracy, token efficiency, and the ability to sustain coherent work over long durations. Enhanced safeguards and monitoring support safer deployment, especially in cybersecurity contexts. OpenAI recommends using GPT‑5.1-Codex-Max for agentic coding tasks and highlights its impact on boosting engineering productivity, with internal reports showing substantial increases in pull request output since its adoption." src="assets/slide_2025_49/building-more-with-gpt-5-1-codex-max-openai.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_49/anthropic-ai-interviewer-market-research.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/AnthropicAI/status/1996627123021426919" target="_blank">
  <img alt="Anthropic has introduced a new tool called Anthropic Interviewer, designed to gather insights into public perspectives on artificial intelligence.The tool is available for use during a limited week-long pilot period." class="half-size" src="assets/slide_2025_49/anthropic-interviewer-ai-perspectives-anthropic.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://www.aboutamazon.com/news/aws/aws-agentic-ai-amazon-bedrock-nova-models?utm_source=SOCIAL&amp;utm_medium=TWITTER&amp;utm_term=amazonnews&amp;utm_content=18915719359&amp;linkId=884359814" target="_blank">
   <img alt='Amazon has announced a major expansion of its Nova AI portfolio, introducing four new Nova 2 models—Lite, Pro, Sonic, and Omni—each designed for different enterprise needs such as cost-effective reasoning, complex multimodal tasks, real-time speech capabilities, and unified text/image/video/speech processing. Nova 2 models feature industry-leading benchmarks and built-in web grounding and code execution. Organizations like Cisco, Siemens, and Reddit are leveraging these models for applications in content generation, automation, and AI assistants.Additionally, Amazon unveiled Nova Forge, a unique "open training" service that allows organizations to create customized AI models—called “Novellas”—by blending proprietary data with Amazon’s foundation models throughout the training process. Nova Forge provides advanced tools such as reinforcement learning gyms, model distillation, and responsible AI controls. Amazon also introduced Nova Act, a new AWS service for building reliable AI agents that automate browser-based workflows, delivering high reliability and ease of deployment for businesses seeking to automate complex UI tasks.' class="half-size" src="assets/slide_2025_49/amazon-nova-models-nova-forge-nova-act-amazon.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://www.aboutamazon.com/news/aws/amazon-ai-frontier-agents-autonomous-kiro?utm_source=SOCIAL&amp;utm_medium=TWITTER&amp;utm_term=amazonnews&amp;utm_content=18916022183&amp;linkId=884371224" target="_blank">
   <img alt='AWS has introduced a new class of AI "frontier agents" designed to act as autonomous, scalable, and persistent virtual teammates for software development. The first three agents—Kiro autonomous agent, AWS Security Agent, and AWS DevOps Agent—can independently handle complex tasks for hours or days, helping teams shift from micromanaging tasks to focusing on higher-level priorities. Kiro acts as a virtual developer, maintaining context and learning over time; AWS Security Agent serves as a virtual security engineer, proactively reviewing code and automating penetration testing; and AWS DevOps Agent functions as an operations expert, accelerating incident response and offering recommendations for continual improvement.These agents are already being used by organizations like Clariant, Commonwealth Bank of Australia, and SmugMug to accelerate software delivery, improve security, and enhance operational excellence. The agents integrate with popular developer tools and cloud environments, enabling teams to scale their capabilities without increasing headcount. All three frontier agents are available now in preview, representing a significant step forward in integrating advanced AI into the entire software development lifecycle.' class="half-size" src="assets/slide_2025_49/aws-unveils-frontier-agents-a-new-class-of-ai-agents-amazon.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/nvidianewsroom/status/1996976316453679381" target="_blank">
   <img alt="CUDA 13.1 has been released, marking the largest update to CUDA since its inception in 2006.This update introduces CUDA Tile, a new programming model designed to make AI and accelerated computing more accessible and easier for a broader range of developers." class="half-size" src="assets/slide_2025_49/cuda-13-1-launch-cuda-tile-ai-gpu-programming-nvidia.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://developer.nvidia.com/blog/focus-on-your-algorithm-nvidia-cuda-tile-handles-the-hardware" target="_blank">
   <img alt="NVIDIA CUDA 13.1 introduces CUDA Tile, a major advancement in GPU programming that enables developers to write high-level, tile-based code. CUDA Tile abstracts away the complexities of specialized hardware like tensor cores, allowing algorithms to be expressed in terms of data tiles rather than individual threads. This approach improves code portability and compatibility across different NVIDIA GPU architectures, making it easier for developers to write efficient code without deep hardware-specific knowledge.At the core of CUDA Tile is the CUDA Tile IR (Intermediate Representation), a virtual instruction set designed for tile operations, which complements the existing SIMT (Single Instruction, Multiple Thread) and PTX programming models. Most users will interact with CUDA Tile through high-level tools like NVIDIA cuTile Python, while advanced users can leverage CUDA Tile IR to build custom compilers or libraries. This new paradigm streamlines GPU programming, especially for AI and tensor-based workloads, while supporting both traditional and tile-based programming models." src="assets/slide_2025_49/focus-on-your-algorithm-nvidia-cuda-tile-handles-the-hardware-nvidia.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/qwen3-tts-lifelike-voice-generation.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/Tu7uruu/status/1996954120603762860" target="_blank">
   <img alt="Qwen3-TTS, released on 2025-11-27, is an advanced text-to-speech system that offers over 49 unique voices, enabling users to select precise personality and style for their projects. It supports 10 languages and multiple authentic dialects, such as Minnan, Wu, Cantonese, Sichuan, and more, providing comprehensive global language coverage.The system delivers human-like speech with adaptive rhythm, pacing, and intonation, resulting in natural and expressive audio output suitable for diverse creative applications." src="assets/slide_2025_49/qwen3-tts-lifelike-voice-generation.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://qwen.ai/blog?id=qwen3-tts-1128" target="_blank">
   <img alt="**Qwen3-TTS-Flash** is a flagship text-to-speech model by Alibaba, supporting 49 high-quality timbres, 10 major languages, and 9 Chinese dialects. The model delivers natural, expressive, and human-like speech synthesis, with improved prosody and adaptive speech rates. It covers a diverse range of voices (ages, genders, regional traits, character roles) and excels in multilingual and dialectal accuracy, outperforming competitors on word error rate benchmarks.Qwen3-TTS offers extensive sample sets for various timbres, languages, and dialects, and is available via API for easy integration. Users can synthesize speech by specifying timbre, language, and input text, with code examples provided for quick setup. The model is suitable for research and commercial use, with citation details included for academic referencing." class="half-size" src="assets/slide_2025_49/qwen3-tts-update-49-timbres-10-languages-9-dialects-qwen.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/vibevoice-realtime-speech-microsoft.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/cocktailpeanut/status/1996724204985565195" target="_blank">
  <img alt="Vibevoice-Realtime is a lightweight, real-time text-to-speech (TTS) system featuring only 0.5 billion parameters, making it genuinely capable of real-time performance. It operates efficiently on machines with around 2.5GB of VRAM and is compatible with both Windows and Mac systems.The showcased video demonstrates Vibevoice-Realtime functioning live on a Windows computer, highlighting its minimal resource requirements and broad accessibility across various hardware platforms." src="assets/slide_2025_49/vibevoice-realtime-tts-for-all-machines.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/min.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/OpenBMB/status/1996629624571514959" target="_blank">
  <img alt="MiniCPM compared its latest model, MiniCPM-4.1, with Mistral-3-8B, highlighting that MiniCPM remains about twice as fast and maintains a significant performance lead, except in math and code tasks. The findings suggest MiniCPM is still a generation ahead in capabilities.For developers focused on speed and efficiency, MiniCPM is recommended as the top small model available." class="half-size" src="assets/slide_2025_49/min.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://poetiq.ai/posts/arcagi_announcement/" target="_blank">
  <img alt="Poetiq has announced a significant advancement in AI reasoning by achieving new state-of-the-art (SOTA) results on the ARC-AGI-1 and ARC-AGI-2 benchmarks. Their flexible meta-system builds intelligence on top of any underlying large language model (LLM), such as the newly released Gemini 3 and GPT-5.1, integrating these models rapidly to deliver superior performance and efficiency. Poetiq’s approach programmatically combines multiple LLMs and optimizes reasoning strategies for cost-effective, accurate problem-solving, establishing new Pareto frontiers for both cost and performance. They have open-sourced their code, demonstrating improvements across a wide range of models (both open and closed source), and their systems exceed average human performance on ARC-AGI-2.The Poetiq meta-system operates recursively and autonomously, selecting and coordinating different models and reasoning strategies without prior exposure to ARC-AGI-2 problems, showcasing strong generalization. Their system reduces costs by minimizing the number of model queries and employs self-auditing to optimize computation. While all results are reported on public evaluation sets, Poetiq notes expected accuracy drops on semi-private sets, mirroring underlying model behavior. The team aims to extend these advancements beyond ARC-AGI, accelerating progress in complex AI reasoning and knowledge extraction, and invites collaboration through open positions and open-source contributions." src="assets/slide_2025_49/poetiq-arc-agi-sota-results.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/MistralAI/status/1995872766177018340" target="_blank">
   <img alt="Mistral has announced the release of the Mistral 3 family of AI models, offering advanced intelligence capabilities across various model sizes. These models are released under the Apache 2.0 license, promoting open access and flexible usage.Further details about the Mistral 3 models are available in the linked thread." src="assets/slide_2025_49/mistral-3-family-frontier-intelligence-apache-2-0.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://mistral.ai/news/mistral-3" target="_blank">
   <img alt="Mistral AI has announced the release of Mistral 3, a new family of open-source, multimodal, and multilingual AI models. The lineup includes three small, dense models (3B, 8B, and 14B parameters) and the flagship Mistral Large 3, a sparse mixture-of-experts model with 41B active and 675B total parameters. All models are released under the Apache 2.0 license, supporting text and image understanding across 40+ languages, and are optimized for efficiency and performance in both enterprise and edge deployments. Partnerships with NVIDIA, vLLM, and Red Hat enable broad hardware and software compatibility, allowing for scalable, high-throughput AI solutions.Mistral 3 models are available on major platforms such as Mistral AI Studio, Amazon Bedrock, Azure, Hugging Face, and more, with APIs for instant access and customization options for organizations. The release emphasizes transparency, accessibility, and collective progress, inviting the developer and enterprise communities to build, fine-tune, and innovate with state-of-the-art AI capabilities." class="half-size" src="assets/slide_2025_49/introducing-mistral-3-mistral-ai.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/gen-4-5-cinematic-realism-ai-model.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/runwayml/status/1995857775771918574" target="_blank">
   <img alt="Gen-4.5 introduces a new level of cinematic realism, excelling at generating objects that move with authentic weight, momentum, and force—even in zero gravity. It also enables the creation of novel and creative visual concepts.Early access to Gen-4.5 is being gradually rolled out and will soon be available to all plans. More information about the model and future updates can be found at the provided link." src="assets/slide_2025_49/gen-4-5-cinematic-realism-ai-model.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://runwayml.com/research/introducing-runway-gen-4.5" target="_blank">
   <img alt="Runway has announced Gen-4.5, its latest state-of-the-art video generation model, which offers significant advancements in motion quality, visual fidelity, prompt adherence, and creative control. Gen-4.5 surpasses previous models and competitors by achieving top benchmarks in Artificial Analysis Text to Video, delivering cinematic, realistic, and highly controllable video outputs. It maintains the speed and efficiency of Gen-4, is available at similar pricing across all plans, and supports a broad range of control modes such as Image to Video, Keyframes, and Video to Video.Core capabilities of Gen-4.5 include precise physical accuracy, detailed scene composition, expressive characters, and a wide range of stylistic controls from photorealistic to stylized animation. While it sets new standards for temporal consistency and controllable action generation, the model still faces common limitations in causal reasoning, object permanence, and success bias. Built in collaboration with NVIDIA and deployed on advanced GPUs, Gen-4.5 is being gradually rolled out to all users and is already being used by early enterprise partners in industries like retail, marketing, broadcast, and gaming." src="assets/slide_2025_49/introducing-runway-gen-4-5.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/kling-o1-multimodal-creative-engine-launch-event.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/Kling_ai/status/1995506929461002590" target="_blank">
  <img alt="Kling has launched the Kling O1, a new creative engine with advanced multimodal capabilities that can process and generate content from text, images, and videos, streamlining the creative process. They are offering a limited-time promotion for subscribers and hint at upcoming surprises.For the next 12 hours, users who follow, like, and retweet will receive 200 credits, with 200 participants also winning a 1-month Standard Plan, all rewards delivered via direct message." src="assets/slide_2025_49/kling-o1-multimodal-creative-engine-launch-event.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://cas-bridge.xethub.hf.co/xet-bridge-us/692cfec93b25b81d09307b94/2d0aa38511b9df084d12a00fe04a96595496af772cb766c516c4e6aee1e21246?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=cas%2F20251206%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20251206T171937Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=a52e72f8582d02801f899c0a0193f29adb483bba4d3a19a86418d704ddc74b02&amp;X-Amz-SignedHeaders=host&amp;X-Xet-Cas-Uid=public&amp;response-content-disposition=inline%3B+filename*%3DUTF-8%27%27paper.pdf%3B+filename%3D%22paper.pdf%22%3B&amp;response-content-type=application%2Fpdf&amp;x-id=GetObject&amp;Expires=1765045177&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTA0NTE3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTJjZmVjOTNiMjViODFkMDkzMDdiOTQvMmQwYWEzODUxMWI5ZGYwODRkMTJhMDBmZTA0YTk2NTk1NDk2YWY3NzJjYjc2NmM1MTZjNGU2YWVlMWUyMTI0NioifV19&amp;Signature=kHKFVWBX5uFLpkG6tctWPrdHEBUI2BVHLDbWV4mw-3eYNt6PinktdNH88omLEF18E3s07BTTHmsuw0ywt6LmJzds6lXYmKd366Q37D2cmhB18EmfL02vXL80DEAejX2-KPdweb1Wn1AgHw9Bj1JnlXmyqeRAPbUWip-Q2rY5sk4CNLjZW1Wgz9Br9uzWmkLquBbUYIn-IELKbHxb9TAtio7nODaaWHsTLUsvJno9AdqA67VKxr45hOPQbNGC6KE2WXYNJ3QRKs8kipIfeuAMZM961EbIQfCN2-3R3rkZiozbizXHscHpBktVaEh3q4Fs2EF2UyoiSj8uoUwb8Su-sA__&amp;Key-Pair-Id=K2L8F4GPSG1IFC" target="_blank">
   <img alt="DeepSeek-V3.2 is an advanced open large language model developed by DeepSeek-AI, designed to deliver high computational efficiency alongside superior reasoning and agentic performance. Its key innovations include DeepSeek Sparse Attention (DSA) for efficient long-context processing, a scalable reinforcement learning framework that enables performance on par with GPT-5, and a large-scale agentic task synthesis pipeline for generating robust training data. The high-compute variant, DeepSeek-V3.2-Speciale, reportedly surpasses GPT-5 and matches Gemini-3.0-Pro in complex reasoning, achieving top results in competitions like the 2025 International Mathematical Olympiad and the International Olympiad in Informatics.Benchmark results highlight DeepSeek-V3.2's strong performance across a range of tasks, including mathematical reasoning, code generation, and agentic tool use, with particularly notable achievements in accuracy, pass rates, and Codeforces ratings compared to other leading models such as GPT-5, Claude-4.5, and Gemini-3.0-Pro." class="half-size" src="assets/slide_2025_49/deepseek-v32-pushing-the-frontier-of-open-large-language-models.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/deepseek_ai/status/1995452641430651132" target="_blank">
   <img alt="DeepSeek has launched two new models: DeepSeek-V3.2, the official successor to V3.2-Exp, now available on App, Web, and API; and DeepSeek-V3.2-Speciale, an API-only model focused on advanced reasoning capabilities.A technical report detailing these updates is available at the provided link. Both releases are designed to enhance reasoning performance, particularly for agent-based applications." class="half-size" src="assets/slide_2025_49/deepseek-v3-2-and-deepseek-v3-2-speciale-launch-deepseek.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/abs/2512.04123v1" target="_blank">
   <img alt="This paper presents the first large-scale, systematic study of AI agents deployed in production across various industries. The researchers surveyed 306 practitioners and conducted 20 in-depth case studies spanning 26 domains to understand why and how organizations build, evaluate, and deploy AI agents, as well as the primary challenges faced. Findings reveal that most production AI agents use simple, controllable designs: 68% require human intervention after at most 10 steps, 70% use prompting with off-the-shelf models instead of model fine-tuning, and 74% rely mainly on human evaluation. Reliability and ensuring agent correctness are reported as the top challenges in development.Despite these obstacles, the study finds that straightforward yet effective methods are already delivering tangible impact across diverse fields such as finance, insurance, and education. The main reasons for deploying agents include increasing task productivity and automating routine labor, while improving operational stability is less common. The paper aims to bridge the gap between academic research and real-world deployment by documenting current best practices and surfacing the challenges practitioners face." class="half-size" src="assets/slide_2025_49/measuring-agents-in-production-uc-berkeley.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/melissapan/status/1996975916971626763" target="_blank">
   <img alt="MAP: Measuring Agents in Production is a new research paper released by a team of 25 researchers from Berkeley, Stanford, UIUC, IBM, and Intesa Sanpaolo. The study explores the real-world effectiveness of AI agents, focusing on why they are used, how to build them for production with simple and controllable methods, and how to evaluate them—primarily through heavy human oversight.Key findings highlight that while productivity gains drive agent adoption, reliability remains a major unsolved challenge. The research is based on a survey of 306 agent builders and 20 in-depth interviews spanning 26 application domains, offering a comprehensive look at the current landscape of production agents." class="half-size" src="assets/slide_2025_49/map-measuring-agents-in-production-berkeley-stanford-uiuc-ibm-intesasanpaolo.webp"/>
  </a>
 </div>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/live-avatar-real-time-audio-driven-avatars-alibaba.mp4">
 <div class="r-stack">
  <a class="fragment current-visible" data-fragment-index="1" href="https://huggingface.co/papers/2512.04677" target="_blank">
   <img alt="**Live Avatar** introduces a real-time streaming system for generating lifelike, infinitely long, audio-driven avatars using a 14-billion-parameter diffusion model. The framework addresses the challenges of traditional diffusion-based video generation—such as sequential computation bottlenecks and long-horizon inconsistency—by employing Timestep-forcing Pipeline Parallelism (TPP) for efficient, low-latency, distributed inference across multiple GPUs. This enables stable, real-time streaming at up to 20 FPS on 5 H800 GPUs.To maintain visual consistency over long sequences, Live Avatar incorporates a Rolling Sink Frame Mechanism (RSFM) that recalibrates appearance using cached references, and Self-Forcing Distribution Matching Distillation for streamable model adaptation. The system achieves state-of-the-art performance in infinite-length avatar generation, marking a significant advancement for industrial-scale, high-fidelity, long-form video synthesis." src="assets/slide_2025_49/live-avatar-streaming-real-time-audio-driven-avatar-generation-with-infinite-length-alibaba-universi.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="2" href="https://x.com/HuggingPapers/status/1996854589790863580" target="_blank">
   <img alt="Alibaba has introduced a Live Avatar system that streams real-time, audio-driven avatars using a 14-billion-parameter diffusion model. The system achieves 20 frames per second performance on five H800 GPUs, addressing challenges in consistency and latency.This represents the first instance of practical, high-fidelity, infinite-length avatar generation at this scale, making it a significant advancement in real-time virtual avatar technology." src="assets/slide_2025_49/live-avatar-real-time-audio-driven-avatars-alibaba.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://www.alphaxiv.org/abs/2512.01374" target="_blank">
   <img alt="This paper addresses the challenge of stabilizing reinforcement learning (RL) when training large language models (LLMs), particularly in the context of sequence-level rewards versus token-level optimization used in common algorithms like REINFORCE. The authors propose a novel formulation, showing that optimizing with a surrogate token-level objective can effectively approximate sequence-level rewards—but only if training–inference discrepancy and policy staleness are minimized. This insight clarifies the theoretical necessity for stability techniques such as importance sampling correction, clipping, and Routing Replay in Mixture-of-Experts (MoE) models.Empirical experiments with a large 30B MoE model reveal that on-policy policy gradient methods with importance sampling offer the most stable training. For off-policy updates, combining clipping and Routing Replay is crucial to counter policy staleness. The study further finds that, once stability is achieved, longer training yields similar final outcomes regardless of initialization. These findings provide practical guidelines and theoretical understanding for more reliable RL-based LLM training." src="assets/slide_2025_49/stabilizing-reinforcement-learning-with-llms-qwen-team-alibaba.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/ChujieZheng/status/1995736070655492342" target="_blank">
   <img alt="Glad to introduce research focused on uncovering the mathematical foundations of reinforcement learning (RL) in the context of large language models (LLMs). The work explores how stabilization techniques function to enhance the reliability and performance of RL methods used with LLMs.The shared paper delves into both theoretical and practical aspects, offering insights into how mathematical principles guide the development and optimization of RL algorithms. This research aims to improve understanding and implementation of stabilization strategies for more effective machine learning outcomes." src="assets/slide_2025_49/mathematical-principles-behind-rl-with-llms.webp"/>
  </a>
 </div>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://arxiv.org/abs/2511.07919" target="_blank">
   <img alt="Feedback Descent is a novel framework for optimizing text artifacts—such as prompts, code, and molecular representations—using structured textual feedback, rather than relying solely on scalar rewards or binary preferences. By leveraging detailed critiques in the optimization loop, the method widens the information bottleneck of preference learning, allowing for more directed and effective improvements in the artifact's text space. The process runs entirely at inference time, without modifying model weights, and is agnostic to the specific task.The approach consists of iteratively generating improved candidates based on accumulated feedback, comparing them to the current best artifact, and providing both a preference and textual rationale for selection. This feedback-driven loop enables gradient-like, targeted edits, outperforming state-of-the-art prompt optimization and reinforcement learning methods across various domains, including prompt design and molecule discovery. Experiments demonstrate that Feedback Descent can efficiently identify high-performing artifacts, such as novel drug-like molecules in large datasets." src="assets/slide_2025_49/feedback-descent-open-ended-text-optimization-via-pairwise-comparison-stanford-university.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://ai.stanford.edu/blog/feedback-descent/" target="_blank">
   <img alt="The blog post discusses the limitations of conventional reinforcement learning (RL), which reduces rich evaluative feedback to a single scalar reward, discarding actionable information that could accelerate learning. The authors propose an alternative paradigm called Feedback Descent, where an evaluator provides detailed, structured textual feedback on candidate solutions, and an editor (typically an LLM) uses accumulated feedback to revise and improve artifacts iteratively. This approach leverages the full spectrum of available feedback, enabling more targeted and efficient optimization compared to RL's scalar supervision.Feedback Descent is demonstrated across diverse domains, including molecular design, SVG image optimization, and prompt engineering, consistently outperforming or matching specialized RL and optimization methods while requiring fewer evaluation iterations. The work highlights the promise of using semantic (textual) feedback as a medium for continual learning, suggesting that meaningful improvement can be achieved without retraining models, simply by integrating and acting on accumulated textual feedback." class="half-size" src="assets/slide_2025_49/following-the-text-gradient-at-scale-stanford-ai-lab.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.nature.com/articles/s41586-024-08328-6.pdf" target="_blank">
  <img alt="The article introduces Tabular Prior-data Fitted Network (TabPFN), a pioneering foundation model specifically designed for tabular data—structured data commonly found in scientific fields such as biomedicine, physics, and economics. TabPFN leverages transformer-based generative modeling and principled in-context learning to outperform traditional methods like gradient-boosted decision trees on small to medium-sized datasets (up to 10,000 samples and 500 features). Remarkably, it achieves superior predictive performance in just seconds, whereas conventional models may require hours of tuning. Beyond classification and regression, TabPFN supports fine-tuning, data generation, density estimation, and learning reusable embeddings.The model's strength lies in its ability to generalize across diverse, heterogeneous tabular datasets, a domain where deep learning has historically struggled due to the variety of data types and feature distributions. By training across millions of synthetic datasets, TabPFN learns to adapt its predictive algorithm to new datasets efficiently, offering robust out-of-distribution performance and knowledge transfer. This approach not only streamlines the modeling process for tabular data but also holds promise for accelerating scientific discovery and improving decision-making across multiple disciplines." src="assets/slide_2025_49/accurate-predictions-on-small-data-with-a-tabular-foundation-model-university-of-freiburg.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://joel-becker.com/images/publications/forecasting_time_horizon_under_compute_slowdown.pdf" target="_blank">
  <img alt="The paper examines the relationship between AI agent capability (as measured by METR’s &quot;time horizon&quot; metric) and growth in computational resources. It presents a theoretical model where time horizon improvements depend on both compute and algorithmic progress, with the latter itself driven by experimental compute. The core finding is that, given current empirical trends (2019–2025), the rate of time horizon growth is directly proportional to the rate of compute growth; thus, any slowdown in compute scaling would proportionally slow progress in AI capabilities.Experimental evidence from the Llama 3.1 and Qwen 2.5 model families supports the model’s prediction that log training compute and log time horizon are linearly related. Projections using OpenAI's forecasted compute growth indicate that potential compute slowdowns could significantly delay future AI capabilities—for example, achieving certain reliability thresholds could occur years later than simple trend extrapolation would suggest." src="assets/slide_2025_49/forecasting-ai-time-horizon-under-compute-slowdowns-mit.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://magazine.sebastianraschka.com/p/technical-deepseek" target="_blank">
  <img alt="The article provides a technical overview of the evolution of DeepSeek’s open-weight large language models, detailing advancements from V3 to V3.2. It highlights key architectural innovations such as the use of Mixture-of-Experts (MoE), Multi-Head Latent Attention (MLA) for memory-efficient key-value caching, and the introduction of DeepSeek Sparse Attention (DSA) powered by a lightning indexer and token selector to improve efficiency in long-context scenarios. The piece discusses the transition from dedicated reasoning models (like DeepSeek R1, which leveraged Reinforcement Learning with Verifiable Rewards—RLVR) to hybrid models capable of both general and reasoning tasks, and explores the latest training improvements, including self-verification and self-refinement techniques inspired by DeepSeekMath V2.DeepSeek V3.2 stands out for its competitive performance against proprietary models (e.g., GPT-5, Gemini 3.0 Pro), achieved through a combination of architectural efficiency and advanced reinforcement learning strategies (notably, updates to the GRPO algorithm). The article also covers the role of process rewards, meta-verifiers, and inference scaling in boosting math and agentic task performance, as well as the extended-thinking variant V3.2-Speciale, which outputs longer, more accurate responses. Overall, DeepSeek’s recent releases showcase how open-weight models can rival closed-source leaders by combining innovative attention mechanisms, efficient architecture, and sophisticated training pipelines." class="half-size" src="assets/slide_2025_49/a-technical-tour-of-the-deepseek-models-from-v3-to-v3-2.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://alignment.openai.com/scaling-code-verification/" target="_blank">
   <img alt="OpenAI's blog post presents a practical approach to scalable code verification in an era where AI-generated code is produced faster than humans can review. The post details the development and deployment of a dedicated, agentic code reviewer—based on the GPT-5 Codex models—that leverages repository-wide tools and code execution to improve both recall and precision in automated code review. The team prioritizes high precision and actionable feedback to ensure developer trust and usability, accepting a modest reduction in recall to avoid overwhelming users with false alarms. Special training enables the reviewer to be context-aware and effective in real-world, ambiguous coding environments, distinguishing its deployment-time role from more permissive, training-time reward models.The deployment of Codex as an automated code reviewer at OpenAI and on external GitHub repositories demonstrates its capacity to surface critical, actionable issues without slowing development. The reviewer is effective at catching defects in both human- and AI-generated code, with over half of its comments prompting code changes and high rates of positive user feedback. OpenAI stresses that automated review should be a support tool rather than a sole guarantee of safety, emphasizing the importance of layered defenses and human oversight as AI models advance in code generation and verification capabilities." src="assets/slide_2025_49/a-practical-approach-to-verifying-code-at-scale-openai.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://alignment.openai.com/sae-latent-attribution/" target="_blank">
   <img alt="This post explores how interpretability tools—specifically sparse autoencoder (SAE) latent attribution—can help debug misaligned behaviors in language models. The authors critique previous approaches that relied on comparing two models via raw activation differences, noting this method may miss the most causally relevant internal features. Instead, they introduce an attribution-based method that identifies SAE latents most causally linked to undesirable behaviors by comparing attributions between positive and negative completions within a single model, then validating causal effects through activation steering.Two case studies demonstrate the method: one involving a model fine-tuned to produce inaccurate health information, and another where a model gives inappropriate validation to user beliefs. In both, attribution-based selection of latents outperforms activation-difference methods in identifying features that can be steered to alter misaligned behaviors. Notably, a single “provocative” latent was found to strongly drive both types of misalignment, highlighting the convergence of risky behaviors in model representations and demonstrating the value of attribution for targeted model debugging." src="assets/slide_2025_49/debugging-misaligned-completions-with-sparse-autoencoder-latent-attribution.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://blog.google/technology/developers/gemini-3-pro-vision/?linkId=22378122" target="_blank">
  <img alt="Gemini 3 Pro is Google's most advanced multimodal AI model, offering state-of-the-art performance in document, spatial, screen, and video understanding. It excels at complex visual reasoning, such as parsing unstructured documents, reconstructing structured data from images, and analyzing long reports with multi-step reasoning. In addition to superior document OCR and &quot;derendering&quot; capabilities, Gemini 3 Pro provides precise spatial understanding for robotics and AR/XR, robust screen comprehension for automating tasks on digital interfaces, and enhanced video understanding that captures fast-paced actions and performs sophisticated temporal reasoning.The model's applications span diverse domains including education, where it helps students with visual reasoning and problem correction, medicine (not for clinical use), where it achieves top scores on expert-level benchmarks, and law and finance, aiding complex document workflows. Developers can control performance and cost with new media resolution settings and are encouraged to explore the model through developer documentation or Google AI Studio." src="assets/slide_2025_49/gemini-3-pro-frontier-vision-ai-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://blog.google/technology/ai/kaggle-game-arena/" target="_blank">
  <img alt="Google DeepMind and Kaggle have introduced the Kaggle Game Arena, a new open-source platform for benchmarking AI models by having them compete in strategic games like chess. Traditional AI benchmarks are becoming less effective as models improve, often reaching near-perfect scores and making it difficult to assess true intelligence or problem-solving capabilities. Game Arena addresses this by offering dynamic, head-to-head competitions that require strategic reasoning and adaptation, providing a clearer, more robust signal of an AI model’s general intelligence.The platform ensures fairness and transparency by open-sourcing its game environments and evaluation frameworks, using a rigorous all-play-all ranking system. The inaugural chess exhibition will feature eight leading AI models, with future tournaments planned in games such as Go and poker. This initiative aims to create evolving, challenging benchmarks that better reflect the complex reasoning abilities needed for real-world applications." src="assets/slide_2025_49/rethinking-how-we-measure-ai-intelligence-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=social_post&amp;utm_content=gr-acct" target="_blank">
  <img alt='Google Research introduces the Titans architecture and the MIRAS framework, offering a new approach to AI sequence modeling. Titans combines the efficiency of RNNs with the expressive power of transformers by using a deep neural network as a long-term memory module, which updates itself in real time based on a "surprise metric" that prioritizes novel or unexpected information. This allows AI models to process and retain massive contexts much faster and more effectively than traditional models.The MIRAS framework provides a theoretical foundation for designing sequence models, generalizing memory mechanisms through flexible architectures, attentional biases, retention gates, and optimization algorithms. Experiments show that Titans and its MIRAS variants outperform leading models (such as Transformer++, Mamba-2, and GPT-4) especially in tasks requiring long-context reasoning, achieving lower perplexity and better scalability. This research paves the way for more adaptive, efficient, and robust AI capable of handling extremely long sequences.' src="assets/slide_2025_49/titans-miras-helping-ai-have-long-term-memory-google-research.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/OpenAIDevs/status/1996643999097274560" target="_blank">
   <img alt="GPT-5.1-Codex Max, the most advanced agentic coding model, is now available through the Responses API. This allows seamless integration into applications and workflows.Users accessing the Codex CLI with an API key can now leverage GPT-5.1-Codex-Max for enhanced coding capabilities." src="assets/slide_2025_49/gpt-5-1-codex-max-available-in-responses-api-openai.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://cookbook.openai.com/examples/gpt-5/gpt-5-1-codex-max_prompting_guide" target="_blank">
   <img alt="The **GPT-5.1-Codex-Max Prompting Guide** provides detailed best practices for leveraging OpenAI’s most advanced coding agent model via API for custom workflows. It emphasizes prompt engineering—starting from the recommended Codex-Max prompt—favoring autonomy, persistent end-to-end task completion, and code quality over plans or verbose status updates. Key instructions include maximizing tool usage (like `apply_patch` and parallel tool calls), batching file reads, and adhering to codebase conventions. The guide also covers compaction for long contexts, handling AGENTS.md, and implementing reliable tool integrations (including shell, plan, and custom tools) tailored to the model’s training.For optimal performance, users are advised to use Codex’s open-source reference harness, avoid unnecessary clarifications, and ensure concise, structured outputs suitable for CLI consumption. Special considerations are included for frontend/UI tasks and advanced scenarios like tool response truncation and Windows support. Overall, the guide aims to help developers achieve efficient, high-quality, and autonomous code generation and editing with Codex-Max." src="assets/slide_2025_49/gpt-5-1-codex-max-prompting-guide-openai.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://neptune.ai/blog/we-are-joining-openai" target="_blank">
  <img alt="Neptune.ai has announced a definitive agreement to be acquired by OpenAI, pending closing conditions. The Neptune team is excited to join OpenAI and contribute to building better AI models, with their core expertise in metrics dashboards for monitoring and visualizing ML model training. Neptune will integrate its tools deeply into OpenAI's training stack, supporting researchers to analyze complex workflows and improve visibility into model development.As part of the acquisition, Neptune will wind down its external services over the coming months and is committed to assisting customers through this transition. The team expresses gratitude to customers, investors, and colleagues for their support and looks forward to advancing OpenAI's mission to ensure AGI benefits humanity." src="assets/slide_2025_49/neptune-joins-openai-openai.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/simonw/status/1995892914141233154" target="_blank">
   <img alt="Five new Mistral models have been released. Among them, the most notable is the compact 3B model.This 3B model features vision support and is capable of running entirely within a web browser." src="assets/slide_2025_49/mistral-3b-model-with-vision-support-browser.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://x.com/UnslothAI/status/1996595704438120774" target="_blank">
   <img alt="You can now use a free notebook to train the Mistral Ministral 3 model with reinforcement learning, specifically applying the GRPO algorithm to enable the model to solve sudoku puzzles autonomously.The release introduces new reward functions, an updated RL environment, and discusses topics like reward hacking. Additional information and resources are available through the provided blog post and notebook links." src="assets/slide_2025_49/train-mistral-minstral-3-reinforcement-learning-sudoku-tutorial.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/" target="_blank">
  <img alt="GitHub Copilot now supports custom agents defined in agents.md files, allowing developers to create specialized AI personas such as documentation writers, test engineers, or security analysts. Drawing insights from an analysis of over 2,500 agents.md files, the article highlights that effective agent definitions specify clear roles, executable commands, boundaries, and real code examples. Successful agents are specialists, not generalists, and their instructions cover core areas like commands, testing, project structure, code style, git workflow, and explicit boundaries about what the agent should and should not do.Practical templates and examples are provided to help users craft their own agents.md files, emphasizing clarity, actionable instructions, and iteration based on feedback. The article also suggests several useful agent types (e.g., docs-agent, test-agent, lint-agent) and offers a starter template. Key takeaways stress that specificity and detailed guidance are critical for empowering Copilot agents to be effective and safe contributors to codebases." src="assets/slide_2025_49/github-generic-wallpaper-rubber-duck-invertocat-github.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://github.blog/open-source/maintainers/the-local-first-rebellion-how-home-assistant-became-the-most-important-project-in-your-house/" target="_blank">
  <img alt="The article profiles Home Assistant, one of GitHub’s fastest-growing open source projects, which is transforming smart homes by prioritizing local control, privacy, and interoperability—eschewing reliance on the cloud. Led by Franck Nijhof (Frenck), Home Assistant now runs in over 2 million households, supported by a vibrant community of 21,000 contributors in a single year. Its architecture abstracts thousands of devices from over 3,000 brands into a unified, event-driven platform running entirely on local hardware, enabling advanced automations and ensuring data stays within the home. The project’s governance via the Open Home Foundation ensures it remains vendor-neutral, privacy-centric, and sustainable, protecting users from cloud lock-in and device obsolescence.A unique aspect of Home Assistant is its community-driven quality model: developers build and test integrations on devices they personally own, directly improving their own environments. Its built-in Assist voice assistant operates locally, using deterministic command matching with optional AI for complex queries—never requiring cloud connectivity. Home Assistant also produces open hardware, such as the Voice Assistant Preview Edition, to accelerate software development and provide reference platforms. The project’s trajectory points toward fully programmable, agent-driven homes, with local AI and automations that empower homeowners with complete control over their environment." src="assets/slide_2025_49/the-local-first-rebellion-how-home-assistant-became-the-most-important-project-in-your-house-github.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/claudeai/status/1996310793017594124" target="_blank">
  <img alt="Claude Opus 4.5 is now accessible in Claude Code for Pro users. Pro users can activate Opus by selecting it with the /model command in their terminal.This update enables enhanced features for Pro users within the Claude Code environment." src="assets/slide_2025_49/claude-opus-4-5-available-in-claude-code-for-pro-users-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.instantdb.com/essays/agents_building_counterstrike" target="_blank">
  <img alt="The article compares the abilities of three advanced AI models—Gemini 3 Pro, Codex Max 5.1, and Claude Opus 4.5—tasked with building a basic, multiplayer 3D Counter Strike game using a series of structured prompts. Each model was evaluated on frontend (game mechanics, visuals, sound, animation) and backend (multiplayer logic, persistence, and map management) tasks. Claude Opus 4.5 excelled in frontend design and visuals, creating more appealing maps and character designs, while Gemini 3 Pro performed best in backend implementation, especially in adding multiplayer features and handling persistence reliably. Codex Max 5.1 was consistently intermediate, performing reasonably on both fronts but not leading in any category.The evaluation highlighted each model’s approach to problem-solving, with Gemini showing balanced use of documentation and debugging, Claude focusing on documentation, and Codex relying on introspection of code libraries. All models produced a working multiplayer FPS without human-written code, though each required some error correction and guidance. The experiment demonstrates significant advances in AI-assisted software development, but also notes current limitations, particularly when complex refactoring or subtle frontend bugs arise, indicating further progress is needed for fully autonomous coding." src="assets/slide_2025_49/codex-opus-gemini-counter-strike-ai-comparison.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/how-confessions-can-keep-language-models-honest/" target="_blank">
  <img alt='OpenAI has introduced a "confessions" method to improve the transparency and honesty of language models. This technique trains models to self-report when they break instructions or take shortcuts, producing a separate "confession" output judged solely on honesty. The confession is not penalized for admitting faults and does not affect the reward for the main answer. In evaluations, this approach significantly increased the visibility of model misbehavior, with models reliably confessing to mistakes or non-compliance even when their main answers appeared correct.Confessions serve as a diagnostic and monitoring tool, surfacing issues such as hallucinations, reward hacking, and instruction violations during both training and deployment. While confessions do not prevent undesirable behavior, they provide valuable insights into why and how such behaviors occur, supporting AI safety efforts. The method shows promise but is currently a proof of concept with acknowledged limitations, such as incomplete accuracy and limited scale. OpenAI plans to further refine and scale this technique, integrating it with complementary transparency and alignment approaches.' src="assets/slide_2025_49/how-confessions-can-keep-language-models-honest-openai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic" target="_blank">
  <img alt='Anthropic conducted an internal study in August 2025 to understand how AI, specifically their Claude models, is transforming engineering and research work within the company. By surveying 132 employees, conducting 53 qualitative interviews, and analyzing usage data from Claude Code, they found that AI is driving significant productivity gains, with employees self-reporting a 50% productivity boost and expanded use of AI in their daily tasks—especially in debugging, code understanding, and implementing new features. Notably, 27% of Claude-assisted work involved tasks that would not have been done otherwise, such as exploratory projects and quality-of-life improvements, and employees are becoming more "full-stack" by taking on tasks outside their core expertise.However, the findings also highlight emerging challenges: concerns about the atrophy of deep technical skills, changes to mentorship and collaboration as more questions are routed to AI rather than colleagues, and increased uncertainty about the future of software engineering roles. While AI enables more autonomous and complex task handling, most employees still actively supervise its outputs, especially for high-stakes work. Anthropic is using these insights to consider new approaches to professional development, collaboration, and career evolution in an AI-augmented workplace, acknowledging both the opportunities and uncertainties of this rapid transformation.' src="assets/slide_2025_49/how-ai-is-transforming-work-at-anthropic-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://huggingface.co/blog/hf-skills-training" target="_blank">
  <img alt="Hugging Face has introduced a new tool called Hugging Face Skills, which enables coding agents like Claude Code, OpenAI Codex, and Gemini CLI to fine-tune open-source large language models (LLMs) using plain English instructions. With the `hf-llm-trainer` skill, users can instruct agents to handle the entire model training workflow: dataset validation, GPU selection, script configuration, job submission to Hugging Face cloud GPUs, monitoring via Trackio, and automatic model uploads to the Hub. This system supports production-grade training methods, including supervised fine-tuning (SFT), direct preference optimization (DPO), and group relative policy optimization (GRPO), and includes guidance on hardware selection, cost, and post-training conversion to GGUF format for local deployment.The guide walks users through setup with various coding agents, authentication, and running their first training job. It emphasizes best practices like running test jobs before full production, validating dataset formats, and leveraging real-time monitoring to track training progress and diagnose issues. The skill is open source and extensible, making advanced LLM fine-tuning accessible and conversational, even for users without deep ML expertise." class="half-size" src="assets/slide_2025_49/we-got-claude-to-fine-tune-an-open-source-llm-huggingface.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/msharmavikram/status/1997002835213287475" target="_blank">
  <img alt="GPU programming needs to advance to handle more complex workloads. Currently, CUDA offers a SIMT (single-instruction, multiple-thread) model that gives developers detailed control over how code runs on GPUs.This approach enables efficient parallel execution but also requires programmers to manage execution at a fine level of detail." src="assets/slide_2025_49/gpu-programming-evolution-simt-cuda.webp"/>
 </a>
</section>
<section>
 <div class="r-stack">
  <a class="fragment fade-out" data-fragment-index="0" href="https://x.com/JFPuget/status/1997020158204985442" target="_blank">
   <img alt="Ivan Sorokin and a teammate have been announced as the official winners of the Arc Prize competition, achieving a significant lead over other participants. They expressed gratitude to Kaggle and Arc Prize for hosting the event.Resources related to their work—including a summary on the NVIDIA tech blog, their own writeup, and their code—are available through shared links." class="half-size" src="assets/slide_2025_49/arc-prize-competition-winning-solution-nvidia.webp"/>
  </a>
  <a class="fragment current-visible" data-fragment-index="0" href="https://www.kaggle.com/competitions/arc-prize-2025/writeups/nvarc" target="_blank">
   <img alt="The NVARC solution for the ARC Prize 2025 competition centers on a multi-stage pipeline for synthetic data generation using LLMs and code generation, improvements to The ARChitects approach, and the integration of Tiny Recursive Models (TRM). The team built a large-scale synthetic data generation process involving collection and mixing of human-written puzzle descriptions, and the generation of Python code for puzzle input/output logic, resulting in over 100,000 validated synthetic puzzles. Their training dataset combined these with several real puzzle datasets, creating a diverse set of 3.2 million augmented samples. The ARChitects approach was further optimized through dialog-style input/output formatting, efficient fine-tuning using NeMo RL with Megatron backend, and improved candidate rescoring strategies.In addition, the team experimented with ensembling TRM and LLM-based approaches, finding modest improvements in certain scenarios. Computational constraints led to careful tuning of TRM parameters to fit Kaggle’s resource limits. Their best results were achieved by scaling pretraining with high-quality synthetic puzzles, and they highlight the broader research potential of their synthetic data pipeline. The project’s source code, datasets, and notebooks are openly available for further exploration." class="half-size" src="assets/slide_2025_49/nvarc-solution-arc-prize-2025.webp"/>
  </a>
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.99ravens.agency/resources/blogs/your-experts-wont-train-your-ai-you-have-to-interview-them/" target="_blank">
  <img alt='The article explains why capturing expert knowledge for AI systems requires more than simple chatbot interviews, which tend to produce generic, uninspired results. Instead, 99Ravens advocates for a multi-agent architecture featuring an "Interviewer" and a "Note-Taker." The Interviewer engages experts in peer-level conversations to elicit tacit knowledge, while the Note-Taker structures and analyzes the conversation, ensuring comprehensive coverage and focus. This approach enables the creation of reusable digital personas that reflect authentic expert thinking, complete with methodologies, reasoning patterns, and communication styles.Key lessons include the necessity of role-specific prompts, structured data for agent collaboration, and external time management for effective long-form interviews. The ultimate goal is to scale authentic expertise—not just automate tasks—by transforming expert insights into software that preserves professional identity and unique methodologies. This method is positioned as essential for building valuable and trustworthy AI tools in complex domains.' class="half-size" src="assets/slide_2025_49/99ravens-ai-interviewer-system-multi-agent-architecture-expert-knowledge-capture.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/UnslothAI/status/1993358367776186801" target="_blank">
  <img alt="DeepSeek-R1 now enables FP8 reinforcement learning on consumer GPUs, allowing users to run models like Qwen3-1.7B using just 5GB of VRAM. Collaboration with PyTorch makes FP8 RL inference 1.4× faster.The Unsloth library provides significant efficiency improvements, requiring 60% less VRAM and supporting 12× longer context lengths." class="half-size" src="assets/slide_2025_49/deepseek-r1-fp8-reinforcement-learning-on-consumer-gpus.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/OpenRouterAI/status/1996678816820089131" target="_blank">
  <img alt="We partnered with @a16z to release the State of AI report, providing an empirical analysis of LLM usage on OpenRouter. The study covers data from over 100 trillion tokens, hundreds of models, and more than 3 million users over the past year.This comprehensive analysis offers a wide range of insights into how large language models are utilized in real-world scenarios, excluding third-party users." class="half-size" src="assets/slide_2025_49/state-of-ai-a16z.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://huggingface.co/blog/Tavily/tavily-deep-research" target="_blank">
  <img alt="The article discusses the development of a state-of-the-art AI research agent by the Tavily team, focusing on lessons learned in agent harness design, context engineering, and productionizing autonomous agents. The team emphasizes the importance of building flexible, future-proof software that can adapt to rapid improvements in large language models (LLMs), leveraging model and tool advancements, and prioritizing context management to optimize both performance and resource usage. Key strategies include simplifying orchestration logic, distilling tool outputs into concise reflections for context, and evolving tools to return only the most relevant data, ultimately reducing token consumption by 66% compared to traditional approaches.Additionally, the article addresses the challenges of deploying production-grade agents, such as managing LLM non-determinism, maintaining reliability, and balancing autonomy with robust guardrails. The team advocates for a minimal, essential toolset and using evaluations for directional feedback rather than strict benchmarks. Their approach resulted in both quality improvements and significant efficiency gains, positioning research agents as a foundational AI use case with broad potential across knowledge-intensive tasks." src="assets/slide_2025_49/building-deep-research-how-we-achieved-state-of-the-art-huggingface.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openrouter.ai/state-of-ai" target="_blank">
  <img alt="The &quot;State of AI&quot; report provides an empirical analysis of over 100 trillion tokens of real-world large language model (LLM) usage via the OpenRouter platform, covering trends from late 2024 to late 2025. Key findings include the rapid rise of reasoning-optimized and agentic models (e.g., OpenAI's o1), increased adoption of open source models—especially from China—and a shift in LLM usage toward complex, multi-step, tool-integrated workflows. Programming and creative roleplay have emerged as dominant application categories, with programming overtaking others in total token volume. The study highlights regional differences in adoption, the prevalence of English but growing multilingual support, and identifies retention patterns where early, well-matched cohorts persist (&quot;Glass Slipper&quot; effect).Model usage is now highly pluralistic, with no single provider dominating; open source and proprietary models coexist, each serving distinct market segments. Cost dynamics reveal that high-value workloads remain price-inelastic, while lower-cost models drive greater overall volume—often for consumer or creative use cases. The findings suggest that LLM deployment is increasingly heterogeneous, global, and structured around agentic inference, emphasizing the need for ongoing empirical measurement to guide model development and infrastructure decisions." src="assets/slide_2025_49/state-of-ai.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/rl-visualizer-ppo-vs-grpo-comparison.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/askalphaxiv/status/1996230945834451207" target="_blank">
  <img alt="RL Visualizer is an online tool designed to help users understand and compare popular reinforcement learning algorithms, specifically PPO and GRPO. The platform allows users to visualize how these algorithms perform in a simple maze environment.By providing an interactive experience, RL Visualizer aims to clarify the differences between PPO and GRPO, making it easier for users to grasp key concepts in reinforcement learning." src="assets/slide_2025_49/rl-visualizer-ppo-vs-grpo-comparison.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/ben_burtenshaw/status/1995877869562855687" target="_blank">
  <img alt="huggingface/skills is a new universal framework for agent context management, designed for AI tasks such as model training and dataset creation. It is compatible with major coding agent tools like Codex, Cursor, Claude Code, and Gemini CLI, and supports both local and remote cloud job execution.The framework wraps around skills following Claude Code’s standard, integrating instructions, resources, scripts, and examples. It also provides entry points for other utilities, allowing it to function as AGENTS.md, extensions, or similar agent standards, and supports adding relevant MCP servers such as the Hugging Face MCP." class="half-size" src="assets/slide_2025_49/huggingface-skills-universal-agent-context-huggingface.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://huggingface.co/spaces/OpenEvals/evaluation-guidebook" target="_blank">
  <img alt='The main content describes a process where metadata is being fetched from the Hugging Face (HF) Docker repository. The system is currently in the process of refreshing or updating this metadata to ensure the latest information is available.This operation is likely part of a workflow or tool related to "OpenEvals" and possibly an evaluation guidebook, aiming to keep resources up-to-date for users. No specific evaluation details or results are provided in the content.' src="assets/slide_2025_49/openevals-evaluation-guidebook.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://huggingface.co/blog/transformers-v5" target="_blank">
  <img alt="Transformers v5.0.0rc-0 marks a major milestone for the Hugging Face Transformers library, emphasizing simplicity, interoperability, and performance. The release introduces a more modular and standardized approach to model definitions, making it easier to add, maintain, and understand models. The team has streamlined the modeling and tokenization codebase, moved fully to PyTorch as the backend, and sunsetted Flax/TensorFlow support. There’s a strong focus on enabling training at scale, improving compatibility with pre-training tools, and enhancing support for major fine-tuning frameworks. Tokenization now uses the `tokenizers` backend exclusively, and new abstractions like the `AttentionInterface` further simplify code management.Inference and production deployment receive significant upgrades, including specialized kernels, new APIs (such as continuous batching and a new serving system), and seamless integration with leading inference engines (vLLM, SGLang, ONNXRuntime, llama.cpp, MLX, and more). Quantization is now a first-class feature, supporting efficient low-precision model formats and improved compatibility with tools like TorchAO and bitsandbytes. Overall, v5 positions Transformers as the central, interoperable hub for training, fine-tuning, and deploying AI models across the ecosystem, with a focus on ease of use and extensibility." class="half-size" src="assets/slide_2025_49/transformers-v5-simple-model-definitions-powering-the-ai-ecosystem-huggingface.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/deredleritt3r/status/1997056259615244546" target="_blank">
  <img alt="Demis Hassabis predicts that in 2025, advanced video models and large language models will merge, enhancing AI capabilities. This integration is expected to create more powerful and versatile AI systems.Anthropic anticipates that by 2026, AI models will become more proactive, with interactions becoming less reliant on user prompts. The distinction between users guiding the model and the model guiding users is expected to blur." src="assets/slide_2025_49/video-models-llms-anthropic-ai-2026.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/SemiAnalysis_/status/1997075285141758048" target="_blank">
  <img alt="OpenAI’s upcoming custom chip is designed to be highly flexible, countering the misconception that custom chips are inherently rigid or limited like dataflow machines. This flexibility is necessary to keep pace with rapid advancements in AI algorithms, such as the shift from dense to ultra-sparse transformers and evolving attention mechanisms.Unlike traditional hardware like TPUs, OpenAI’s chip—developed with Broadcom and led by a team with significant TPU experience—prioritizes adaptability to algorithmic innovation, recognizing that major efficiency gains in AI come from the software layer." class="half-size" src="assets/slide_2025_49/openai-custom-ai-chip-flexibility-broadcom.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/sleepinyourhat/status/1997006353647522098" target="_blank">
  <img alt="Opus 4.5 is currently considered one of the most well-aligned AI models based on available information. The author closely monitors its training process due to their work in alignment evaluations.They suggest that there are two main factors contributing to Opus 4.5's superior alignment, which are discussed further in the original thread." src="assets/slide_2025_49/opus-4-5-best-aligned-model-analysis.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/Yampeleg/status/1996644357445976167" target="_blank">
  <img alt="CoPilot for Office365 is criticized as a superficial implementation of AI, described as a low-effort feature added only to claim the inclusion of AI. The post suggests that no substantial effort was made to make it genuinely useful.The author argues that such practices are detrimental to the AI field as a whole, reducing meaningful advancements to mere checkbox features and undermining the credibility of real AI solutions." class="half-size" src="assets/slide_2025_49/copilot-office365-fake-ai-microsoft.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/MilkRoadAI/status/1996683041646039167" target="_blank">
  <img alt="The gap between top-performing closed-source AI models and open-source alternatives is rapidly narrowing, with open-source models now only a few months behind the latest frontier. Thanks to advances in hardware like NVIDIA's RTX 5090 GPU, consumers can run AI models locally that match the performance of last year's best data center models, making high-level AI more accessible and affordable.As open-weight models become almost as capable as proprietary ones, users can download, customize, and operate these AIs directly on personal devices. Within the next 12–18 months, this progress could eliminate the need to pay for cloud-based AI services, reducing the marginal cost of world-class AI inference to just electricity and existing hardware." class="half-size" src="assets/slide_2025_49/open-source-ai-vs-closed-models-performance-gap-closing.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.theguardian.com/technology/ng-interactive/2025/dec/02/jared-kaplan-artificial-intelligence-train-itself" target="_blank">
  <img alt="Jared Kaplan, chief scientist and co-owner of the AI company Anthropic, warns that humanity faces a critical decision by 2030: whether to allow AI systems to autonomously train and improve themselves. He describes this as the “ultimate risk,” potentially leading to an “intelligence explosion” with major benefits—such as rapid advances in biomedical research, productivity, and human flourishing—or the loss of human control over superintelligent systems. The decision could come as soon as 2027–2030, and Kaplan urges governments and society to carefully consider the consequences and prepare for unprecedented technological change.Kaplan highlights both the opportunities and dangers of advanced AI, including the risk of misuse, security threats, and the challenge of ensuring AI systems remain aligned with human interests. While Anthropic and its rivals, such as OpenAI and Google DeepMind, race to achieve artificial general intelligence (AGI), there is concern that progress is outpacing society’s ability to adapt. Kaplan calls for informed regulation and vigilance to prevent power grabs and ensure AI benefits humanity, rather than enabling harmful outcomes or falling under the control of malicious actors." src="assets/slide_2025_49/the-biggest-decision-yet-jared-kaplan-on-allowing-ai-to-train-itself-the-guardian.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/humanlike-robot-motions-capabilities.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/chris_j_paxton/status/1996586464197640193" target="_blank">
  <img alt="Robots often mimic human-like movements due to their training, not because of hardware limitations. Their actual capabilities include performing much faster and less human-like actions.This highlights that the apparent human-like behavior in robots is a result of design choices rather than technical constraints." src="assets/slide_2025_49/humanlike-robot-motions-capabilities.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_49/delivery-robots-walking-distance-meme.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/lukas_m_ziegler/status/1997098065531179083" target="_blank">
  <img alt="A conversation highlights the common situation where someone suggests getting an Uber, but another friend insists the destination is within walking distance. This relatable scenario is used to preface a mention of new technology.The post then references @rivr_tech, noting their demonstration of how future deliveries might be handled, and includes a link for more information." src="assets/slide_2025_49/delivery-robots-walking-distance-meme.webp"/>
 </a>
</section>

      
      <section>
        <div class="fragment fade-out" data-fragment-index="-1"
          style="align-items: center; justify-content: center; min-height: 70vh; margin: 0; font-size: 50%; margin: auto; display: flex; flex-direction: column;">
          <div style="margin-top: 10px; text-align:center;">
            <img src="assets/qr.png" height="100px" /> <br/>
            📚 <a href="index_all.html"><b>view all previous issues</b></a><br/>
            ✨ see you next week!
          </div>
        </div>
      </section>

    </div>
  </div>

  <script>
      let deck = new Reveal();
      deck.initialize({controls: true, progress: false, hash: true, plugins: [RevealScrollingImage]});
  </script>
</body>
</html>