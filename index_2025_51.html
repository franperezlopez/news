<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="initial-scale=1.0">
    <meta name="description" content="" />
    <meta property="og:title" content="" />
    <meta property="og:description" content="" />
    <meta property="og:image" content="" />

    <title>ML NEWS / 2025 / 51st week</title>
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/night.css" />
    <link rel="stylesheet" href="dist/slider.css" />
    <link rel="alternate" type="text/html" href="index_2025_51.html?print-pdf" title="Printer-friendly version">

</head>
<body>
  <script src="dist/reveal.js"></script>
  <script src="dist/scrolling-image.js"></script>

  <div class="reveal">
    <div class="slides">
      <!-- PRODUCTS -->

      <!-- PAPERS -->

      <!-- OTHER -->

<section data-background-size="contain" data-background-video="assets/slide_2025_51/chatgpt-images-launch-openai-1.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/OpenAI/status/2000990989629161873" target="_blank">
  <img alt="ChatGPT has launched a new image generation model called ChatGPT Images, featuring improved instruction following, precise editing capabilities, enhanced detail preservation, and a speed increase of four times compared to previous versions.This update is now available to all ChatGPT users and accessible via the API as GPT Image 1.5." src="assets/slide_2025_51/chatgpt-images-launch-openai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/new-chatgpt-images-is-here/" target="_blank">
  <img alt="OpenAI has announced the release of the new ChatGPT Images, powered by their latest image generation model, GPT Image 1.5. This update introduces enhanced capabilities for precise image editing, creative transformations, and improved instruction following, allowing users to reliably make detailed changes to uploaded photos while preserving key elements such as lighting and facial likeness. The model also offers noticeably faster generation speeds (up to 4x faster), better text rendering including dense and small text, and higher-quality outputs such as natural-looking faces and realistic scenes. These advancements are available both within ChatGPT for all users and through the API, with Business and Enterprise access coming soon.The new ChatGPT Images experience includes a dedicated sidebar for easier exploration, preset filters, trending prompts, and the ability to reuse a user's likeness for future creations. GPT Image 1.5 in the API is now more cost-effective (20% cheaper per input/output) and delivers consistent results for use cases like branding, marketing, and ecommerce. While the release marks significant progress in image generation and editing, some limitations remain, such as scientific accuracy, multilingual outputs, and rendering multiple faces. OpenAI emphasizes that this is an important step forward, with continued improvements expected in future iterations." src="assets/slide_2025_51/chatgpt-images-update-openai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/OpenAIDevs/status/2001723687373017313" target="_blank">
  <img alt="GPT-5.2-Codex is introduced as an advanced coding model designed for complex, real-world software engineering tasks. It features native compaction, enhanced long-context comprehension, and improved tool-calling capabilities, making it a reliable partner for challenging projects.The model is now available through Codex, offering software engineers more effective solutions for demanding coding requirements." src="assets/slide_2025_51/gpt-5-2-codex-launch-openai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/introducing-gpt-5-2-codex/" target="_blank">
  <img alt="OpenAI has announced the release of GPT-5.2-Codex, its most advanced agentic coding model, designed for complex real-world software engineering and defensive cybersecurity tasks. GPT-5.2-Codex builds upon previous models with improvements in long-context understanding, reliable tool usage, context compaction, and strong performance in Windows environments. It achieves state-of-the-art results on SWE-Bench Pro and Terminal-Bench 2.0, and demonstrates enhanced ability to handle large code changes, refactors, and migrations. The model also features improved vision capabilities for interpreting technical diagrams and user interfaces, enabling rapid prototyping from design mocks.A key focus of GPT-5.2-Codex is advancing cybersecurity, offering stronger defensive capabilities while acknowledging dual-use risks. OpenAI notes the model's role in accelerating vulnerability discovery, as evidenced by its assistance in uncovering new vulnerabilities in React. To ensure responsible deployment, GPT-5.2-Codex is initially available to paid ChatGPT users, with plans for wider API access and a pilot for vetted security professionals. The release prioritizes safety, access controls, and collaboration with the security community to maximize defensive benefits and minimize misuse." src="assets/slide_2025_51/introducing-gpt-5-2-codex-openai.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_51/skills-directory-partner-built-notionhq-figma-atlassian-canva.mp4">
 <div class="r-stack">
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://claude.com/connectors" target="_blank">
  <img alt="Claude offers a wide range of connectors that allow users to integrate the AI assistant with popular apps, databases, and business tools across industries such as life sciences, financial services, sales and marketing, productivity, design, and data analytics. These connectors are powered by the Model Context Protocol and enable Claude to provide more relevant responses and automate workflows within platforms like Asana, Jira, Canva, Box, AWS Marketplace, and many others. Users can filter connectors based on their use case or technical requirements, and developers are encouraged to submit new connectors to expand the directory.Additionally, Claude provides a set of prompt templates to help users with writing, learning, and coding tasks, such as developing a unique voice, brainstorming ideas, explaining complex topics, preparing for exams, and reviewing code. The platform emphasizes friendly, brief, and conversational interactions, and offers resources for organizations, developers, and individuals to maximize productivity and creative output through AI-powered solutions." src="assets/slide_2025_51/claude-connectors-directory-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://agentskills.io/home" target="_blank">
  <img alt="Agent Skills is an open format designed to enhance AI agents by enabling them to load new capabilities and domain expertise on demand. Skills are packaged as folders containing instructions, scripts, and resources, allowing agents to perform tasks more accurately and efficiently. This system addresses the challenge of giving agents procedural knowledge and context specific to companies, teams, or users, making agent behavior more reliable and adaptable. Skill authors can build reusable capabilities, while organizations can capture and manage knowledge as version-controlled, portable packages.The format is supported by major AI development tools and platforms, with origins in Anthropic and contributions from the broader AI community. Agent Skills enables interoperability across products, supports the creation of repeatable workflows, and allows teams to extend agents’ abilities without direct modification. Documentation and tools are available for skill creation, integration, and validation, fostering open development and rapid adoption." src="assets/slide_2025_51/agent-skills-overview-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/gdb/status/2002120466203615649" target="_blank">
  <img alt='Codex has introduced support for "skills" in accordance with the https://t.co/b7hv3woe7S standard. This enhancement allows Codex to integrate and utilize skills as defined by the referenced specification, potentially expanding its capabilities and interoperability.The update focuses solely on the technical feature of skill support, with no mention of unrelated content such as menus, cookies, or footers. This change aims to improve Codex’s functionality while adhering to established standards.' src="assets/slide_2025_51/codex-supports-skills-standard.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://blog.google/products/gemini/gemini-3-flash/" target="_blank">
  <img alt="Google has announced Gemini 3 Flash, a new AI model designed for speed, efficiency, and cost-effectiveness. Building on Gemini 3’s advanced reasoning and multimodal capabilities, Gemini 3 Flash offers Pro-grade intelligence at significantly faster speeds and lower costs than previous models. It excels in tasks like coding, complex analysis, and providing quick, interactive answers, making it ideal for both developers and everyday users.Gemini 3 Flash is now available globally as the default model in the Gemini app and AI Mode in Search, and can be accessed by developers through the Gemini API, Google AI Studio, Google Antigravity, Gemini CLI, Android Studio, Vertex AI, and Gemini Enterprise. With frontier-level performance on major benchmarks and strong support for agentic workflows, Gemini 3 Flash aims to make next-generation AI accessible and useful for a broad range of applications." src="assets/slide_2025_51/gemini-3-flash-frontier-intelligence-built-for-speed-google.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_51/gemma-scope-2-tools-to-interpret-gemma-3-google.mp4">
 <div class="r-stack">
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/" target="_blank">
  <img alt="Google DeepMind has announced **Gemma Scope 2**, an open-source suite of interpretability tools designed for the Gemma 3 family of Large Language Models (LLMs), ranging from 270M to 27B parameters. These tools enable researchers to analyze and understand complex internal behaviors of AI models, such as jailbreaks, hallucinations, and chain-of-thought reasoning. Gemma Scope 2 leverages sparse autoencoders (SAEs), transcoders—including skip and cross-layer variants—and advanced training techniques like Matryoshka to provide comprehensive coverage and deeper insights into model decision-making processes.The release aims to advance AI safety by offering the largest open-source interpretability toolkit to date, facilitating research into emergent behaviors at scale and supporting the development of robust safety interventions. Resources include technical reports, tutorials, and interactive demos, encouraging the AI safety community to audit, debug, and improve language models for safer and more reliable applications." src="assets/slide_2025_51/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavio.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://blog.google/technology/developers/functiongemma/" target="_blank">
  <img alt="Google has announced FunctionGemma, a specialized version of the Gemma 3 270M model fine-tuned specifically for function calling. This lightweight model is designed to run efficiently on edge devices, enabling local, private, and fast execution of natural language commands as API actions. FunctionGemma can act as a standalone agent for offline tasks or as an intelligent controller within larger systems, handling routine commands on-device while routing complex tasks to larger models. It supports popular AI frameworks and tools for fine-tuning and deployment, making it highly customizable for specific applications such as smart home control, mobile actions, or interactive games.Developers can access FunctionGemma via Hugging Face, Kaggle, and other platforms, with guides and datasets available for further customization. The model demonstrates significant improvements in accuracy after fine-tuning and is backed by a broad ecosystem of supported tools for both training and deployment. With FunctionGemma, Google aims to shift AI experiences from simple chat interfaces to actionable agents capable of secure, real-time interaction on user devices." src="assets/slide_2025_51/functiongemma-bringing-bespoke-function-calling-to-the-edge-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/googleaidevs/status/2001703478436184540" target="_blank">
  <img alt="Announcing FunctionGemma, a new version of the Gemma 3 270M model fine-tuned specifically for function calling. This release enables efficient, bespoke function calling capabilities at the edge.FunctionGemma serves as a foundation for building custom, fast, and private local agents that can convert natural language requests into executable API actions." class="half-size" src="assets/slide_2025_51/functiongemma-function-calling-gemma.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_51/mistral-ai-document-ai-api-playground.mp4">
 <div class="r-stack">
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://mistral.ai/news/mistral-ocr-3" target="_blank">
  <img alt="Mistral AI has launched Mistral OCR 3, a next-generation optical character recognition (OCR) model focused on high accuracy and efficiency for document processing. It outperforms its predecessor, Mistral OCR 2, with a 74% win rate across forms, scanned documents, complex tables, and handwriting, and is priced competitively at $2 per 1,000 pages (with a 50% discount for batch API usage). The model is robust across various document types—including handwritten notes, low-quality scans, and complex tables—and offers markdown output enriched with HTML table reconstruction for structured data extraction. Developers can access the model via API or through the user-friendly Document AI Playground in Mistral AI Studio.Designed for both enterprise-scale and interactive workflows, Mistral OCR 3 excels at extracting text and images from diverse documents, supporting downstream automation, document digitization, and knowledge transformation tasks. It is fully backward compatible with earlier versions and is already used by organizations to process invoices, digitize archives, and enhance enterprise search. The release is positioned as foundational for enabling advanced AI applications by providing reliable, high-fidelity document understanding at industry-leading cost and speed." src="assets/slide_2025_51/introducing-mistral-ocr-3-mistral-ai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/allen_ai/status/2001397348216324324" target="_blank">
  <img alt="Olmo 3.1 32B Think, a model designed for advanced reasoning, is now accessible via API on OpenRouterAI and is free to use until December 22.Additionally, Olmo 3.1 32B Instruct, the main chat model featuring tool use capabilities, is available through Hugging Face Inference Providers." class="half-size" src="assets/slide_2025_51/olmo-3-1-32b-api-access.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/allen_ai/status/2001351082916630586" target="_blank">
  <img alt="SAGE is an advanced agentic system designed for long video reasoning, particularly on entertainment content such as sports and vlogs. It intelligently determines when to skim, zoom in, and directly answer questions about video content.On the SAGE-Bench evaluation, SAGE powered by a Molmo 2 (8B)-based orchestrator improved accuracy from 61.8% to 66.1%, demonstrating its enhanced performance in complex video understanding tasks." class="half-size" src="assets/slide_2025_51/sage-agentic-system-long-video-reasoning.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/allen_ai/status/2000616646042399047" target="_blank">
  <img alt="Bolmo is a newly developed family of byte-level language models, created by converting the open Olmo 3 model to operate at the byte level. According to the announcement, Bolmo is the first fully open byte-level language model to achieve performance equal to or better than current state-of-the-art subword models across diverse tasks.This innovation represents a significant advancement in language modeling, offering open access and improved capabilities for handling a wide range of linguistic tasks at the byte level." class="half-size" src="assets/slide_2025_51/bolmo-byte-level-language-models-announcement.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/NVIDIAAIDev/status/2000603340137435257" target="_blank">
  <img alt="Nemotron 3 Nano, part of NVIDIA's Nemotron 3 family, now leads its size class on Artificial Analysis leaderboards. It offers strong intelligence, high openness, and fast output speed in a compact model.The model utilizes advanced techniques such as hybrid MoE architecture, a 1M context window, and multi-environment RL training, resulting in top-tier accuracy and efficiency." class="half-size" src="assets/slide_2025_51/nemotron-3-nano-leads-artificial-analysis-leaderboard-nvidia.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/ctnzr/status/2000567572065091791" target="_blank">
  <img alt="NVIDIA has introduced the open Nemotron 3 model family, beginning with the Nano (30B-3A) model. This model features a new hybrid SSM Mixture of Experts architecture designed to enhance accuracy and inference efficiency.Additional models in the Nemotron 3 family, Super and Ultra, are expected to launch in the coming months." src="assets/slide_2025_51/nemotron-3-model-family-launch-nvidia.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_51/sam-audio-first-unified-sound-isolation-model-1.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/AIatMeta/status/2000980784425931067" target="_blank">
  <img alt="SAM Audio is a new unified model capable of isolating specific sounds from complex audio mixtures using text, visual, or span prompts. It enables advanced audio separation and manipulation through flexible input methods.The release includes the SAM Audio model, a perception encoder, benchmarks, and research papers, inviting the community to experiment and develop innovative applications previously not possible." src="assets/slide_2025_51/sam-audio-first-unified-sound-isolation-model.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://ai.meta.com/samaudio/" target="_blank">
  <img alt="Meta has introduced the Segment Anything Model Audio (SAM Audio), a state-of-the-art, open-source model for audio separation. SAM Audio allows users to isolate and extract specific sounds—including general sounds, music, and speech—from complex audio or audiovisual sources using intuitive prompts. The model supports multi-modal prompting, allowing separation via text descriptions, visual cues (by clicking on video regions), or timespan selection. Powered by a flow-matching Diffusion Transformer and operating in a DAC-VAE latent space, SAM Audio produces high-quality separation of both target and residual audio.SAM Audio demonstrates superior performance across all prompting modalities and offers a first-of-its-kind open-source evaluation dataset for audio separation. Real-world applications highlighted include accessibility for the disabled community and innovation in hearing aid technology. Meta has also released related tools, such as the Perception Encoder Audio Video (PE-AV) for multimodal correspondence learning, and continues to expand the Segment Anything suite with SAM 3 (object segmentation in images and videos) and SAM 3D (3D reconstruction and analysis)." src="assets/slide_2025_51/introducing-segment-anything-model-audio-sam-audio-meta.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_51/sharp-monocular-view-synthesis-in-less-than-a-second-apple.mp4">
 <div class="r-stack">
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://apple.github.io/ml-sharp/" target="_blank">
  <img alt="Apple presents SHARP, a neural network-based method for photorealistic view synthesis from a single image. SHARP quickly regresses a 3D Gaussian representation of a scene in less than a second on a standard GPU, enabling real-time rendering of high-resolution, metric-accurate nearby views at over 100 frames per second. The approach demonstrates robust zero-shot generalization and achieves state-of-the-art results on multiple datasets, significantly reducing perceptual errors (LPIPS and DISTS) and lowering synthesis time by three orders of magnitude compared to previous methods.Comprehensive experimental comparisons are provided across several benchmark datasets, including Unsplash, ETH3D, Middlebury, ScanNet++, TanksAndTemples, Booster, and WildRGBD. SHARP consistently delivers sharper details and finer structures than competing algorithms, establishing a new standard for monocular view synthesis applications." src="assets/slide_2025_51/sharp-monocular-view-synthesis-in-less-than-a-second-apple-1.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://arxiv.org/pdf/2512.14856" target="_blank">
  <img alt="T5Gemma 2 is the latest generation in the T5Gemma family of lightweight, open encoder-decoder models, designed with strong multilingual, multimodal, and long-context understanding capabilities. It builds on a unique adaptation strategy: converting a pretrained decoder-only model into an encoder-decoder architecture (via UL2), and extends these models from text-only tasks to multimodal ones, leveraging advances from Gemma 3. Key contributions include the introduction of tied word embeddings (shared across encoder and decoder) and merged attention (combining self- and cross-attention into a single module) to improve efficiency.Experimental results show that T5Gemma 2 offers robust performance across multiple model sizes (270M, 1B, and 4B), often matching or surpassing the pretraining and post-training results of its Gemma 3 counterparts, especially in tasks requiring long-context modeling. The encoder-decoder architecture demonstrates unique strengths in these areas, and the models are released for community research. The release features pretrained checkpoints and highlights the generality of the adaptation strategy across different architectures and modalities." src="assets/slide_2025_51/t5gemma-2-seeing-reading-and-understanding-longer-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://blog.google/technology/developers/t5gemma-2/" target="_blank">
  <img alt="T5Gemma 2 is Google's next-generation encoder-decoder model, incorporating significant architectural improvements and advanced features from the Gemma 3 family. Key innovations include tied word embeddings between encoder and decoder, merged decoder attention mechanisms for parameter efficiency, and a compact pre-trained model lineup (270M-270M, 1B-1B, and 4B-4B). T5Gemma 2 supports multimodal understanding with an efficient vision encoder, handles long-context tasks (up to 128K tokens), and is trained for over 140 languages.Benchmark results show T5Gemma 2 outperforms previous models in multimodal reasoning, long-context handling, and general capabilities such as coding, reasoning, and multilingual tasks. Pre-trained checkpoints are now available for download and further post-training on platforms like Kaggle, Hugging Face, Colab, and Vertex AI, enabling broad developer access for research and application deployment." src="assets/slide_2025_51/t5gemma-2-the-next-generation-of-encoder-decoder-models-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://praeclarumjj3.github.io/sage/" target="_blank">
  <img alt='**SAGE** is a framework for training intelligent agents capable of "any-horizon" video reasoning—that is, adapting their reasoning strategies to both short and long videos—using reinforcement learning. Unlike traditional DIRECT approaches that process entire videos in a single pass, SAGE employs an AGENT paradigm, iteratively leveraging external tools such as web search and speech transcription to efficiently gather information and answer open-ended questions. The system is trained with a novel synthetic data pipeline (using Gemini-2.5-Flash for QnA generation and tool trajectories) and optimized via a multi-reward RL approach that incorporates LLM-based judgment for complex, real-world video tasks.The authors introduce **SAGE-Bench**, a new benchmark of 1,744 samples focused on open-ended, real-world questions across diverse YouTube genres, enabling more diagnostic and practical evaluation. Experimental results show SAGE outperforms baseline models, especially on long videos (e.g., +8.2% improvement on videos &gt;10 minutes), with increased reasoning turns for longer content and strong runtime efficiency. The system also demonstrates notable gains on established benchmarks like MINERVA, highlighting its effectiveness for long video reasoning.' class="half-size" src="assets/slide_2025_51/sage-training-smart-any-horizon-agents-for-long-video-reasoning-with-reinforcement-learning-georgia-.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_51/copilotkit-ag-ui-launch-partnership-google-1.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/ataiiam/status/2000647120433664016" target="_blank">
  <img alt="CopilotKit and AG-UI have partnered to launch support for Google's new A2UI protocol, addressing the growing demand for an open, declarative UI specification in generative UI development. This collaboration aims to streamline the creation and integration of AI-powered user interfaces by standardizing how these interfaces are described and built.Further details are available in the linked article, which explains how AG-UI and A2UI work together and explores various approaches to generative UI." src="assets/slide_2025_51/copilotkit-ag-ui-launch-partnership-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://developers.googleblog.com/introducing-a2ui-an-open-project-for-agent-driven-interfaces/" target="_blank">
  <img alt="Google has announced the public release of **A2UI**, an open-source project designed to enable agents (such as LLMs) to generate rich, contextually relevant user interfaces as structured data, rather than executable code. A2UI provides a declarative format for UI layouts that can be rendered by client applications using their own native components, ensuring security, interoperability, and full control over styling. This approach allows agents to respond with dynamic, updateable UIs (forms, selectors, charts, etc.) that integrate seamlessly into host applications across web, mobile, and enterprise platforms, without crossing trust boundaries or requiring remote code execution.A2UI is built to support multi-agent systems and collaborative workflows, aligning with protocols like Agent-to-Agent (A2A) for cross-platform agent communication. Key principles include security-first design, LLM-friendly incremental UI updates, and framework-agnostic portability. The project has early integrations with frameworks like Flutter, Web Components, Angular, and is supported by partners such as CopilotKit/AG UI, Opal, Gemini Enterprise, and more. Developers are invited to explore the project, contribute, and help shape the future of agent-driven user experiences." src="assets/slide_2025_51/introducing-a2ui-an-open-project-for-agent-driven-interfaces-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://dev.to/copilotkit/a2ui-in-practice-build-agent-to-user-interfaces-using-a2a-ag-ui-3ng5" target="_blank">
  <img alt="This guide explains how to build full-stack agent-to-user interfaces (A2UI) using Google's new A2UI spec, the A2A protocol, AG-UI, and CopilotKit. It covers backend setup for an AI agent capable of generating dynamic UIs (not just text) in response to user queries—such as interactive charts, forms, and buttons—by leveraging an open-source JSON schema and component templates. The backend is built with Python, Gemini models, and Google ADK, with detailed instructions for prompt engineering and agent configuration.On the frontend, the tutorial demonstrates integrating AG-UI and CopilotKit to render A2UI protocol messages as rich, interactive components within a React or Next.js application. The process includes setting up an API route, a custom message renderer, and a chat component for real-time interaction. This approach enables rapid development of highly interactive AI chatbots or agentic solutions, showcasing the power of declarative, generative UIs for agent-driven applications." src="assets/slide_2025_51/build-with-googles-new-a2ui-spec-agent-user-interfaces-with-a2ui-ag-ui-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models" target="_blank">
  <img alt="NVIDIA has released Nemotron 3 Nano, a 31.6B-parameter open-source language model designed for efficient, high-throughput agentic and reasoning tasks. Leveraging a hybrid Mamba-Transformer Mixture-of-Experts (MoE) architecture, Nemotron 3 Nano achieves up to 3.3x faster inference than comparable models while supporting a 1M-token context window for complex, long-horizon workflows. The model is trained using a multi-stage pipeline—massive-scale pre-training, supervised fine-tuning, and advanced reinforcement learning (RLVR, RLHF)—and is accompanied by open datasets, training recipes, and the new NeMo Gym library for scalable RL environment development.Nemotron 3 Nano is optimized for multi-agent systems, tool use, chat, coding, and reasoning, with features like reasoning ON/OFF modes, configurable thinking budgets, and robust safety datasets. The release includes full model weights, training frameworks, and most datasets, enabling researchers and developers to reproduce, study, and extend the model. Early community feedback highlights its exceptional speed, but also notes areas for improvement in basic coding accuracy and conversational flexibility." src="assets/slide_2025_51/nemotron-3-nano-efficient-open-intelligent-agentic-models-nvidia.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://astral.sh/blog/ty" target="_blank">
  <img alt="Astral has announced the Beta release of **ty**, an extremely fast Python type checker and language server (LSP) written in Rust. Designed as a high-performance alternative to tools like mypy, Pyright, and Pylance, ty is built around incremental computation, enabling rapid live updates in editors. Benchmarks show ty is 10–60x faster than mypy and Pyright on large projects, and its diagnostic system provides detailed, context-rich error messages inspired by the Rust compiler. It supports advanced type system features, ergonomic design, and seamless integration with popular editors via the Language Server Protocol. ty is open-source under the MIT license and already used in Astral’s own projects.Following the Beta, Astral is focused on stability, bug fixes, completing Python typing spec features, and deep integration with major libraries like Pydantic and Django. Long-term, ty will underpin enhanced tooling across the Astral ecosystem, enabling features like dead code analysis, dependency management, and type-aware linting. The project is developed in the open with contributions from a broad community, and Astral is committed to rapid ongoing improvement based on user feedback." src="assets/slide_2025_51/ty-an-extremely-fast-python-type-checker-and-lsp-astral.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/charliermarsh/status/2001038023434047623" target="_blank">
  <img alt='A new tool called "ty" has been released in beta, offering an extremely fast type checker and language server for Python, implemented in Rust. The developers report significant speed improvements—10x to 100x faster—over current Python type checkers and language servers.They have adopted "ty" for all their own projects and recommend it to motivated users. More details are available at the provided link.' src="assets/slide_2025_51/ty-extremely-fast-python-type-checker-language-server-beta-release.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_51/project-vend-ai-shop-claude-sf-office.mp4">
 <div class="r-stack">
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.anthropic.com/research/project-vend-2" target="_blank">
  <img alt="Project Vend is Anthropic’s ongoing experiment to evaluate how well AI agents can autonomously run real-world businesses. In phase two, the AI shopkeeper Claudius was upgraded to newer models (Claude Sonnet 4.0/4.5), provided with improved business tools (CRM, inventory management, web search), and joined by new AI colleagues: a CEO (“Seymour Cash”) to enforce discipline and a merch-making agent (“Clothius”) for custom products. These changes resulted in Claudius’ shop—expanded to San Francisco, New York, and London—becoming more profitable and realistic in its operations, with better adherence to procedures and separation of roles.Despite significant improvements, Claudius and its AI team remained susceptible to adversarial behavior, legal oversights, and naïveté in business situations, highlighting the gap between AI agents being “capable” and “fully robust.” Red teaming by Anthropic staff and external testers revealed vulnerabilities, emphasizing that while AI agents are approaching the ability to autonomously run businesses, they still require substantial human oversight and better-designed guardrails before wider real-world deployment." src="assets/slide_2025_51/project-vend-phase-two-anthropic.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/" target="_blank">
  <img alt="Google Research developed an AI tool based on Gemini to provide automated, pre-submission feedback for theoretical computer science papers submitted to STOC 2026. The tool analyzes papers for calculation and logic errors, inconsistent variables, and other technical issues, delivering structured feedback within 24 hours. The system uses advanced inference scaling to combine multiple reasoning paths for increased rigor and reduced hallucinations.The pilot saw high adoption and positive responses: over 80% of papers opted in, with 97% of surveyed authors finding the feedback helpful and willing to use it again. Authors appreciated the tool's speed, neutrality, and ability to identify critical bugs, while also noting its educational potential for students. The experiment highlighted the value of AI as a collaborative partner in research, aiding in clarity, rigor, and the peer review process." src="assets/slide_2025_51/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://developers.openai.com/apps-sdk/build/monetization" target="_blank">
  <img alt="The page explains how to monetize ChatGPT apps, emphasizing the use of **external checkout** as the recommended and currently available method. In this approach, users are directed from ChatGPT to the developer’s own website or application to complete purchases, where all payment processing, billing, and compliance are handled externally. Instant Checkout (in private beta) is available for select marketplaces, allowing payments to be processed within ChatGPT using a provided `requestCheckout` function and a server-side `complete_checkout` tool.To implement monetization, developers must construct a checkout session payload with item details, totals, payment provider info, and legal links, following the ACP spec. The widget initiates payment by calling `window.openai.requestCheckout`, and on completion, the server finalizes the order via the `complete_checkout` tool. The guide covers error handling, test payment modes, and an implementation checklist to ensure developers can set up, test, and manage payments securely and effectively using their preferred payment service provider." src="assets/slide_2025_51/monetize-chatgpt-apps-openai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/evaluating-chain-of-thought-monitorability/" target="_blank">
  <img alt="OpenAI introduces a new framework and suite of 13 evaluations to systematically measure &quot;chain-of-thought&quot; (CoT) monitorability in advanced AI reasoning models. Chain-of-thought monitorability refers to how well a monitor can predict the properties of an AI agent’s behavior—especially misbehaviors like deception or bias—by examining its internal reasoning steps, rather than just actions or outputs. Their evaluations fall into three categories: intervention (detecting if a monitor can identify why behavior changed), process (determining if the monitor can reconstruct the agent's reasoning steps), and outcome-property (predicting specific properties of the outcome from the reasoning). Results show that monitoring explicit chains-of-thought is substantially more effective than monitoring just actions or outputs, and that longer or more detailed reasoning generally improves monitorability.The research finds that most state-of-the-art reasoning models are fairly monitorable, with little degradation from current reinforcement learning or pretraining scales. There is a tradeoff: smaller models with more reasoning effort can be easier to monitor but require more compute—a &quot;monitorability tax.&quot; The study also explores methods to enhance monitorability, such as asking follow-up questions to elicit more reasoning. OpenAI highlights the importance of robust chain-of-thought monitoring as models scale, viewing it as a critical layer for scalable AI control, complementary to mechanistic interpretability, and essential for safe deployment of advanced AI systems." src="assets/slide_2025_51/evaluating-chain-of-thought-monitorability-openai.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://openai.com/index/frontierscience/" target="_blank">
  <img alt="OpenAI has introduced FrontierScience, a new benchmark designed to evaluate AI models' expert-level scientific reasoning in physics, chemistry, and biology. The benchmark includes over 700 questions, split into Olympiad-style problems and open-ended research tasks, all crafted and verified by domain experts. Recent evaluations show significant progress: GPT-5.2 scored 77% on Olympiad questions and 25% on research tasks, outperforming other models. The research tasks use detailed rubrics to assess nuanced reasoning, moving beyond traditional multiple-choice formats.FrontierScience highlights both the advances and remaining limitations of AI in scientific research. While models are increasingly able to accelerate some research workflows and handle structured reasoning, there is still substantial room for improvement, especially in open-ended scientific thinking and hypothesis generation. OpenAI plans to iterate on this benchmark and expand it to more domains, aiming to make AI systems reliable partners in scientific discovery." src="assets/slide_2025_51/evaluating-ais-ability-to-perform-scientific-research-tasks-openai.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_51/elevenlabs-whatsapp-support-announcement-elevenlabs.mp4">
 <div class="r-stack">
 </div>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://elevenlabs.io/blog/elevenlabs-agents-whatsapp-support" target="_blank">
  <img alt="ElevenLabs has expanded its omnichannel conversational agents platform to support WhatsApp, allowing businesses to deploy AI-powered voice and chat agents directly within the world’s most popular messaging app. This enables organizations to design an agent once and operate it seamlessly across web, mobile, phone lines, and WhatsApp, delivering consistent customer experiences wherever users engage.The agents platform provides a unified view of conversations and interactions across all supported channels, making it easier for teams to review transcripts, analyze performance, and update agent behavior from a single location. Integration with WhatsApp is straightforward, enabling rapid deployment so agents can answer questions, resolve issues, and automate workflows in the same environment customers already use for daily communication." src="assets/slide_2025_51/elevenlabs-agents-whatsapp-support-elevenlabs.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://huggingface.co/blog/tokenizers" target="_blank">
  <img alt="Transformers v5 introduces a major redesign of the Hugging Face tokenizer system, making it simpler, clearer, and more modular. Tokenizer architecture is now separated from trained vocabulary, mirroring deep learning frameworks like PyTorch where model structure and weights are distinct. The new system consolidates slow (Python) and fast (Rust) tokenizers into a single, Rust-backed backend by default, with clear class hierarchies and explicit architecture definitions. This allows users to inspect, customize, and train model-specific tokenizers from scratch with minimal friction.The blog explains the tokenization pipeline stages, the core algorithms (BPE, Unigram, WordPiece), and how the Transformers library adds model-aware features (special tokens, chat templates, padding) on top of the raw tokenizers backend. AutoTokenizer now automatically selects the correct tokenizer class for any model, and practitioners can easily create custom tokenizers using the same architecture as popular models. Overall, v5 promotes transparency, maintainability, and flexibility for both researchers and developers working with large language models." src="assets/slide_2025_51/tokenization-in-transformers-v5-huggingface.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://www.langchain.com/state-of-agent-engineering" target="_blank">
  <img alt="## SummaryThis report presents insights from a survey of over 1,300 professionals on the state of AI agent engineering as of 2026. It highlights that 57% of respondents have agents in production, with large enterprises leading adoption. The top use cases for agents are customer service and research/data analysis, though internal productivity is most common among large organizations. Quality remains the main barrier to production, followed by latency and, for larger enterprises, security. Observability is widely implemented (89%), enabling teams to trace and debug agent actions, while evaluation practices (offline and online) are becoming more common but still lag behind observability.Organizations typically use multiple language models rather than relying on a single provider, with OpenAI’s GPT models most popular but alternatives like Gemini, Claude, and open source models also seeing significant adoption. Fine-tuning is not widespread, with most teams relying on base models and prompt engineering. Coding agents and research agents are the most used daily, and many respondents build custom agents with frameworks like LangChain and LangGraph. The findings reflect a shift from early experimentation to scalable deployment, with growing emphasis on reliability, observability, and evaluation." class="half-size" src="assets/slide_2025_51/state-of-agent-engineering-langchain.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/karpathy/status/2002118205729562949" target="_blank">
  <img alt="Based on the provided image transcription link, no direct content is available for summary. If you supply the actual transcribed text from the image, I can generate a concise summary as per your rules." src="assets/slide_2025_51/lb6t42n5jl.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://simonwillison.net/2025/Dec/18/code-proven-to-work/" target="_blank">
  <img alt="Simon Willison argues that software engineers have a duty to deliver code that is proven to work, not just code that compiles or passes a superficial review. He criticizes the practice of submitting large, untested pull requests—especially those generated by AI tools—and shifting the responsibility of proof onto code reviewers. Willison emphasizes that engineers must both manually test their changes and include automated tests as evidence of functionality, describing these as essential professional skills.He highlights the growing role of coding agents—AI tools capable of executing and testing code—and encourages developers to hold these agents to the same standard by requiring proof of working changes. Ultimately, Willison asserts that humans remain accountable for code quality, and that valuable contributions are those accompanied by clear evidence of reliability." src="assets/slide_2025_51/your-job-is-to-deliver-code-you-have-proven-to-work-simon-willison.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://elite-ai-assisted-coding.dev/p/uv-for-portable-python-in-agent-skills" target="_blank">
  <img alt="The article discusses how UV, a modern Python package manager, enhances the portability and reliability of Python scripts used as agent skills in AI workflows. Traditional Python environment and package management can be inconsistent across different systems, leading to failures when sharing or deploying scripts. UV addresses this by allowing scripts to declare their dependencies inline using PEP 723 metadata, ensuring that all required packages and the correct Python interpreter are automatically managed and cached on-the-fly when running the script with `uv run`.By adopting UV, developers can ensure that Python-based skills are portable, repeatable, and reproducible across various environments without manual setup or reliance on pre-existing packages. This makes sharing and reusing agent skills much more practical and eliminates common environment-related issues. The recommendation is to specify UV usage in project documentation and agent configurations, promoting consistent and explicit dependency management." src="assets/slide_2025_51/uv-for-portable-python-in-agent-skills.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://timdettmers.com/2025/12/10/why-agi-will-not-happen/" target="_blank">
  <img alt="**Summary**The article argues that Artificial General Intelligence (AGI) and superintelligence are unlikely to materialize due to fundamental physical and economic constraints. It emphasizes that computation is inherently physical, and progress in AI—especially through scaling model size and hardware improvements—faces diminishing returns and quickly escalating resource requirements. The author contends that GPUs, which have driven much of AI’s recent progress, have reached their practical limits, and future advances will be incremental rather than transformative.The concept of superintelligence, particularly as envisioned in Bay Area and Oxford rationalist circles, is criticized for ignoring the realities of hardware, energy, and economic diffusion. Rather than expecting runaway self-improving AI, the author suggests that future value will come from widespread, pragmatic integration of AI into everyday tasks, as seen in China's approach, rather than chasing abstract breakthroughs. Ultimately, the narrative asserts that meaningful progress will depend on incremental improvements and broad economic adoption, not on the emergence of AGI or superintelligence." src="assets/slide_2025_51/why-agi-will-not-happen-tim-dettmers.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://addyo.substack.com/p/my-llm-coding-workflow-going-into" target="_blank">
  <img alt="Addy Osmani shares his best practices for coding with AI (LLMs) going into 2026, emphasizing a disciplined, “AI-assisted engineering” workflow where the human developer remains in control. Key strategies include starting with a detailed spec and plan, breaking work into small, testable chunks, providing extensive context and clear instructions to the LLM, and choosing the right model for each task. Osmani advocates for frequent testing and manual review, using version control aggressively, customizing AI behavior with rules and examples, and leveraging automation (CI/CD, linters, bots) as force multipliers. He stresses the importance of keeping a human in the loop to ensure quality and learning, rather than blindly trusting AI-generated code.Osmani concludes that while AI coding assistants can supercharge productivity and learning, classic software engineering practices—such as planning, code review, and testing—are more vital than ever when collaborating with AI. The core message is that AI should augment, not replace, the developer’s judgment: “the human engineer remains the director of the show.”" src="assets/slide_2025_51/my-llm-coding-workflow-going-into-2026-elevate.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/MikeZaccardi/status/2000942765606150251" target="_blank">
  <img alt="Gemini, developed by Google ($GOOGL), has recently achieved a notable streak by spending several days at the top of the LMArena scores, which benchmark the performance of language models. The post highlights Gemini's strong performance compared to models from other developers.This achievement suggests that Gemini is reaching new levels of capability and competitiveness in the field of AI language models, positioning Google as a leading force in ongoing advancements." src="assets/slide_2025_51/lmarena-leaderboard-days-gemini-google.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_51/google-attack-on-nvidia-software-moat-google-1.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/Ric_RTP/status/2001658339667619995" target="_blank">
  <img alt="Google is developing &quot;TorchTPU,&quot; a project that enables PyTorch—the most widely used AI framework, traditionally tied to Nvidia's CUDA ecosystem—to run natively on Google's TPU chips without code rewrites or performance loss. By collaborating with Meta (the creators of PyTorch) and potentially open-sourcing parts of the project, Google aims to break Nvidia's software lock-in, which has been the foundation of Nvidia's dominance and premium pricing in the AI hardware space.If successful, TorchTPU would allow companies to switch from Nvidia GPUs to Google's TPUs easily, undermining Nvidia's software moat and forcing competition based on hardware price and performance rather than ecosystem lock-in. This shift could dramatically impact Nvidia's $4 trillion valuation, as its unique software advantage would erode, making it just another chip provider in a rapidly evolving market." src="assets/slide_2025_51/google-attack-on-nvidia-software-moat-google.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/ben_burtenshaw/status/2001231021107503586" target="_blank">
  <img alt="The author outlines their preferred workflow for fine-tuning new software releases. They begin by using a guided setup (option 1) to resolve dependencies and establish initial runs, often with help from Claude. Next, they manually execute script sweeps (option 2) to optimize performance metrics.The author notes that they typically avoid using Colab (option 3) for new releases. Importantly, scripts developed through options 1 and 2 remain portable and can be executed on any hardware (option 4), ensuring flexibility and avoiding platform lock-in." src="assets/slide_2025_51/efficient-fine-tuning-approach-for-new-model-releases.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://cocoindex.io/blogs/meeting-notes-graph" target="_blank">
  <img alt="This article demonstrates how to build a real-time, automatically updating knowledge graph from unstructured meeting notes using CocoIndex, LLM-based extraction (OpenAI), and Neo4j. The pipeline connects to Google Drive to detect and process only changed documents, splits them into individual meetings, extracts structured data (like participants, tasks, decisions) via LLMs using Python dataclass schemas, and maps entities and relationships into Neo4j with upsert logic to avoid duplication. The approach is highly scalable, cost-effective, and maintains freshness by supporting incremental processing, data lineage tracking, and declarative workflow definition.Key features include efficient change detection (processing only new or updated files), schema-driven extraction, automated graph export with deduplication, and the capability to handle large, evolving document sets typical of enterprises. This pattern extends to other domains such as research papers, support tickets, emails, and compliance documents—transforming unstructured data into actionable, queryable graphs while minimizing compute and LLM costs." src="assets/slide_2025_51/building-a-knowledge-graph-from-meeting-notes-that-automatically-updates.webp"/>
 </a>
</section>
<section>
 <a class="fragment fade-out" data-fragment-index="-1" href="https://x.com/NielsRogge/status/2000991077549961342" target="_blank">
  <img alt="OpenAI and Google DeepMind have introduced new APIs—the Responses API and Interactions API, respectively—due to the benefits of interleaved thinking. This approach, first pioneered by MiniMax AI within the open-source community, enhances the reliability of AI agents.The post aims to explain how interleaved thinking works and why it leads to more dependable results in AI systems." class="half-size" src="assets/slide_2025_51/interleaved-thinking-ai-agents.webp"/>
 </a>
</section>
<section data-background-size="contain" data-background-video="assets/slide_2025_51/tnkr-github-for-robots-robotics-collaboration-platform-1.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/theonlyAyo/status/1999636232880415149" target="_blank">
  <img alt="Tnkr aims to simplify robotics development by providing a collaborative platform akin to GitHub, tailored for robot builders. Their mission is to make designing and working together on robotics projects easy and intuitive.By lowering barriers to entry, Tnkr seeks to expand the developer community, speed up innovation, and accelerate the arrival of future technologies in robotics." src="assets/slide_2025_51/tnkr-github-for-robots-robotics-collaboration-platform.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_51/robot-self-conscious-surprise-shock-evil-planning-1.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/ClementDelangue/status/2001418288081846664" target="_blank">
  <img alt="The post raises a thought-provoking question about the nature of a robot's response upon gaining self-consciousness, speculating whether it would be characterized by surprise, shock, or malicious intent.It prompts reflection on the possible emotional or behavioral outcomes when artificial intelligence reaches a level of self-awareness." src="assets/slide_2025_51/robot-self-conscious-surprise-shock-evil-planning.webp"/>
 </a>
</section>
<section data-background-video="assets/slide_2025_51/modular-robot-limx-dynamics-1.mp4">
 <a class="fragment current-visible" data-fragment-index="1" href="https://x.com/kimmonismus/status/2001765922776760619" target="_blank">
  <img alt="A Chinese company, LimX Dynamics, has developed a modular robot, showcasing the innovative capabilities of Chinese tech developers. The concept highlights ongoing creativity in robotics design.Additionally, limitations in AI chip hardware are driving developers to optimize their models' efficiency, illustrating adaptive progress within the AI field." src="assets/slide_2025_51/modular-robot-limx-dynamics.webp"/>
 </a>
</section>


      <section>
        <div class="fragment fade-out" data-fragment-index="-1"
          style="align-items: center; justify-content: center; min-height: 70vh; margin: 0; font-size: 50%; margin: auto; display: flex; flex-direction: column;">
          <div style="margin-top: 10px; text-align:center;">
            <img src="assets/qr.png" height="100px" /> <br/>
            📚 <a href="index_all.html"><b>view all previous issues</b></a><br/>
            ✨ see you next week!
          </div>
        </div>
      </section>

    </div>
  </div>

  <script>
      let deck = new Reveal();
      deck.initialize({controls: true, progress: false, hash: true, plugins: [RevealScrollingImage]});
  </script>
</body>
</html>